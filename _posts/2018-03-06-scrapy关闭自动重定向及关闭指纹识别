---
layout:     post
title:      scrapy关闭自动重定向及关闭指纹识别
subtitle:   scrapy
date:       2018-03-06
author:     muyalei
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - scrapy
---

### 详细说明
1.
示例代码：<br>
```
def start_requests(self):
    yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result)
    
def parse_result(self,response):
    print(response.status)
```
调试信息：<br>
![2018-03-06-scrapy关闭自动重定向及关闭指纹识别_1.jpg]()
>不加额外参数时，下载器中间件检测到重定向，自动follow到重定向后页面，重定向后页面的响应状态吗为200，框架下载该页面

2.
示例代码：<br>
```
def start_requests(self):
    yield scrapy.Request(url=self.base_url,headers=self.headers,meta={'dont_redirect':True},callback=self.parse_result)
    
def parse_result(self,response):
    print(response.status)
```
调试信息：<br>
![2018-03-06-scrapy关闭自动重定向及关闭指纹识别_2.jpg]()
>dont_redirect参数告诉下载器中间件，不自动follow重定向，当响应状态码是302时，这个请求在下载器中间件处理完成，交给spider中间件继续处理。spider模块默认只处理响应状态码2xx
的结果，301、302、404的状态码在spider中间件被过滤掉，该请求处理结束，不会再去调用callback指定的解析方法。

3.示例代码：<br>
```
def start_requests(self):
    yield scrapy.Request(url=self.base_url,headers=self.headers,meta={'dont_redirect':True,'handle_httpstatus_list':[301,302]},callback=self.parse_result)
    
def parse_result(self,response):
    print(response.status)
```
调试信息：<br>
![2018-03-06-scrapy关闭自动重定向及关闭指纹识别_3.jpg]()
>scrapy中自定义的spider模块默认只处理响应状态码是2xx的response，handle_httpstatus_list增加spider模块可以处理的响应状态,该例中，响应状态码是301、302时，正常通过spider中间件，该例使自定义的spider部分可以处理响应状态码是301，302的响应。<br>
meta={'dont_redirect':True,'handle_httpstatus_list':[301,302]}，通过meta附带这两个参数，当请求出现重定向，禁止框架自动处理重定向（即不follow重定向的url，直接返回重定向前的response），
该响应状态码是302的response通过下载器中间完成,进入spider中间件处理，由于302被增加为可以处理的响应，正常调用callback指定的解析函数。

### 1种特殊情况（目测，这是个巨坑！！）
1.
示例代码：<br>
```
    def start_requests(self):
        yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result,dont_filter=True)  #dont_filter=True告诉scheduler不进行重复请求过滤
        yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result,dont_filter=True)  
        yield scrapy.Request(url=*self.base_url*,headers=self.headers,callback=self.parse_CityInfo,dont_filter=True)

def parse_CityInfo(self,response):
    yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result,dont_filter=True)  

def parse_result(self,response):
    print(response.status)
```
调试信息：<br>
![2018-03-06-scrapy关闭自动重定向及关闭指纹识别_4.jpg]()
![2018-03-06-scrapy关闭自动重定向及关闭指纹识别_5.jpg]()
>start_reqeusts中调用parse_CityInfo的请求url与parse_CityInfo生成的请求是同一个url，使用dont_filter=True参数，关闭url指纹去重，start_requests方法中生成的多个重复请求，调试信息中都能看到有重定向，设置dont_redirect、handle_httpstatus_list参数也可以正常作用与下载器中间件、spider中间件。
但是，parse_CityInfo中生成的重复请求，调试信息中看不到重定向信息，直接返回一个200的响应，且dont_redirect、handle_httpstatus_list参数不起作用。<br>

2.
示例代码：<br>
```
    def start_requests(self):
        yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result,dont_filter=True)  #dont_filter=True告诉scheduler不进行重复请求过滤
        yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result,dont_filter=True)  
        yield scrapy.Request(url=*'http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#std:setting-SPIDER_MIDDLEWARES_BASE'*,headers=self.headers,callback=self.parse_CityInfo,dont_filter=True)

def parse_CityInfo(self,response):
    yield scrapy.Request(url=self.base_url,headers=self.headers,callback=self.parse_result,dont_filter=True)  

def parse_result(self,response):
    print(response.status)
```
调试信息：<br>
![2018-03-06-scrapy关闭自动重定向及关闭指纹识别_5.jpg]()
>这个例子跟上例的区别在于，start_reqeusts中调用parse_CityInfo的请求与parse_CityInfo生成的请求不是同样的url，此时，parse_CityInfo中生成的请求在调试信息中也
看到了302跳转，dont_redirect、handle_httpstatus_list参数也能正常作用于下载器/spider中间件。

>个人猜测：像本例这样，parse_CityInfo方法中生成的请求的url与调用该方法的请求（start_reqeusts方法中的scrapy.Request(..,callbak=self.parse_CityInfo)）的url
相同，parse_CityInfo中的请求直接拿scrapy.Request(..,callbak=self.parse_CityInfo)这个请求的结果作为这个请求的response。<br>
https://map.baidu.com/mobile/webapp/search/search/qt=s&wd=%E5%8D%97%E4%BA%AC%E5%B8%82&c=131&searchFlag=bigBox&version=5&exptype=dep&src_from=webapp_all_bigbox&sug_forward=&src=0/vt=/?pagelets%5B%5D=pager&pagelets%5B%5D=page_data&t=110673<br>
https://map.baidu.com/mobile/webapp/place/list/qt=s&wd=%E5%8C%BB%E7%96%97%E7%BE%8E%E5%AE%B9&c=315&searchFlag=bigBox&version=5&exptype=dep&src_from=webapp_all_bigbox&sug_forward=&src=0&pn=99&rn=10&res_x=0.000000&res_y=0.000000/vt=&showall=1<br>
上面这两个，域名相同的url，也存在同样的问题。
由于没找到相关介绍文档，只能做这样的猜测，希望熟悉这部分原理的朋好友指正！


### 转载他人文章
下文转自[http://www.q2zy.com/articles/2015/12/15/note-of-scrapy/](http://www.q2zy.com/articles/2015/12/15/note-of-scrapy/)

#### 过滤重复 URL
作为爬虫框架的代表，scrapy 在去重上自然也是有考虑的， 在一次爬虫运行过程中，scrapy 会自动对请求的所有地址做去重处理，保证同一个 URL 不会被重复访问多次， 一方面是减小了无意义的开销，另一方面更是避免了超链接连成环导致死循环。

然而，这次要爬的某网站的设计比较奇葩：

某内容页面 URL 为 http://domain/path/{id}，这个 URL 本身没有什么问题， 但是这个 URL 访问之后其实是会设置一个 cookie，然后重定向到一个固定网址，然后那个网址会根据 cookie 来给出返回的内容。

这种设计不得不说真还是第一次见，当 scrapy 遇到这种设计的时候， 由于被重定向到的目标网址都是同一个 URL，而 scrapy 遇到重定向的时候实际上是重发一个新请求到新的目标地址，然后这时候去重的判断就会把重发的所有的请求全部认为是重复请求了。 这个问题本身其实并不难解决，但是由于对于重定向的请求去重的时候，scrapy 并没有给出任何日志提醒，所以导致最开始一直找不到原因。 知道原因后，其实很容易搜到，比如 stack overflow 上[这篇](http://stackoverflow.com/questions/18928253/why-is-my-scrapy-spider-not-following-the-request-callback-in-my-item-parse-func)。

解决方案也就是在构造 Request 对象时增加一个参数 dont_filter=True，从而对此请求禁用掉调度器中的过滤。

#### 返回状态码检查
默认情况下，scrapy 会认为返回状态码 2xx 是成功的 http 请求，而 301、302 会做重定向， 而我要爬的这个网站大概是做了个频率限制（抑或是它太渣？），当频率过高时会给个 302 重定向到 error 页面，而 error 页面给的是 200。 也就是说，当 error 出现的时候，scrapy 会 follow 302，然后认为请求成功了， 所以这里我们需要首先对这个请求禁掉 302 follow，然后将 302 加入成功的 http 请求状态码范围。

禁掉 302 的话，在构造 Request 的时候，meta 参数中需要增加一项 'dont_redirect': True， 然后设置成功请求码范围的话，在 stack overflow 上也可以搜到[这篇](http://stackoverflow.com/questions/9698372/scrapy-and-response-status-code-how-to-check-against-it)， 即在爬虫类中加入属性 handle_httpstatus_list = [302]。
   
#### Ending
最后不得不说，用了框架之后，确实是很多事情上都方便了很多， 印象比较深的比如这个[自动频率限制](http://doc.scrapy.org/en/1.0/topics/autothrottle.html)确实很好使，开了之后就基本没被刚说的那个有频率限制的网站报错了。 不过由于对于框架的不熟悉，所以有时候很简单的一个问题可能会导致查起问题来很麻烦， 好在这代价大多数时候都还是可以忍受的。

虽然现在挺喜欢 scrapy，但不得不承认，遇到 js 动态生成的东西，目测就要歇菜了，大概还是得靠 phantomjs 这种东西了， 但 python 才是真爱啊，不到不得已，干嘛要用 js 去折腾自己呢！
