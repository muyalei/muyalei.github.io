---
layout: default
author: muyalei
date: 2018-12-11
title: hive安装
tags:
   - hive相关
---


***整理自[https://segmentfault.com/a/1190000011303459](https://segmentfault.com/a/1190000011303459)***


# Apache Hive-2.3.0 快速搭建与使用

## Hive 简介
******
Hive 是一个基于 hadoop 的开源数据仓库工具，用于存储和处理海量结构化数据。它把海量数据存储于 hadoop 文件系统，而不是数据库，但提供了一套类数据库的数据存储和处理机制，并采用 HQL （类 SQL ）语言对这些数据进行自动化管理和处理。我们可以把 Hive 中海量结构化数据看成一个个的表，而实际上这些数据是分布式存储在 HDFS 中的。 Hive 经过对语句进行解析和转换，最终生成一系列基于 hadoop 的 map/reduce 任务，通过执行这些任务完成数据处理。

Hive 诞生于 facebook 的日志分析需求，面对海量的结构化数据， Hive 以较低的成本完成了以往需要大规模数据库才能完成的任务，并且学习门槛相对较低，应用开发灵活而高效。

Hive 自 2009.4.29 发布第一个官方稳定版 0.3.0 至今，不过一年的时间，正在慢慢完善，网上能找到的相关资料相当少，尤其中文资料更少，本文结合业务对 Hive 的应用做了一些探索，并把这些经验做一个总结，所谓前车之鉴，希望读者能少走一些弯路。

## 准备工作
******
### 环境
```
JDK:1.8  
Hadoop Release:2.7.4  
centos:7.3  

node1（master）  主机: 192.168.252.121  
node2（slave1）  从机: 192.168.252.122  
node3（slave2）  从机: 192.168.252.123  

node4（mysql）   从机: 192.168.252.124  
```
### 依赖环境

安装Apache Hive前提是要先安装hadoop集群，并且hive只需要在hadoop的namenode节点集群里安装即可（需要在有的namenode上安装)，可以不在datanode节点的机器上安装。还需要说明的是，虽然修改配置文件并不需要把hadoop运行起来，但是本文中用到了hadoop的hdfs命令，在执行这些命令时你必须确保hadoop是正在运行着的，而且启动hive的前提也需要hadoop在正常运行着，所以建议先把hadoop集群启动起来。

安装MySQL 用于存储 Hive 的元数据（也可以用 Hive 自带的嵌入式数据库 Derby，但是 Hive 的生产环境一般不用 Derby），这里只需要安装 MySQL 单机版即可，如果想保证高可用的化，也可以部署 MySQL 主从模式；

**Hadoop**
 
[Hadoop-2.7.4 集群快速搭建](https://segmentfault.com/a/1190000011266759)

**MySQL** 随意任选其一

[CentOs7.3 安装 MySQL 5.7.19 二进制版本](https://segmentfault.com/a/1190000010864818)
 
[搭建 MySQL 5.7.19 主从复制，以及复制实现细节分析](https://segmentfault.com/a/1190000010867488)

注：也可以使用mariadb，实测hadoop2.8、mariadb根据该教程安装hive也成功了。

## 安装
******

### 下载解压
```
su hadoop
cd /home/hadoop/
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-2.3.0/apache-hive-2.3.0-bin.tar.gz
tar -zxvf apache-hive-2.3.0-bin.tar.gz
mv apache-hive-2.3.0-bin hive-2.3.0
```
注：该下载地址已经过期，可以使用[这个地址](http://mirror.bit.edu.cn/apache/hive/hive-2.3.4/

)
