---
layout: default
author: muyalei
date: 2018-12-21
title: sqool使用
tags:
   - sqoop相关
---


***整理自[http://shiyanjun.cn/archives/624.html](http://shiyanjun.cn/archives/624.html)、[http://www.cnblogs.com/xuyou551/p/7991973.html](http://www.cnblogs.com/xuyou551/p/7991973.html)、[https://blog.csdn.net/u013850277/article/details/78808631](https://blog.csdn.net/u013850277/article/details/78808631)***

## 简介：

Sqoop可以在HDFS/Hive和关系型数据库之间进行数据的导入导出，其中主要使用了import和export这两个工具。这两个工具非常强大，提供了很多选项帮助我们完成数据的迁移和同步。比如，下面两个潜在的需求：
  1. 业务数据存放在关系数据库中，如果数据量达到一定规模后需要对其进行分析或同统计，单纯使用关系数据库可能会成为瓶颈，这时可以将数据从业务数据库数据导入（import）到Hadoop平台进行离线分析。
  2. 对大规模的数据在Hadoop平台上进行分析以后，可能需要将结果同步到关系数据库中作为业务的辅助数据，这时候需要将Hadoop平台分析后的数据导出（export）到关系数据库。


## 使用

1. 将mysql数据导入hdfs

命令：
```
sqoop import --connect jdbc:mysql://host:port/db --username xx --password xx --table xx --target-dir xx
```

2. 将mysql数据导入hive

命令：
```
sqoop import --connect jdbc:mysql://host:port/db --username xx --password xx --table --hive-import --hive-table xx
```
*注意：*
   * 如果这个表没有主键，需要加 `-m 1` 参数，将其运行在一个map中。
   * 关于-m参数更详细的解释：sqoop导入mysql表时，没有指定map数量时，会并行导入。并行导入时会根据表的主键类型选着不同的Splitter，参见DataDrivenDBInputFormat的getSplitter方法，TextSplitter会现将string转BigDecimal再转string ，这个过程可能会产生乱码和特殊字符，导致后期的sql语法错误。<br/>
   建议：
    * 导入中不要选用String类型的主键切分map个数 
    * 导入小表时指定-m 1，单map 即节省资源，又防止并行切分字符串主键问题
   *参考自[https://blog.csdn.net/MuQianHuanHuoZhe/article/details/80585672](https://blog.csdn.net/MuQianHuanHuoZhe/article/details/80585672)*

3. 执行导入Hive的命令（如果Hive中没有存在对应的hive表，则会依据mysql 的表来创建对应的表，字段属性也跟mysql的一致）
   * 这张表不存在的情况下（默认会自动创建对应的Hive表并全量将数据加载进去）
   * 这张表存在的情况下（默认往表中追加数据）

4. 复杂查询条件导入
  * where参数的使用<br/>
  举例：--where " SXQMC='广东省广州市萝岗区' and XZJDMC='九龙镇' and BMC='女' and S_LAST_UPDATED > '2018-01-04 03:10:13'  and  S_LAST_UPDATED < '2018-01-04 03:21:00' " <br/>
  * query参数的使用<br/>
  举例：--query " select * from test1  where SXQMC='广东省广州市番禺区' and BMC='女' and S_LAST_UPDATED > '2018-01-04 03:10:13'  and  S_LAST_UPDATED < '2018-01-04 03:21:00'  AND \$CONDITIONS" 

5. sqoop分隔符问题<br/>
  在将mysql 中的数据导入到hive中，mysql 中的数据如下:<br/>
  ![2018-12-21-sqoop使用_图片1.png]()

  XH=1在mysql中这是一条数据，但是数据对应的某一列数据有换行符。<br/>
  在进行sqoop import 数据时，如果不加其他参数，导入的数据默认的列分隔符是’\001’，默认的行分隔符是’\n’。也就像下面的数据那样，在导入时出现换行符时hive以为这条数据已经结束，并将其后面输入的数据当做另一条数据。<br/> 
  因而hive 默认会解析成两条数据，这样就造成导入数据时出现了数据跟原表不一致的问题。如下图所示：<br/>
  ![2018-12-21-sqoop使用_图片2.png]()
  
  解决方法：<br/>
  加上参数–hive-drop-import-delims来把导入数据中包含的hive默认的分隔符去掉 <br/>
    

6. 常用参数<br/>
  *详情参考[http://shiyanjun.cn/archives/624.html](http://shiyanjun.cn/archives/624.html)*

  * import和export工具有些通用的选项，如下所示<br/>
  选项	                                     含义说明<br/>
  --connect <jdbc-uri>	                 指定JDBC连接字符串<br/>
  --connection-manager <class-name>	     指定要使用的连接管理器类<br/>
  --driver <class-name>	                 指定要使用的JDBC驱动类<br/>
  --hadoop-mapred-home <dir>	         指定$HADOOP_MAPRED_HOME路径<br/>
  --help	                             打印用法帮助信息<br/>
  --password-file	                     设置用于存放认证的密码信息文件的路径<br/>
  -P	                                 从控制台读取输入的密码<br/>
  --password <password>	                 设置认证密码<br/>
  --username <username>	                 设置认证用户名<br/>
  --verbose	                             打印详细的运行信息<br/>
  --connection-param-file <filename>	 可选，指定存储数据库连接参数的属性文件<br/>

  * 数据导入工具import<br/>
  import工具，是将HDFS平台外部的结构化存储系统中的数据导入到Hadoop平台，便于后续分析。我们先看一下import工具的基本选项及其含义，如下所示：<br/>
  选项	                              含义说明 <br/>
  --append	                       将数据追加到HDFS上一个已存在的数据集上<br/>
  --as-avrodatafile	               将数据导入到Avro数据文件<br/>
  --as-sequencefile	               将数据导入到SequenceFile<br/>
  --as-textfile	                   将数据导入到普通文本文件（默认）<br/>
  --boundary-query <statement>	   边界查询，用于创建分片（InputSplit）<br/>
  --columns <col,col,col…>	       从表中导出指定的一组列的数据<br/>
  --delete-target-dir	           如果指定目录存在，则先删除掉<br/>
  --direct	                       使用直接导入模式（优化导入速度）<br/>
  --direct-split-size <n>	       分割输入stream的字节大小（在直接导入模式下）<br/>
  --fetch-size <n>	               从数据库中批量读取记录数<br/>
  --inline-lob-limit <n>	       设置内联的LOB对象的大小<br/>
  -m,--num-mappers <n>	           使用n个map任务并行导入数据<br/>
  -e,--query <statement>           导入的查询语句<br/>
  --split-by <column-name>	       指定按照哪个列去分割数据<br/>
  --table <table-name>	           导入的源表表名<br/>
  --target-dir <dir>	           导入HDFS的目标路径<br/>
  --warehouse-dir <dir>	           HDFS存放表的根路径<br/>
  --where <where clause>	       指定导出时所使用的查询条件<br/>
  -z,--compress	                   启用压缩<br/>
  --compression-codec <c>	       指定Hadoop的codec方式（默认gzip）<br/>
  --null-string <null-string>	   果指定列为字符串类型，使用指定字符串替换值为null的该类列的值<br/>
  --null-non-string <null-string>  如果指定列为非字符串类型，使用指定字符串替换值为null的该类列的值<br/>
  
  













