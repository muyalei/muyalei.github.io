---
layout: default
author: muyalei
title: spark2.4.4安装
date: 2019-10-14
tags:
  - spark相关
---

***整理自[http://dblab.xmu.edu.cn/blog/1689-2/](http://dblab.xmu.edu.cn/blog/1689-2/)***

结合另一篇hadoop2.8.1的安装笔记，本机最后安装的是hadoop2.8.1+spark2.4.4的环境


在Linux系统中打开浏览器，访问[Spark官方下载地址](http://spark.apache.org/downloads.html)，按照如下图下载：

![https://github.com/muyalei/muyalei.github.io/blob/gh-pages/img/2019-10-14-spark2.4.4%E5%AE%89%E8%A3%85_%E5%9B%BE%E7%89%87_1.png](https://github.com/muyalei/muyalei.github.io/blob/gh-pages/img/2019-10-14-spark2.4.4%E5%AE%89%E8%A3%85_%E5%9B%BE%E7%89%87_1.png)

由于我们已经自己安装了Hadoop，所以，在“Choose a package type”后面需要选择“Pre-build with user-provided Hadoop [can use with most Hadoop distributions]”，然后，点击“Download Spark”后面的“spark-2.1.0-bin-without-hadoop.tgz”下载即可。下载的文件，默认会被浏览器保存在“/home/hadoop/下载”目录下。需要说明的是，Pre-build with user-provided Hadoop: 属于“Hadoop free”版，这样，下载到的Spark，可应用到任意Hadoop 版本。

Spark部署模式主要有四种：Local模式（单机模式）、Standalone模式（使用Spark自带的简单集群管理器）、YARN模式（使用YARN作为集群管理器）和Mesos模式（使用Mesos作为集群管理器）。
这里介绍Local模式（单机模式）的 Spark安装。我们选择Spark 2.1.0版本，并且假设当前使用用户名hadoop登录了Linux操作系统。
```
sudo tar -zxf ~/下载/spark-2.1.0-bin-without-hadoop.tgz -C /usr/local/
cd /usr/local
sudo mv ./spark-2.1.0-bin-without-hadoop/ ./spark
sudo chown -R hadoop:hadoop ./spark          # 此处的 hadoop 为你的用户名
```
安装后，还需要修改Spark的配置文件spark-env.sh
```
cd /usr/local/spark
cp ./conf/spark-env.sh.template ./conf/spark-env.sh
```
编辑spark-env.sh文件(vim ./conf/spark-env.sh)，在第一行添加以下配置信息:
```
export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
```
有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。

然后通过如下命令，修改环境变量
```
vim ~/.bashrc
```
在.bashrc文件中添加如下内容
```
export JAVA_HOME=/usr/lib/jvm/default-java
export HADOOP_HOME=/usr/local/hadoop
export SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH
export PYSPARK_PYTHON=python3
export PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH
```
PYTHONPATH环境变量主要是为了在Python3中引入pyspark库，PYSPARK_PYTHON变量主要是设置pyspark运行的python版本。
.bashrc中必须包含JAVA_HOME,HADOOP_HOME,SPARK_HOME,PYTHONPATH,PYSPARK_PYTHON,PATH这些环境变量。如果已经设置了这些变量则不需要重新添加设置。
接着还需要让该环境变量生效，执行如下代码：
```
source ~/.bashrc
```
配置完成后就可以直接使用，不需要像Hadoop运行启动命令。
通过运行Spark自带的示例，验证Spark是否安装成功。
```
cd /usr/local/spark
bin/run-example SparkPi
```
执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 grep 命令进行过滤（命令中的 2>&1 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）:
```
bin/run-example SparkPi 2>&1 | grep "Pi is"
```
这里涉及到Linux Shell中管道的知识，详情可以参考Linux Shell中的管道命令
过滤后的运行结果如下图示，可以得到π 的 5 位小数近似值：

![]https://github.com/muyalei/muyalei.github.io/blob/gh-pages/img/2019-10-14-spark2.4.4%E5%AE%89%E8%A3%85_%E5%9B%BE%E7%89%87_2.png()https://github.com/muyalei/muyalei.github.io/blob/gh-pages/img/2019-10-14-spark2.4.4%E5%AE%89%E8%A3%85_%E5%9B%BE%E7%89%87_2.png

