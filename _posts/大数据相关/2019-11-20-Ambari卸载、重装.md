---
layout: default
author: muyalei
title: Ambari卸载、重装
date: 2019-11-20
tags:
    - 大数据
---

***整理自[https://blog.csdn.net/qqqqq2015/article/details/80975520](https://blog.csdn.net/qqqqq2015/article/details/80975520)***


原作者大神：https://blog.csdn.net/wk022/article/details/49278419

背景：系统是centos6.5，内网环境，无法上网

 

1、 通过ambari将集群中的所用组件都关闭，如果关闭不了，直接kill-9 XXX

2、 关闭ambari-server，ambari-agent

ambari-server stop
ambari-agent stop
3、yum删除所有Ambari组件

yum remove -y hadoop_2* hdp-select* ranger_2* zookeeper* bigtop*atlas-metadata* ambari* spark* slide* strom* hive*
以上命令可能不全，执行完一下命令后，再执行

 rpm –qa|grep Ambari版本号
    查看是否还有没有卸载的，如果有，继续通过#yum remove XXX卸载

4，删除postgresql的数据

     postgresql软件卸载后，其数据还保留在硬盘中，需要把这部分数据删除掉，如果不删除掉，重新安装ambari-server后，有可能还应用以前的安装数据，而这些数据时错误数据，所以需要删除掉。

 rm -rf /var/lib/pgsql
5、 删除用户

    ambari安装hadoop集群会创建一些用户，清除集群时有必要清除这些用户，并删除对应的文件夹。这样做可以避免集群运行时出现的文件访问权限错误的问题。总之，Ambari自己创建的东西全部删完，不然的话重新安装的时候会报各种“找不到文件”的错误

sudo userdel oozie
sudo userdel hive
sudo userdel ambari-qa
sudo userdel flume 
sudo userdel hdfs 
sudo userdel knox 
sudo userdel storm 
sudo userdel mapred
sudo userdel hbase 
sudo userdel tez 
sudo userdel zookeeper
sudo userdel kafka 
sudo userdel falcon
sudo userdel sqoop 
sudo userdel yarn 
sudo userdel hcat
sudo userdel atlas
sudo userdel spark
sudo userdel ams
sudo userdel zeppelin
 
sudo rm -rf /home/atlas
sudo rm -rf /home/accumulo
sudo rm -rf /home/hbase
sudo rm -rf /home/hive
sudo rm -rf /home/oozie
sudo rm -rf /home/storm
sudo rm -rf /home/yarn
sudo rm -rf /home/ambari-qa
sudo rm -rf /home/falcon
sudo rm -rf /home/hcat
sudo rm -rf /home/kafka
sudo rm -rf /home/mahout
sudo rm -rf /home/spark
sudo rm -rf /home/tez
sudo rm -rf /home/zookeeper
sudo rm -rf /home/flume
sudo rm -rf /home/hdfs
sudo rm -rf /home/knox
sudo rm -rf /home/mapred
sudo rm -rf /home/sqoop
 
sudo rm -rf /var/lib/ambari*
sudo rm -rf /usr/lib/python2.6/site-packages/ambari_*
sudo rm -rf /usr/lib/python2.6/site-packages/resource_management
sudo rm -rf /usr/lib/ambari-*
 
sudo rm -rf /etc/ambari-*
sudo rm -rf /etc/hadoop
sudo rm -rf /etc/hbase
sudo rm -rf /etc/hive
sudo rm -rf /etc/hive2 
sudo rm -rf /etc/oozie
sudo rm -rf /etc/sqoop 
sudo rm -rf /etc/zookeeper
sudo rm -rf /etc/flume 
sudo rm -rf /etc/storm 
sudo rm -rf /etc/tez_hive2 
sudo rm -rf /etc/spark2 
sudo rm -rf /etc/phoenix 
sudo rm -rf /etc/pig 
sudo rm -rf /etc/hive-hcatalog
sudo rm -rf /etc/tez 
sudo rm -rf /etc/falcon 
sudo rm -rf /etc/knox 
sudo rm -rf /etc/hive-webhcat
sudo rm -rf /etc/kafka 
sudo rm -rf /etc/slider 
sudo rm -rf /etc/storm-slider-client
sudo rm -rf /etc/spark 
 
sudo rm -rf /var/run/spark
sudo rm -rf /var/run/hadoop
sudo rm -rf /var/run/hbase
sudo rm -rf /var/run/zookeeper
sudo rm -rf /var/run/flume
sudo rm -rf /var/run/storm
sudo rm -rf /var/run/webhcat
sudo rm -rf /var/run/hadoop-yarn
sudo rm -rf /var/run/hadoop-mapreduce
sudo rm -rf /var/run/kafka
sudo rm -rf /var/run/hive	
sudo rm -rf /var/run/oozie	
sudo rm -rf /var/run/sqoop	
sudo rm -rf /var/run/hive-hcatalog	
sudo rm -rf /var/run/falcon	
sudo rm -rf /var/run/hadoop-hdfs	
sudo rm -rf /var/run/ambari-metrics-collector
sudo rm -rf /var/run/ambari-metrics-monitor	
sudo rm -rf /var/log/hadoop-hdfs	
sudo rm -rf /var/log/hive-hcatalog
sudo rm -rf /var/log/ambari-metrics-monitor
sudo rm -rf /var/log/hadoop
sudo rm -rf /var/log/hbase
sudo rm -rf /var/log/flume
sudo rm -rf /var/log/sqoop
sudo rm -rf /var/log/ambari-server
sudo rm -rf /var/log/ambari-agent
sudo rm -rf /var/log/storm
sudo rm -rf /var/log/hadoop-yarn
sudo rm -rf /var/log/hadoop-mapreduce
sudo rm -rf /var/log/knox 
sudo rm -rf /var/lib/slider
 
sudo rm -rf /usr/lib/flume
sudo rm -rf /usr/lib/storm
sudo rm -rf /var/lib/hive 
sudo rm -rf /var/lib/oozie
sudo rm -rf /var/lib/flume
sudo rm -rf /var/lib/hadoop-yarn
sudo rm -rf /var/lib/hadoop-mapreduce
sudo rm -rf /var/lib/hadoop-hdfs
sudo rm -rf /var/lib/zookeeper
sudo rm -rf /var/lib/knox 
sudo rm -rf /var/log/hive 
sudo rm -rf /var/log/oozie
sudo rm -rf /var/log/zookeeper
sudo rm -rf /var/log/falcon
sudo rm -rf /var/log/webhcat
sudo rm -rf /var/log/spark
sudo rm -rf /var/tmp/oozie
sudo rm -rf /tmp/ambari-qa
sudo rm -rf /tmp/hive 
sudo rm -rf /var/hadoop
sudo rm -rf /hadoop/falcon
sudo rm -rf /tmp/hadoop 
sudo rm -rf /tmp/hadoop-hdfs
sudo rm -rf /usr/hdp
sudo rm -rf /usr/hadoop
sudo rm -rf /opt/hadoop
sudo rm -rf /tmp/hadoop
sudo rm -rf /var/hadoop
sudo rm -rf /hadoop
 
sudo rm -rf /usr/bin/worker-lanucher
sudo rm -rf /usr/bin/zookeeper-client
sudo rm -rf /usr/bin/zookeeper-server
sudo rm -rf /usr/bin/zookeeper-server-cleanup
sudo rm -rf /usr/bin/yarn 
sudo rm -rf /usr/bin/storm
sudo rm -rf /usr/bin/storm-slider 
sudo rm -rf /usr/bin/worker-lanucher
sudo rm -rf /usr/bin/storm
sudo rm -rf /usr/bin/storm-slider 
sudo rm -rf /usr/bin/sqoop 
sudo rm -rf /usr/bin/sqoop-codegen 
sudo rm -rf /usr/bin/sqoop-create-hive-table 
sudo rm -rf /usr/bin/sqoop-eval 
sudo rm -rf /usr/bin/sqoop-export 
sudo rm -rf /usr/bin/sqoop-help 
sudo rm -rf /usr/bin/sqoop-import 
sudo rm -rf /usr/bin/sqoop-import-all-tables 
sudo rm -rf /usr/bin/sqoop-job 
sudo rm -rf /usr/bin/sqoop-list-databases 
sudo rm -rf /usr/bin/sqoop-list-tables 
sudo rm -rf /usr/bin/sqoop-merge 
sudo rm -rf /usr/bin/sqoop-metastore 
sudo rm -rf /usr/bin/sqoop-version 
sudo rm -rf /usr/bin/slider 
sudo rm -rf /usr/bin/ranger-admin-start 
sudo rm -rf /usr/bin/ranger-admin-stop 
sudo rm -rf /usr/bin/ranger-kms
sudo rm -rf /usr/bin/ranger-usersync-start
sudo rm -rf /usr/bin/ranger-usersync-stop
sudo rm -rf /usr/bin/pig 
sudo rm -rf /usr/bin/phoenix-psql 
sudo rm -rf /usr/bin/phoenix-queryserver 
sudo rm -rf /usr/bin/phoenix-sqlline 
sudo rm -rf /usr/bin/phoenix-sqlline-thin 
sudo rm -rf /usr/bin/oozie 
sudo rm -rf /usr/bin/oozied.sh 
sudo rm -rf /usr/bin/mapred 
sudo rm -rf /usr/bin/mahout 
sudo rm -rf /usr/bin/kafka 
sudo rm -rf /usr/bin/hive 
sudo rm -rf /usr/bin/hiveserver2 
sudo rm -rf /usr/bin/hbase
sudo rm -rf /usr/bin/hcat 
sudo rm -rf /usr/bin/hdfs 
sudo rm -rf /usr/bin/hadoop 
sudo rm -rf /usr/bin/flume-ng 
sudo rm -rf /usr/bin/falcon 
sudo rm -rf /usr/bin/beeline
sudo rm -rf /usr/bin/atlas-start 
sudo rm -rf /usr/bin/atlas-stop 
sudo rm -rf /usr/bin/accumulo
 

6、清理数据库

        删除mysql中ambari库

drop database ambari;
7、通过以上清理后，重新安装ambari和hadoop集群（包括HDFS，YARN+MapReduce2，Zookeeper，AmbariMetrics，Spark）成功。


