Is Rostowski right?
At first sight, such a connection seems far-fetched.
Putin’s show of strength involved military force and the implicit threat of a gas embargo, not monetary power (which he does not have).
Throughout the conflict over Crimea, the focus has been on Ukraine’s relationship with the European Union, not with the eurozone.
Moreover, Ukraine’s recent monetary history has been defined by an exchange-rate peg to the US dollar, not the euro.
So how could the euro be relevant to Russia’s annexation of Crimea?
Rostowski’s point is that European countries demonstrated throughout the euro crisis that they had very little appetite for solidarity, even with their partners in the monetary union.
How much solidarity would they be willing to display vis-à-vis a non-European Union country?
Russia, the reasoning goes, interpreted the EU’s hesitant management of the turmoil as a license to act.
And it could go further for the same reason.
Clearly, the series of events following the financial meltdown in 2008 can be viewed as a crisis of solidarity.
When a common response to Europe’s banking debacle was needed, the answer was that each country should take care of its own financial institutions.
When Greece lost access to financial markets, several months were needed to engineer a response, which took great care not to rely on EU funds and to limit each country’s financial commitment.
Indeed, when a ��firewall” was finally created, its size was strictly limited and no form of joint liability was permitted.
And Eurobonds were quickly rejected, because they would have created open-ended mutualized-debt obligations.
Similarly, though it had been envisaged that the European Stability Mechanism could be used to recapitalize banks, it was eventually decided that the ESM would lend only to governments, rather than assuming bank risk directly.
And, most recently, negotiations to establish an EU banking union once again confronted the challenge of forging a common resolution mechanism while limiting each member state’s commitment.
In short, each time the question of European solidarity was raised, the answer was: “Yes, but only if absolutely necessary, and only to the minimum possible extent.”
Russian reactions to the Ukrainian uprising, meanwhile, have shown how vivid the memory of World War II remains in Moscow.
It is fair to assume that the Kremlin may have noticed that Europe had no wish to emulate the United States and engineer a Marshall Plan of its own.
More generally, Putin may have concluded that an EU that is so reluctant to take risks for the good of its own members would certainly not take risks for a mere neighbor.
A key dimension of the current standoff over Ukraine is energy, and it raises the same question about European solidarity.
As a recent study by Bruegel has shown, the EU as a whole could, with some effort, dispense with gas imports from Russia.
But doing so would require EU member states to regard security of supplies as a matter of common concern, not as an issue that each country must address on its own.
For example, in response to an embargo affecting a particular country, other EU members would draw on their reserves, increase their own production, pay more for imports, or cut consumption a bit.
But this sense of solidarity has been consistently lacking in the EU’s energy-policy debate.
The underlying question is whether it is right to assume that the euro should have created more solidarity.
Those who first imagined the single currency expected it to have profound consequences.
In their eyes, it would be a means to forge a community.
Currency borders generally coincide with political borders, so the creation of a monetary union was expected to give rise to some sort of common polity.
Sharing a currency was expected to create a sense of common destiny, and hence solidarity, among the participants.
That did not happen.
Even before the crisis, it was clear that citizens and governments alike regarded (wrongly) the euro as a mere practicality.
Its introduction was viewed as a technocratic affair, to be handled by central bankers and finance ministers, not as the cornerstone of a common European identity.
Its creation did not cause the EU budget to increase by a single euro, nor did it lead to deeper political integration.
The commitment called for by a common currency was consistently underestimated.
In hindsight, it was a mistake to believe in the euro’s spontaneous community-creating power.
Though there clearly is a link between currency areas and political communities (consider the dissolution of the ruble zone at the time of the Soviet Union’s collapse), it is political community that creates the solidarity needed to foster currency linkages, not vice versa.
Rostowski is certainly right: the euro’s weakness has emboldened Putin.
In the end, however, the right question may be whether the Crimea crisis will eventually bolster European solidarity – and thus the euro.
The New Growth Conundrum
PARIS – For most governments, the rate of economic growth that can reasonably be expected in the coming years is a key question.
And, at least for the advanced economies, it has become a particularly puzzling one.
If the past is a good predictor of the future, the outlook is bleak.
Since 2008, economic growth has consistently disappointed expectations.
Of the countries most affected by the financial crisis, only a few – the United States, Germany, and Sweden – have rediscovered the path to sustained growth.
Yet, even for them, GDP in 2013 was far below the level projected prior to the crisis.
The consensus view among economists and policymakers is that the financial crisis and the euro crisis have damaged both demand and supply, but that a gradual healing process has begun.
On the demand side, according to this view, the hangover from pre-crisis private indebtedness and crisis-generated public indebtedness still weighs on domestic demand.
This is likely to persist for several more years, though the burden will diminish steadily.
Gradually, consumers will start spending and investing again (as is becoming the case in the US), and fiscal policy will become neutral again (as is already the case in Germany).
On the supply side, the crisis has lowered potential output growth, because, in Europe at least, firms have invested less, impeding the adoption of new technologies.
Moreover, in some cases – for example, the United Kingdom – wage decreases and flexible layoff rules have encouraged firms to substitute labor for capital, reducing output per employee.
Clogged capital markets and resistance to social hardship have also delayed the replacement of incumbent firms by more efficient new entrants.
The aggregate result has been lower-than-anticipated productivity: in the UK, more person-hours were needed to produce a unit of output in 2013 than in 2007.
Here, too, the supply-side effect of the crisis is likely to persist until firms invest in new equipment, innovation accelerates, and the churning process in labor markets resumes.
But the view that advanced economies are gradually healing is challenged from both sides.
Starting with demand, Larry Summers, the Harvard economist and senior US official under Presidents Bill Clinton and Barack Obama, recently proposed that advanced economies have found themselves in the grip of secular stagnation.
Summers’s view is that pre-crisis indebtedness was not an exogenous anomaly; it was the consequence of insufficient global demand.
The global distribution of income had shifted away from the advanced countries’ middle class toward the rich and the emerging economies, resulting in excess worldwide savings.
The only way to avoid stagnation was to push the middle class deeper into debt, helped by low interest rates and lenient lending rules.
In other words, the savings glut (as former US Federal Reserve Chairman Ben Bernanke called it) predated the crisis and could continue to affect global demand, unless the emerging countries’ middle class provides the global economy with a new consumer of last resort.
This is likely to happen eventually; but, despite efforts by the US and the International Monetary Fund in the context of the G-20, this rebalancing process has not yet been completed.
The challenge on the supply side stems from a new dispute among economists and technology experts about the pace of technological progress.
For Robert Gordon of Northwestern University, information and communication technologies have already delivered most of the productivity boost that can be expected from them; there is no major innovation wave in sight that could offset the slowdown in potential growth.
Laggards can look forward to reaping catch-up dividends; but countries at the technology frontier should accept that very slow annual per capita growth – little more than 1% – is the new normal.
By contrast, the MIT scholars Erik Brynjolfsson and Andrew McAfee argue that the Second Machine Age is yet to come.
They claim that ever-increasing computing power, worldwide connectivity, and the almost unlimited potential for generating new innovations through recombining existing processes will trigger major transformations in both production and consumption, in the same way that the steam engine transformed the world in the nineteenth century.
Growth should accelerate as a consequence, at least if properly measured.
Combining the challenges cited by Gordon and Summers to the view that advanced economies are gradually healing leads to some depressing conclusions.
If Gordon is right about slow productivity growth, the debt overhang inherited from the crisis and public-finance woes will persist for much longer than anticipated.
If, in addition, Summers is right that demand is bound to remain deficient, the combination of financial troubles and persistent mass unemployment is likely to push governments toward radical solutions – debt default, inflation, or financial protectionism.
If, on the contrary, Brynjolfsson and McAfee are right, growth will be much more robust, and debt issues will be forgotten sooner than expected.
The challenge, instead, will be to cope with the labor-reducing and income-inequality effects of emerging technologies.
That will be especially true if these transformations take place against the background drawn by Summers of persistent mass unemployment.
The risk is that social problems become intractable, as technological advances come to be regarded as benefiting the rich while creating additional hardship for the masses.
In such a scenario, governments will have to look for innovative responses.
Scenarios like these may seem far-fetched.
But, while certainly discomfiting, they are hardly irrelevant.
The Price of Scottish Independence
NEW YORK – Though the world’s eyes now are on Scotland’s referendum on independence from the United Kingdom, Scotland is not alone in seeking to redraw national boundaries.
There are independence movements in many other parts of the world; indeed, 39 new states have joined the United Nations since 1980.
Many more aspirants are waiting in the wings, and would likely be encouraged by a Scottish “Yes” vote.
The Scottish pro-independence campaign is based on four claims.
The first is cultural: to protect and strengthen the identity of the Scottish people.
The second is ideological: to move Scotland toward a Scandinavian-style social democracy.
The third is political: to bring democratic governance closer to the people.
And the fourth is economic: to lay claim to a larger share of North Sea oil and gas.
UK political leaders and many European governments are strongly urging the Scots to vote against independence.
Scottish independence, the “No” campaign argues, would bring few if any of the claimed benefits; on the contrary, it would cause many economic calamities, ranging from financial panics to the flight of jobs and industry from Scotland.
Moreover, an independent Scotland might be excluded from the European Union and NATO.
What should the rest of the world think about this debate?
Should the Scottish independence campaign be hailed as a breakthrough for claims to cultural identity&nbsp;and self-governance?
Or should it be viewed as yet another source of instability and weakness in Europe – one that would increase uncertainty in other countries and parts of the world?
Secession movements can, no doubt, cause great instability.
Consider the regional and even global turmoil over Kosovo, South Sudan, Kurdistan, and Crimea.
Yet national independence can also be handled peacefully and smoothly.
The 1993 division of Czechoslovakia into the Czech Republic and Slovakia – the famed “velvet divorce” – imposed no significant or lasting costs on either successor state.
Both accepted the division, and, knowing that their future lay within the EU, focused their attention on accession.
Here, then, is a plausible and positive scenario for an independent Scotland.
The rest of the UK (called the “RUK” in the current debate), including England, Wales, and Northern Ireland, would quickly and efficiently negotiate the terms of independence with Scotland, agreeing how to share the UK’s public debt and public assets, including offshore oil and gas. Both sides would be pragmatic and moderate in their demands.
At the same time, the EU would agree immediately to Scotland’s continued membership, given that Scotland already abides by all of the required laws and democratic standards.
Similarly, NATO would agree immediately to keep Scotland in the Alliance (though the Scottish National Party’s pledge to close US and British nuclear-submarine bases would be a complication to be overcome).
Both Scotland and the RUK might agree that Scotland would temporarily keep the British pound but would move to a new Scottish pound or the euro.
If such monetary arrangements are transparent and cooperatively drawn, they could occur smoothly and without financial turmoil.
But if the RUK, the EU, and NATO respond vindictively to a Yes vote – whether to teach Scotland a lesson or to deter others (such as Catalonia) – matters could become very ugly and very costly.
Suppose that a newly independent Scotland is thrown out of the EU and NATO, and told that it will remain outside for years to come.
In this scenario, a financial panic could indeed be provoked, and both Scotland and the RUK would suffer economically.
The key point is that the costs of separation are a matter of choice, not of inevitability.
They would depend mainly on how the RUK, the EU, and NATO decided to respond to a Yes vote, and how moderate a newly independent Scotland would be in its negotiating positions.
If cool heads prevail, Scottish independence could proceed at a relatively low cost.
The dangers of national secession are much greater in places without overarching entities like the EU and NATO to constrain the situation among the successor states.
In such circumstances, unilateral claims of independence opposed by the national government or a sub-national unit often lead to a breakdown of trade and finance – and often to outright war, as we saw in the breakup of the Soviet Union, Yugoslavia, and most recently, Sudan.
In those cases, separation was indeed followed by deep economic and political crises, which in some ways persist.
Indeed, in the case of ex-Yugoslavia and the former Soviet Union, the EU and NATO absorbed some but not all of the successor states, thereby raising major geopolitical tensions.
International politics in the twenty-first century can no longer be about nation-states alone.
Most key issues that are vital for national wellbeing – trade, finance, the rule of law, security, and the physical environment – depend at least as much on the presence of effective regional and global institutions.
Even if Scotland declares independence, it will – and should continue to be – bound by a dense web of European and global rules and responsibilities.
I am personally sympathetic to Scotland’s independence as a way to bolster Scottish democracy and cultural identity.
Yet I support independence only on the assumption that Scotland and the RUK would remain part of a strong and effective EU and NATO.
Certainly, a Yes vote would put an even higher premium on effective EU governance.
But, if the EU and NATO were to “punish” a newly independent Scotland by excluding it, real disaster could ensue, not only for Scotland and the UK, but also for European democracy and security.
Cities and Sustainable Development
NEW YORK – Tacloban in the Philippines has now joined the growing list of cities – including New Orleans, Bangkok, Moscow, New York, Beijing, Rio de Janeiro, and Port-au-Prince, to name just a few – pummeled in recent years by climate catastrophes.
Many of the world’s largest cities, built on seacoasts and rivers, face the threat of rising sea levels and intensifying storms.
So the new global development agenda now taking shape should empower cities to help lead the way to sustainable development in the twenty-first century.
The importance of cities in today’s world economy is unprecedented.
Until the Industrial Revolution, human history was overwhelmingly rural.
Only around 10% of people lived in cities.
Today, the share of urbanites is around 53% and is likely to rise to around 67% by 2050.
Because per capita incomes are higher in cities than in rural areas, the world’s cities today are estimated to account for more than 80% of global income, with the largest 600 accounting for around half.
Most of the new jobs over the next few decades will be created in cities, offering livelihoods to hundreds of millions of young people and, as China and Brazil have demonstrated, helping to slash extreme poverty.
Cities are also the innovation hubs for public policy.
Every day, mayors are called on to get the job done for residents.
They are the ones responsible for providing safe water, garbage collection, safe housing, infrastructure, upgraded slums, protection from disasters, and emergency services when catastrophes hit.
So it is not surprising that while national governments often are paralyzed by partisan politics, city governments foster action and innovation.
In the United States, for example, Martin O’Malley, Baltimore’s former mayor and now Maryland’s popular governor, pioneered the use of advanced information systems for urban management.
New York City’s outgoing mayor, Michael Bloomberg, worked relentlessly to implement a new sustainability plan (called PlaNYC).
And the city’s incoming mayor, Bill de Blasio, is championing a bold program of educational innovations to narrow the vast gaps in income, wealth, and opportunity that divide the city.
Sustainable development offers a new concept for the world economy in the twenty-first century.
Rather than focusing solely on income, sustainable development encourages cities, countries, and the world to focus simultaneously on three goals: economic prosperity, social inclusion, and environmental sustainability.
Economic prosperity speaks for itself.
Social inclusion means that all members of society – rich and poor, men and women, majority and minority groups – should have equal rights and equal opportunities to benefit from rising prosperity.
And environmental sustainability means that we must reorient our economies and technologies to provide basic services like safe water and sanitation, combat human-induced climate change, and protect biodiversity.
Achieving these three goals will require good governance, public finance, and effective institutions.
Cities will be in the front lines of the battle for sustainable development.
Not only do they face direct threats; they also have the best opportunities to identify and deliver solutions.
As high-density, high-productivity settlements, cities can provide greater access to services of all kinds – including energy, water, health, education, finance, media, transport, recycling, and research – than can most rural areas.
The great challenge for cities is to provide this access inclusively and sustainably.
A significant part of the solution will come through advanced technologies, including information systems and materials science.
The information and communications revolution has spawned the idea of the “smart city,” which places the relevant technologies at the heart of systems that collect and respond to information: smart power grids, smart transport networks (potentially including self-driving vehicles), and smart buildings and zoning.
The advances in materials science open the possibility of much more energy-efficient residences and commercial buildings.
Cities also give rise to the opportunity to combine public utilities, as when urban power plants use the steam released in electricity generation to provide hot water and heating to residents.
Yet technology will be only part of the story.
Cities need to upgrade their governance, to allow for a greater role for poorer and more marginalized communities, and to enable much more effective coordination across city lines when a metropolitan area is home to many individual cities.
Metropolitan governance is therefore crucial, as smart cities require networks that operate at the metropolitan scale.
When the metropolitan scale is recognized, the importance of leading urban areas is even more remarkable.
New York City has around 8.4 million people, but the NYC metropolitan area has roughly 25 million people, with an economy estimated at about $1.4 trillion per year.
If this metropolitan area were a country, it would rank about 14th in the world in GDP terms.
A wise political doctrine known as subsidiarity holds that public-policy challenges should be assigned to the lowest level of government able to address them, thereby ensuring maximum democratic participation in problem solving and the greatest opportunity to tailor solutions to genuine local needs.
While some issues – for example, a national highway or rail system – require national-level problem solving, many key challenges of sustainable development are best confronted at the urban level.
The world’s governments are now negotiating the Sustainable Development Goals, which will guide the world’s development agenda from 2015 to 2030.
In an important meeting on September 25, the United Nations General Assembly agreed that the SDGs would be adopted at a global summit in September 2015, with the next two years used to select the priorities.
An urban SDG, promoting inclusive, productive, and resilient cities, would greatly empower tens of thousands of cities worldwide to take up the cause of sustainable development for their own citizens, their countries, and the world.
Why Bill Gates Gets It Wrong
NEW YORK – In his review of Nina Munk’s error-filled and out-of-date book, Bill Gates oddly abandons the rigorous approach to measurement and evaluation that defines his foundation’s invaluable work.
He simply accepts Munk’s assertion that the Millennium Villages Project – an ongoing development project across more than 20 African countries – has failed.
In fact, it is flourishing.
This credulousness is puzzling.
Munk’s book covers only a sliver of the first half of a ten-year project, and only two of 12 villages.
And she never “lived for extended periods in the Millennium Villages.”
Munk spent an average of around six days per year – around 36 days over six years – actually visiting the villages, and usually at a stretch of 2-3 days.
Moreover, she came to the story as a reporter for the magazine Vanity Fair, with no training or experience in public health, agronomy, economics, or African development.
Worse, Munk’s observations frequently seem to have been, at the very least, greatly exaggerated for narrative effect.
Does Bill Gates really believe that I advocated specific crops without worrying about whether there was a market for them, or that I failed to consider national taxation in my ongoing advice to government leaders?
Moreover, the agricultural strategies and choices in the MVP have been led by African agronomists, some of the very best in Africa – often working hand in hand with Bill’s own agricultural staff in the Alliance for a Green Revolution in Africa (AGRA).
Bill will be happy to know that the MVP will be properly and professionally evaluated next year – on time at its conclusion (and at the end of the Millennium Development Goals in 2015).
The assessment will be based on the very considerable data that have been collected over the past decade, and on extensive new survey data that will be collected in 2015.
Moreover, the evaluation will include comparisons with areas surrounding the Millennium Villages.
In fact, I hope that the Bill & Melinda Gates Foundation will help to carry out the detailed, independently supervised survey work needed for a full evaluation of this complex project.
Let me provide some more good news, based on the detailed data on community health delivery, morbidity (disease), and mortality that the MVP collects each month.
Mortality rates are down sharply in the Millennium Villages.
In fact, the current evidence, to be examined in greater detail next year, suggests that the bold goal of reducing under-five mortality rates to below 30 deaths per 1,000 births has been achieved or is within reach by 2015, and at a remarkably low cost to the health system.
Recently, one of the Gates Foundation’s senior staff members visited the Millennium Village site in northern Nigeria.
Afterwards, he confirmed to me personally that he and his team were deeply impressed by what they saw of the Millennium Village health system in operation.
So let me take this opportunity to reiterate a challenge that I have posed to Bill.
He can pick any district in rural Africa, and our team will work with the local communities using the Millennium Village health approach to reduce the under-five mortality rate to below 30/1,000 – a rate characteristic of many middle-income countries – at an annual health-sector cost of just $60 per person.
And we will do it in five years or less.
That success, I believe, would help Bill and others to recognize the remarkable value of investing in low-cost rural health systems that follow the design principles of the Millennium Village Project.
Finally, given concerns, shared by Bill, about the MVP’s sustainability and scalability, it is no small matter that host governments are strong advocates of the approach.
These governments’ leaders have seen the Millennium Villages day in and day out over almost a decade.
They are putting their own money and policies behind expanded implementation of the MVP’s guiding concepts.
For example, Nigeria has used the MVP concepts for national-scale delivery of health and education services in all 774 of the country’s Local Government Areas.
Governments across the region have taken over $100 million in financing from the Islamic Development Bank to scale up the MVP concepts themselves.
Around a dozen countries are now starting or have approached the Millennium Village Project to help them start their own Millennium Villages.
And the Pan African Youth Leadership Network, Africa’s own young people, recently visited the Millennium Village in Senegal, and requested the support of the MVP to expand the Millennium Village Project’s techniques and strategies in their home countries and regions.
This spread of the Millennium Village approach throughout Africa shows that African political and community leaders consider the MVP’s methods, strategies, and systems to be highly useful in combating poverty in rural Africa.
Nina Munk’s book is out of date and misses the mark.
I invite Bill Gates to visit one or more Millennium Village sites on an upcoming trip to Africa to see first hand why the approach is of such practical interest across the continent.
Winning the Fight Against Killer Diseases
MAPUTO – One of the greatest successes in development aid in the past decade has been the Global Fund to Fight AIDS, Tuberculosis, and Malaria.
The Global Fund has saved millions of lives and helped countries around the world beat back three epidemic diseases.
Now it is appealing to the world’s governments and the private sector for another three years of funding, with governments set to decide on further financing in early December in Washington, DC.
Back in 2000, the HIV/AIDS epidemic was devastating the world’s poorest countries, especially in Africa.
New antiretroviral medicines had been developed and were being used in rich countries, but were too expensive for the poorest.
Millions of poor people were dying of AIDS, even though the new medicines could have kept them alive.
Two other major killer diseases, malaria and TB, were also resurgent.
A frightening new epidemic of multi-drug-resistant TB – or MDR-TB, which was much harder to treat than mainstream TB – had erupted.
There were also cases of extreme MDR-TB, soon called XDR-TB, which resisted even the back-up medicines.
Back in 2000, the rich countries were not taking adequate steps to fight AIDS, TB, and malaria. Aid flows were tiny.
At the time, I had recently been appointed by the Director-General of the World Health Organization (WHO) to help bring finance ministers and health ministers together to see what could be done, both immediately and in the longer term.
Our advisory group, known as the Commission on Macroeconomics and Health, recommended that rich countries scale up their health-care aid to poor countries, including efforts to fight AIDS, TB, and malaria.
This aid would save lives, improve well-being, and help strengthen economic development.
Former Norwegian Prime Minister Gro Harlem Brundtland, the remarkable director-general of the WHO at the time, strongly supported this recommendation.
At the international AIDS conference in Durban, South Africa, in July 2000, I described why a new global fund was needed to fight AIDS.
In early 2001, former United Nations Secretary General Kofi Annan launched a powerful and persuasive appeal to establish the Global Fund.
Leaders around the world responded to Annan’s call; within months, the Global Fund was born.
I remember those days vividly.
In international public-health circles, there was great excitement.
Yet there was also frustration and bewilderment, as vocal opponents of foreign aid began to oppose the increased funding for disease control.
Several economists with little knowledge of public health became outspoken opponents.
They argued on the basis of free-market ideology rather than evidence, claiming that foreign aid always fails.
Fortunately, world leaders listened to public-health specialists and not to the aid skeptics.
US President George W. Bush’s administration provided strong and important support for the Global Fund – and also created new US programs to fight AIDS and malaria.
By the second half of the 2000’s, programs to fight the three main killer communicable diseases were scaling up around the world.
Over the objection of the aid skeptics, the Global Fund provided financial support for massive free distribution of bed nets, diagnostics, and medicines to address malaria.
Lo and behold, for the first time in a generation, deaths from malaria in Africa began to fall (steeply in some places).
Hundreds of thousands of lives, mainly African children, were now being saved every year.
Children were spared not only death but also debilitating infections, enabling them to attend school and lead more productive lives in the future.
The same thing happened with HIV/AIDS and TB.
Back in 2000, before the Global Fund was established, infected people in developing countries died of AIDS without any chance to receive life-saving antiretroviral medicines.
By 2010, more than six million people in developing countries were receiving antiretroviral treatment.
Similarly, testing and treatment for TB rose sharply, including a strong increase in several heavily affected Asian countries.
The aid skeptics were proved wrong.
Aid for health has worked.
The world has benefited enormously from the triumph of generosity, professionalism, common decency, and good sense.
Yet the battle to mobilize adequate financing remains.
The same skeptics repeat their tired opposition without reference to a decade of evidence.
It is shocking how their free-market fundamentalism (or simply ideological opposition to aid of any kind) can blind them to life-and-death needs and the efficacy of practical approaches that are well known to health professionals. (They are also blind to professional methods in other areas, such as food production.)
The Global Fund is urgently appealing for a minimum of $5 billion per year for the next three years – a tiny sum relative to the world economy (and equal to roughly $5 per person in the high-income countries).
It could wisely use twice that amount.
It seems likely that the US Government will agree to contribute one-third of the $5 billion if the rest of the world delivers the remainder.
The United Kingdom recently made a strong pledge, and the world now awaits the announcements of Germany, Canada, Australia, Japan, and other long-standing and new donor countries in Europe, the Middle East, and Asia.
Millions of people around the world will live, or die, depending on what these governments decide in December.
May they, and we, choose life.
Responding to Ebola
NEW YORK – The horrific Ebola epidemic in at least four West African countries (Guinea, Liberia, Sierra Leone, and Nigeria) demands not only an emergency response to halt the outbreak; it also calls for re-thinking some basic assumptions of global public health.
We live in an age of emerging and re-emerging infectious diseases that can spread quickly through global networks.
We therefore need a global disease-control system commensurate with that reality.
Fortunately, such a system is within reach if we invest appropriately.
Ebola is the latest of many recent epidemics, also including AIDS, SARS, H1N1 flu, H7N9 flu, and others.
AIDS is the deadliest of these killers, claiming nearly 36 million lives since 1981.
Of course, even larger and more sudden epidemics are possible, such as the 1918 influenza during World War I, which claimed 50-100 million lives (far more than the war itself).
And, though the 2003 SARS outbreak was contained, causing fewer than 1,000 deaths, the disease was on the verge of deeply disrupting several East Asian economies including China’s.
Second, once a new infectious disease appears, its spread through airlines, ships, megacities, and trade in animal products is likely to be extremely rapid.
These epidemic diseases are new markers of globalization, revealing through their chain of death how vulnerable the world has become from the pervasive movement of people and goods.
Third, the poor are the first to suffer and the worst affected.
The rural poor live closest to the infected animals that first transmit the disease.
They often hunt and eat bushmeat, leaving them vulnerable to infection.
Poor, often illiterate, individuals are generally unaware of how infectious diseases – especially unfamiliar diseases – are transmitted, making them much more likely to become infected and to infect others.
Moreover, given poor nutrition and lack of access to basic health services, their weakened immune systems are easily overcome by infections that better nourished and treated individuals can survive.
And “de-medicalized” conditions – with few if any professional health workers to ensure an appropriate public-health response to an epidemic (such as isolation of infected individuals, tracing of contacts, surveillance, and so forth) – make initial outbreaks more severe.&nbsp;
Finally, the required medical responses, including diagnostic tools and effective medications and vaccines, inevitably lag behind the emerging diseases.
In any event, such tools must be continually replenished.
This requires cutting-edge biotechnology, immunology, and ultimately bioengineering to create large-scale industrial responses (such as millions of doses of vaccines or medicines in the case of large epidemics).
The AIDS crisis, for example, called forth tens of billions of dollars for research and development – and similarly substantial commitments by the pharmaceutical industry – to produce lifesaving antiretroviral drugs at global scale.
Yet each breakthrough inevitably leads to the pathogen’s mutation, rendering previous treatments less effective.
There is no ultimate victory, only a constant arms race between humanity and disease-causing agents.
So, is the world ready for Ebola, a newly lethal influenza, a mutation of HIV that could speed the transfer of the disease, or the development of new multi-drug-resistant strains of malaria or other pathogens?
The answer is no.
Though investment in public health increased significantly after 2000, leading to notable successes in the fights against AIDS, tuberculosis, and malaria, there has recently been a marked shortfall in global spending on public health relative to need.
Donor countries, failing to anticipate and respond adequately to new and ongoing challenges, have subjected the World Health Organization to a debilitating budget squeeze, while funding for the Global Fund to Fight AIDS, Tuberculosis, and Malaria has fallen far short of the sums needed to win the war against these diseases.
Here is a shortlist of what urgently needs to be done.
First, the United States, the European Union, the Gulf countries, and East Asian states should establish a flexible fund under WHO leadership to combat the current Ebola epidemic, probably at an initial level of $50-$100 million, pending further developments.
This would allow a rapid public-health response that is commensurate to the immediate challenge.
Second, donor countries should quickly expand both the Global Fund’s budget and mandate, so that it becomes a global health fund for low-income countries.
The main goal would be to help the poorest countries establish basic health systems in every slum and rural community, a concept known as Universal Health Coverage (UHC).
The greatest urgency lies in Sub-Saharan Africa and South Asia, where health conditions and extreme poverty are worst, and preventable and controllable infectious diseases continue to rage.
In particular, these regions should train and deploy a new cadre of community health workers, trained to recognize disease symptoms, provide surveillance, and administer diagnoses and appropriate treatments.
At a cost of just $5 billion per year, it would be possible to ensure that well-trained health workers are present in every African community to provide lifesaving interventions and respond effectively to health emergencies like Ebola.
Finally, high-income countries must continue to invest adequately in global disease surveillance, the WHO’s outreach capacities, and life-saving biomedical research, which has consistently delivered massive benefits for humanity during the past century.
Despite tight national budgets, it would be reckless to put our very survival on the fiscal chopping block.
The Limits of Climate Negotiations
NEW YORK – If the world is to solve the climate-change crisis, we will need a new approach.
Currently, the major powers view climate change as a negotiation over who will reduce their CO2 emissions (mainly from the use of coal, oil, and gas).
Each agrees to small “contributions” of emission reduction, trying to nudge the other countries to do more.
The United States, for example, will “concede” a little bit of CO2 reduction if China will do the same.
For two decades, we have been trapped in this minimalist and incremental mindset, which is wrong in two key ways.
First, it is not working: CO2 emissions are rising, not falling.
The global oil industry is having a field day – fracking, drilling, exploring in the Arctic, gasifying coal, and building new liquefied natural gas (LNG) facilities.
The world is wrecking the climate and food-supply systems at a breakneck pace.
Second, “decarbonizing” the energy system is technologically complicated.
America’s real problem is not competition from China; it’s the complexity of shifting a $17.5 trillion economy from fossil fuels to low-carbon alternatives.
China’s problem is not the US, but how to wean the world’s largest, or second largest economy (depending on which data are used) off its deeply entrenched dependence on coal.
These are mainly engineering problems, not negotiating problems.
To be sure, both economies could decarbonize if they cut output sharply.
But neither the US nor China is ready to sacrifice millions of jobs and trillions of dollars to do so.
Indeed, the question is how to decarbonize while remaining economically strong.
Climate negotiators cannot answer that question, but innovators like Elon Musk of Tesla, and scientists like Klaus Lackner of Columbia University, can.
Decarbonizing the world’s energy system requires preventing our production of vast and growing amounts of electricity from boosting atmospheric CO2 emissions.
It also presupposes a switchover to a zero-carbon transport fleet and a lot more production per kilowatt-hour of energy.
Zero-carbon electricity is within reach.
Solar and wind power can deliver that already, but not necessarily when and where needed.
We need storage breakthroughs for these intermittent clean-energy sources.
Nuclear power, another important source of zero-carbon energy, will also need to play a big role in the future, implying the need to bolster public confidence in its safety.
Even fossil fuels can produce zero-carbon electricity, if carbon capture and storage is used.
Lackner is a world leader in new CCS strategies.
Electrification of transport is already with us, and Tesla, with its sophisticated electric vehicles, is capturing the public’s imagination and interest.
Yet further technological advances are needed in order to reduce electric vehicles’ costs, increase their reliability, and extend their range.
Musk, eager to spur rapid development of the vehicles, made history last week by opening Tesla’s patents for use by competitors.
Technology offers new breakthroughs in energy efficiency as well.
New building designs have slashed heating and cooling costs by relying much more on insulation, natural ventilation, and solar power.
Advances in nanotechnology offer the prospect of lighter construction materials that require much less energy to produce, making both buildings and vehicles far more energy efficient.
The world needs a concerted push to adopt to low-carbon electricity, not another “us-versus-them” negotiation.
All countries need new, low-carbon technologies, many of which are still out of commercial reach.
Climate negotiators should therefore be focusing on how to cooperate to ensure that technology breakthroughs are achieved and benefit all countries.
They should take their cue from other cases in which government, scientists, and industry teamed up to produce major changes.
For example, in carrying out the Manhattan Project (to produce the atomic bomb during World War II) and the first moon landing, the US government set a remarkable technological goal, established a bold timetable, and committed the financial resources needed to get the job done.
In both cases, the scientists and engineers delivered on time.
The example of atomic bombs might seem an unpleasant one, yet it raises an important question: If we ask governments and scientists to cooperate on war technology, shouldn’t we do at least the same to save the planet from carbon pollution?
In fact, the process of “directed technological change,” in which bold objectives are set, milestones are identified, and timelines are put into place, is much more common than many realize.
The information-technology revolution that has brought us computers, smart phones, GPS, and much more, was built on a series of industry and government roadmaps.
The human genome was mapped through such a government-led effort – one that ultimately brought in the private sector as well.
More recently, government and industry got together to cut the costs of sequencing an individual genome from around $100 million in 2001 to just $1,000 today.
A dramatic cost-cutting goal was set, scientists went to work, and the targeted breakthrough was achieved on time.
Fighting climate change does depend on all countries having confidence that their competitors will follow suit.
So, yes, let the upcoming climate negotiations spell out shared actions by the US, China, Europe, and others.
But let’s stop pretending that this is a poker game, rather than a scientific and technological puzzle of the highest order.
We need the likes of Musk, Lackner, General Electric, Siemens, Ericsson, Intel, Electricité de France, Huawei, Google, Baidu, Samsung, Apple, and others in laboratories, power plants, and cities around the world to forge the technological breakthroughs that will reduce global CO2 emissions.
There is even a place at the table for ExxonMobil, Chevron, BP, Peabody, Koch Industries, and other oil and coal giants.
If they expect their products to be used in the future, they had better make them safe through the deployment of advanced CCS technologies.
The point is that targeted and deep decarbonization is a job for all stakeholders, including the fossil-fuel industry, and one in which we must all be on the side of human survival and wellbeing.
Deep Decarbonization
NEW YORK – Have a look at what happened around the world this past month.
Australia’s heat wave filled headlines when temperatures reaching 45° Celsius disrupted the Australian Open tennis tournament.
California’s extreme drought forced the governor to declare a state of emergency.
Major floods in Indonesia killed dozens and displaced tens of thousands.
Beijing’s coal-induced smog forced people to stay in their homes, closed highways, and diverted flights.
Such events are daily warnings to the world: wake up before it is too late.
We have entered the Age of Sustainable Development.
Either we make peace with the planet, or we destroy our hard-won prosperity.
The choice seems obvious, but our actions speak louder than words. Humanity continues on a path of ruin, driven by short-term greed and ignorance.
Much (though not all) of the global environmental crisis stems from the world’s fossil-fuel-based energy system.
More than 80% of all primary energy in the world comes from coal, oil, and gas.
When these fossil fuels are burned, they emit carbon dioxide, which in turn changes the Earth’s climate.
The basic physics has been known for more than a century.
Unfortunately, a few oil companies (ExxonMobil and Koch Industries are the most notorious) have devoted enormous resources to sowing confusion even where there is clear scientific consensus.
But, in order to save the planet we know, and to preserve the world’s food supply and the well-being of future generations, there is no alternative to shifting to a new, low-carbon energy system.
There are three parts to this transition.
The first is improved energy efficiency, meaning that we should use much less energy to achieve the same level of well-being.
For example, we can design our buildings to use sunlight and natural-air circulation so that they require far less commercial energy for heating, cooling, and ventilation.
Second, we need to shift to solar, wind, hydro, nuclear, geothermal, and other forms of energy that are not based on fossil fuels.
The technology exists to use these alternatives safely, affordably, and at a scale large enough to replace almost all of the coal, and much of the oil, that we use today.
Only natural gas (the cleanest-burning fossil fuel) would remain a significant source of energy by mid-century.
Finally, to the extent that we continue to rely on fossil fuels, we must capture the resulting CO2 emissions at power plants before they escape into the atmosphere.
The captured CO2 would then be injected underground or under the ocean floor for safe long-term storage.
Carbon capture and sequestration (CCS) is already being used successfully on a very small scale (mainly to enhance oil recovery in depleted wells).
If (and only if) it proves successful for large-scale use, coal-dependent countries like China, India, and the United States could continue to use their reserves.
American politicians have proved to be incapable of designing policies to shift the US to low-carbon energy use.
Such policies would include a rising tax on CO2 emissions, large-scale research-and-development efforts in low-carbon technologies, a shift to electric vehicles, and regulations to phase out all coal-fired power plants except those that install CCS.
Yet politicians are pursuing none of these policies adequately.
Climate-change foes have spent billions of dollars to influence policymakers, support election campaigns by defenders of fossil fuels, and defeat candidates who dare to promote clean energy.
The Republican Party as a whole attracts massive financial support from opponents of decarbonization, and these donors aggressively fight even the smallest step toward renewable energy.
For their part, many Democratic members of the US Congress are also in the pro-fossil-fuel camp.
A few big players in the energy industry, showing no concern for truth (much less for our children, who will bear the consequences of our present folly), have teamed up with Rupert Murdoch.
Indeed, Murdoch, the Koch Brothers, and their allies behave just like Big Tobacco in denying scientific truths; even use the same experts for hire.
The situation is generally the same around the world.
Wherever powerful lobbies defend existing coal or oil interests, politicians typically are afraid to tell the truth about the need for low-carbon energy.
Brave politicians who do tell the truth about climate change are found mainly in countries that do not have a powerful fossil-fuel lobby.
Consider the fate of one courageous exception to this rule.
Kevin Rudd, the former Australian prime minister, tried to implement a clean-energy policy in his coal-producing country.
Rudd was defeated in his re-election bid by a candidate whose backing from an alliance of Murdoch and coal companies enabled him to outspend Rudd by a huge margin.
Murdoch’s tabloids pump out anti-scientific propaganda opposing climate-change policies not only in Australia, but also in the US and elsewhere.
The reason all of this matters is that the path to deep decarbonization is open to us.
Yet time is very short.
The world needs to stop building new coal-fired power plants (except those that implement CCS) and to shift to low-carbon electricity.
It needs to phase out the internal combustion engine for almost all new passenger vehicles by around 2030, shifting to vehicles powered by electricity.
And it needs to adopt energy-saving technologies that consume less commercial energy.
The technologies are available and will get better and cheaper with use, if only fossil-fuel lobbies can be held at bay.
If this happens, people around the world will discover something wonderful.
Not only will they have saved the planet for the next generation; they will also enjoy sunshine and clean, healthy air.
And they will ask what took so long when the Earth itself was at dire risk.
Let the Middle East Govern Itself
NEW YORK – It is time for the United States and other powers to let the Middle East govern itself in line with national sovereignty and the United Nations Charter.
As the US contemplates yet another round of military action in Iraq and intervention in Syria, it should recognize two basic truths.
First, US interventions, which have cost the country trillions of dollars and thousands of lives over the past decade, have consistently destabilized the Middle East, while causing massive suffering in the affected countries.
Second, the region’s governments – in Syria, Saudi Arabia, Turkey, Iran, Iraq, Egypt, and elsewhere – have both the incentive and the means to reach mutual accommodations.
What is stopping them is the belief that the US or some other outside power (such as Russia) will deliver a decisive victory on their behalf.
When the Ottoman Empire collapsed at the end of World War I, the great powers of the day, Britain and France, carved out successor states in order to ensure their control over the Middle East’s oil, geopolitics, and transit routes to Asia.
Their cynicism – reflected, for example, in the Sykes-Picot Agreement –&nbsp;established a lasting pattern of destructive outside meddling.
With America’s subsequent emergence as a global power, it treated the Middle East in the same way, relentlessly installing, toppling, bribing, or manipulating the region’s governments, all the while mouthing democratic rhetoric.
For example, less than two years after Iran’s democratically elected parliament and prime minister, Mohammad Mossadegh, nationalized the Anglo-Iranian Oil Company in 1951, the US and Britain used their secret services to topple Mossadegh and install the incompetent, violent, and authoritarian Shah Reza Pahlavi.
Not surprisingly, the Islamic Revolution that overthrew the Shah in 1979 brought a wave of virulent anti-Americanism in its wake.
Rather than seeking rapprochement, however, the US supported Saddam Hussein during Iraq’s eight-year war with Iran in the 1980s.
Iraq fared no better with the British and Americans.
Britain ruthlessly created a subservient Iraqi state after WWI, backing Sunni elites to control the majority Shia population.
After oil was discovered in the 1920s, Britain assumed control over the new oil fields, using military force as needed.
The US supported the 1968 coup that brought the Ba’ath Party – and Saddam – to power.
With Saddam’s invasion of Kuwait in 1990, however, the US turned on him, and has been entwined in Iraq’s politics ever since, including two wars, sanction regimes, the toppling of Saddam in 2003, and repeated attempts, as recently as this month, to install a government that it considered acceptable.
The result has been an unmitigated catastrophe: the destruction of Iraq as a functioning society in an ongoing civil war, fueled by outside powers, that has caused economic ruin and collapsing living standards.
Hundreds of thousands of Iraqis have died in the violence since 1990.
Syria endured decades of French dominance after WWI, and then alternatingly hot and cold relations with the US and Europe since the 1960s.
During the past decade, the US and its allies have tried to weaken, and then, starting in 2011, to topple President Bashar al-Assad’s regime, mainly in a proxy war to undermine Iranian influence in Syria.
The results have been devastating for the Syrian people.
Assad remains in power, but more than 190,000 Syrians are dead and millions have been displaced as a result of an insurrection supported by the US and its allies (with Assad backed by Russia and Iran).
Some US officials are now reportedly considering an alliance with Assad to fight the militant Islamic State, whose rise was enabled by the US-backed insurrection.
After decades of cynical and often secret interventions by the US, Britain, France, Russia, and other outside powers, the region’s political institutions are based largely on corruption, sectarian politics, and brute force.
Yet whenever a new Middle East crisis erupts, the latest being triggered by the Islamic State’s recent gains, the US intervenes again, perhaps to change a government (as it has just orchestrated in Iraq) or to launch a new bombing assault.
Backroom dealings and violence continue to rule the day.
Pundits claim that Arabs cannot manage democracy.
In fact, the US and its allies simply don’t like the results of Arab democracy, which all too often produces governments that are nationalist, anti-Israel, Islamist, and dangerous to America’s oil interests.
When the ballots go in that direction, the US simply ignores the election results (as it did, for example, in 2006, when Hamas won a large majority of the popular vote in Gaza).
The US cannot stop the spiral of violence in the Middle East.
The damage in Libya, Gaza, Syria, and Iraq demands that a political solution be found within the region, not imposed from the outside.
The UN Security Council should provide an international framework in which the major powers pull back, lift crippling economic sanctions, and abide by political agreements reached by the region’s own governments and factions.&nbsp;
Iran, Turkey, Egypt, Syria, Saudi Arabia, the UAE, and other neighbors know one another well enough – thanks to 2,000-plus years of trade and war – to sort out the pieces themselves, without interference from the US, Russia, and the former colonial powers of Europe.
The countries of the Middle East have a common interest in starving hyper-violent groups like the Islamic State of arms, money, and media attention.
They also share an interest in keeping oil flowing to world markets – and in capturing the bulk of the revenues.
I am not claiming that all will be well if the US and other powers pull back.
There is enough hatred, corruption, and arms in the region to keep it in crisis for years to come.
And nobody should expect stable democracies any time soon.
But lasting solutions will not be found as long as the US and other foreign powers continue to meddle in the region.
One hundred years after the start of WWI, colonial practices must finally come to an end.
The Middle East needs the opportunity to govern itself, protected and supported by the UN Charter, not by any individual great power.
Our Last Chance for a Safe Planet
MELBOURNE – Humanity has just about run out of time to address climate change.
Scientists have pointed out that a rise in temperature of 2º Celsius above pre-industrial levels will put the Earth in dangerous, uncharted territory.
Yet we currently are on a path toward an increase of 4º or more this century.
The last chance for action has arrived.
That chance lies in Paris in December 2015, when the world’s governments meet for the 21st annual United Nations climate-change meeting.
But this time will be different.
Either governments will agree to decisive action, as they have promised, or we will look back at 2015 as the year when climate sanity slipped through our fingers.
In 1992, the world’s governments adopted the UN Framework Convention on Climate Change, promising to avoid “dangerous anthropogenic [human-induced] interference in the climate system” by reducing the rate of emission of greenhouse gases, especially carbon dioxide.
But, though the treaty entered into force in 1994, the rate of emissions of greenhouse gases, including CO2, has actually increased.
In 1992, global combustion of coal, oil, and gas, plus cement production, released 22.6 billion tons of CO2 into the air.
In 2012, the most recent year for which comparable data are available, emissions were 34.5 billion tons.
Humanity has accelerated, rather than controlled, human-induced climate change.
This is now the greatest moral issue of our time.
Global fossil-fuel use gravely threatens the poor, who are the most vulnerable to climate change (though the rich are the main cause), and future generations, who will inherit a planet that has become unlivable in many places, with food supply subject to massive shocks.
We are causing this harm in an age when technological breakthroughs enable the world to shift from dangerous fossil fuels to low-carbon energy sources, such as wind, solar, nuclear, and hydro, and reduce the impact of fossil fuels by using carbon capture and storage (CCS) technology.
Pope Francis recently put it just right: “Safeguard Creation,” he said. “Because if we destroy Creation, Creation will destroy us!
Never forget this!”
Yet, for the many powerful interests, climate change remains a game, with the goal being to delay action for as long as possible.
The giant fossil-fuel companies have continued to lobby behind the scenes against the shift to low-carbon energy, and have used their vast wealth to buy media coverage designed to sow confusion.
Rupert Murdoch’s media empire in the United States, the United Kingdom, Australia, and elsewhere stands out as playing a particularly cynical and harmful role in spreading anti-scientific propaganda.
Even so, the politics of climate change may be changing for the better – a change reflected in the Pope’s forceful message.
Here are six reasons why the stalemate might soon end.
First, the world is waking up to the calamity that we are causing.
Though the Murdoch propaganda machine churns out a daily stream of anti-scientific falsehoods, the public also sees prolonged droughts (now in parts of Brazil, California, and Southeast Asia, to name a few places), massive floods (recently in Bosnia and Serbia), and lethal heat waves (in many parts of the world).
Second, the world’s citizens do not want to go down in flames.
Public opinion has so far succeeded in blocking the construction of the Keystone XL Pipeline, which would accelerate the production of Canada’s oil sands – a shocking prospect, given that neither Canada nor the US yet have committed to a climate plan.
Third, more severe climate shocks may lie ahead.
This year could prove to be a major El Niño year, when the waters of the Eastern Pacific warm and create global climate disruptions.
A big El Niño now would be even more dangerous than usual, because it would add to the overall rising trend in global temperatures.
Indeed, many scientists believe that a big El Niño could make 2015 the hottest year in the Earth’s history.
Fourth, both the US and China, the two largest emitters of CO2, are finally beginning to get serious.
President Barack Obama’s administration is trying to stop the construction of new coal-fired power plants, unless they are equipped with CCS technology.
China, for its part, has realized that its heavy dependence on coal is causing such devastating pollution and smog that it is leading to massive loss of life, with life expectancy down as much as five years in regions with heavy coal consumption.
Fifth, the Paris negotiations are finally beginning to attract global attention from both the public and world leaders.
UN Secretary-General Ban Ki-moon has called for political leaders to attend a special summit in September 2014, 14 months ahead of the Paris meeting, to launch intensive negotiations.
The UN expert network that I direct, the Sustainable Development Solutions Network (UN SDSN), will issue a major report in July on how each of the major economies can successfully decarbonize the energy system.
Finally, technological advances in low-carbon energy systems, including photovoltaics, electric vehicles, CCS, and fourth-generation nuclear power with greatly enhanced safety features, all help make the transition to low-cost, low-carbon energy technologically realistic, with huge benefits for human health and planetary safety.
Starting this fall, the UN SDSN will create a platform for all global citizens to participate in the hard work of saving the planet.
The SDSN will offer a free, online introductory course to climate change, and then host a global online “negotiation” of a global climate agreement.
We expect that hundreds of thousands, perhaps millions, of interested citizens worldwide will participate online, showing the way for the politicians.
The control of climate change is a moral imperative and a practical necessity – far too important to be left to politicians, Big Oil, and their media propagandists.
The Dollar and Its Rivals
CAMBRIDGE – Since 1976, the US dollar’s role as an international currency has been slowly waning.
International use of the dollar to hold foreign-exchange reserves, denominate financial transactions, invoice trade, and as a vehicle in currency markets is below its level during the heyday of the Bretton Woods era, from 1945 to 1971.
But most people would be surprised by what the most recent numbers show.
There is an abundance of explanations for the downward trend.
Since the Vietnam War, US budget deficits, money creation, and current-account deficits have often been high.
Presumably as a result, the dollar has lost value relative to other major currencies or in terms of purchasing power.
Meanwhile, the US share of global output has declined.
And, most recently, the disturbing willingness of some members of the US Congress to pursue a strategy that would cause the Treasury to default on legal obligations has undermined global confidence in the dollar’s privileged status.
Moreover, some emerging-market currencies are joining the club of international currencies for the first time.
Indeed, some analysts have suggested that the Chinese renminbi may rival the dollar as the leading international currency by the end of the decade.
But the dollar’s status as an international currency has not fallen uniformly.
Interestingly, the periods when the public is most concerned about the issue do not coincide with the periods when the dollar’s share in international transactions is in fact falling.
By the criteria of international use as a reserve currency among central banks and as a vehicle in foreign-exchange markets, the most rapid declines took place from 1978 to 1991 and from 2001 to 2010.
Between these two intervals, from 1992 to 2000, there was a clear reversal of the trend, notwithstanding a popular orgy of dollar declinism around the middle of that decade.
Central banks held only an estimated 46% of their foreign-exchange reserves in dollars in 1992, but that share rebounded to almost 70% by 2000.
Subsequently, the long-term downward trend resumed.
According to one estimate, the dollar’s share in central-banks’ foreign reserves declined from about 70% in 2001 to barely 60% in 2010.
During the same decade, its share in the foreign-exchange market also declined: the dollar constituted one side or the other in 90% of foreign-exchange trades in 2001, but only 85% in 2010.
The International Monetary Fund’s most recent statistics suggest, unexpectedly, another pause in the dollar’s long-term decline.
According to the IMF, the dollar’s share in foreign-exchange reserves stopped falling in 2010 and has been flat since then.
If anything, the share is up slightly thus far in 2013.
Similarly, the Bank for International Settlements (BIS) reported in its recent triennial survey that the dollar’s share in the world’s foreign-exchange trades rose from 85% in 2010 to 87% in 2013.
Given dysfunctional US fiscal policy, the dollar’s resilience is surprising.
Or maybe we should no longer be surprised.
After all, when the global financial crisis erupted in 2008 from the bowels of the American subprime-mortgage market, global investors responded by fleeing to the US, not from it.
They obviously still regard US Treasury bills as a safe haven and the dollar as the top international currency, especially given the absence of good alternatives.
In particular, the euro has its own all-too-obvious problems.
Indeed, the euro’s share in reserve holdings and foreign-exchange transactions have both declined by several percentage points in the most recent statistics.
At the same time, the IMF’s data indicate that the vaunted renminbi is not yet among the top seven currencies in terms of central-bank reserve holdings.
And, according to the BIS, while the renminbi has finally broken into the top ten currencies in foreign-exchange markets, it still accounts for only 2.2% of all transactions, just behind the Mexican peso’s 2.5% share.
Despite recent moves by the Chinese government, the renminbi still has a long way to go.
To try to explain the recent stabilization of the dollar’s status, one might note something that the last three years have in common with the previous period of temporary reversal from 1992 to 2000: striking improvements in the US budget deficit.
By the end of the 1990’s, the record deficits of the 1980’s had been transformed into record surpluses; today, the federal deficit is less than half its 2010 level.