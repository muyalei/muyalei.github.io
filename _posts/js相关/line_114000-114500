These measures not only distort resource allocation; they also often lead to collusion between businesses and government officials seeking opportunities to enter the private sector upon retirement.
Eliminating them would go a long way toward increasing corporate-tax revenue, without stifling growth.
Abe’s growth strategy has the potential to bring massive benefits to Japan.
But it will also demand sacrifices.
Consumption-tax hikes will be borne by consumers; the TPP will create new challenges for farmers; and deregulation will run counter to some bureaucrats’ interests.
In this context, it is reasonable to expect businesses to relinquish some of their tax exemptions.
The Abe government has presented a set of forward-looking reforms – and appears determined to follow through on implementing them, even if doing so means confronting those with a vested interest in their failure.
If the third arrow succeeds in sustaining Japan’s economic revival, there will no longer be any room to doubt the merits of Abenomics.
Is Abenomics Working?
TOKYO – Last April, Japan’s government implemented a long-planned consumption-tax hike, from 5% to 8%, the first in a two-step increase that is expected to bring the rate to 10% by 2015.
The hike – a key feature of “Abenomics,” Prime Minister Shinzo Abe’s three-pronged strategy to revive Japan’s economy – signals the government’s long-term commitment to fiscal consolidation.
But it has also dealt Japan a heavy macroeconomic blow.
Preliminary GDP data show a 6.8% contraction year-on-year in the second quarter of this year – the largest since the 2011 earthquake and tsunami that devastated the country.
Moreover, consumer spending fell by a record amount, contributing to a total real (inflation-adjusted) decline of 5.9% from last July.
But it is not all bad news.
Expansionary monetary policy – the second of three so-called “arrows” of Abenomics, after fiscal stimulus – has brought down the unemployment rate to just 3.8%.
The ratio of job openings to applicants has exceeded parity, and the GDP deflator narrowed to close to zero.
Such data have given rise to two opposing views.
Some economists worry that negative second-quarter data will dampen inflation expectations, thereby undermining Abe’s plan for boosting growth.
Meanwhile, the Bank of Japan (BOJ) is emphasizing the positive outcomes of its monetary policy – and is hesitating to continue its expansionary measures.
If the first view proves correct, the BOJ will need to ease monetary policy further to counter falling inflation.
If the BOJ is right, it should maintain its current approach, while the government should either postpone the next consumption-tax increase or implement it in two 1% increments, instead of a single 2% hike.
Of course, the second-quarter GDP data show the economy’s immediate response to the hike.
But no decision should be made until the third-quarter results are released, providing a clearer picture of what will happen to Japan’s economy after it absorbs the first rate increase.
Fortunately, that is precisely what Abe intends to do.
In any case, the success of monetary policy is difficult to deny.
As the deflation gap narrows, however, the overall impact of monetary policy will weaken, as it increasingly influences prices more than output.
That is why it is time for Japan’s leaders to shift their focus from the demand-focused first and second arrows to the supply-oriented third arrow: a new growth strategy.
When there is sufficient excess supply in the economy, promoting supply-side productivity is practically useless without efforts to boost demand.
That implies that it was not appropriate to focus on growth until the deflation gap narrowed considerably – that is, until now.
The third arrow is not a traditional industrial-policy-based approach.
On the contrary, it emphasizes reform of the labor market, deregulation, and a reduction in the corporate-tax rate.
A key component of Abe’s growth strategy is to expand the workforce – a major challenge, given that Japanese society is aging rapidly.
One logical solution would be to integrate more foreign labor into the Japanese economy.
But efforts to promote immigration face considerable social and cultural barriers.
A simpler solution would be to mobilize working-age women who already – or plan to – stay at home.
By removing the barriers to employment that women face – whether practical obstacles, like insufficient childcare services, or social constraints – Japan could substantially increase women’s workforce-participation rate, creating an invaluable buffer against the growing labor shortage.
The second imperative for boosting growth is the removal of excessively cumbersome government regulations.
Under the current system, it took 34 years to approve the establishment of a new medical school – the result of collusion between government officials and doctors.
Abe’s plan calls for introducing a series of less strictly regulated special economic zones, each with a specific objective – for example, adopting new medical technologies or attracting foreign businesses.
Such a move promises to help prevent damaging obstructionism by the authorities.
At the same time, the government should work with the country’s trade unions to boost the flexibility and efficiency of the labor market.
Finally, Abe’s growth strategy demands a corporate-tax reduction – a powerful tool for increasing the tax base in a world in which countries are competing to attract multinational companies.
Indeed, lower taxes are vital to increase foreign and domestic investment in Japan.
Some of these initiatives, particularly deregulation, will undoubtedly face resistance from bureaucrats concerned about losing their influence.
But, as long as Abe, backed by Chief Cabinet Secretary Yoshihide Suga, remains committed to his stated objectives, Japan’s economic future will remain bright.
The Ukrainian Revolution’s Jewish Question
WARSAW – “And from a Jewish perspective?”
I asked Josef Zissels.
The veteran Ukrainian dissident, Jewish activist, and passionate advocate of Ukraine’s “Maidan” movement, had just finished briefing a Warsaw audience about the movement’s spectacular victory and President Viktor Yanukovych’s fall from power.
“There is no Jewish perspective,” he answered.
“There are Jews on both sides of the divide.”
That is certainly true.
For example, Aleksander Feldman, the chairman of the Jewish Fund for Ukraine, is a prominent parliamentarian for Yanukovych’s Party of Regions – though he condemned the deposed president after his fall.
And several Jewish oligarchs were close to Yanukovych until the very end.
But support by Jews for the Maidan movement was much more salient.
Four of the 82 protesters killed in Kyiv’s Independence Square were Jewish, and a Jewish sotnia, or “hundred” – a term, ironically, associated with Cossack pogromists – defended the square against Yanukovych’s uniformed goons.
And yet, alongside Jews at the Maidan were Ukrainian nationalists, with their long history of anti-Semitism.
That history is important, not only because it justifies treating them with suspicion.
More important, it animates Russian President Vladimir Putin’s repeated denunciations of “neo-Nazis, Russophobes, and anti-Semites” allegedly running rampant in the streets of Kyiv, forcing a reluctant Russia to protect Jews, Russians, and any decent Ukrainians who remain.
What are we to make of such claims?
In recent weeks, violence targeting Jews has indeed occurred, including the stabbing of a rabbi in Kyiv and the firebombing of a synagogue in Zaporizhia.
But it is impossible to ascertain the perpetrators, and the Maidan nationalists – the Svoboda party, which has five members in the new government and idolizes wartime leader Stepan Bandera, and the even more extreme Right Sector – have taken pains to stress that anti-Semitism is not part of their program today.
Such disavowals should not be discounted as mere window dressing.
After all, one dresses windows with what one knows the customer wants to see.
And the customer of the Maidan is the Ukrainian people, not The New York Times.
If nationalists believe that they will not curry favor with Ukrainians by Jew-baiting, that is a welcome development.
Still, though Ukraine’s chief rabbis and Jewish leaders have emphatically rejected Putin’s claims of anti-Semitic excesses, there is enough hatred and blood in Ukraine’s recent history to make one worry.
Anti-Semitism was an integral part of European twentieth-century nationalisms, and Bandera’s Organization of Ukrainian Nationalists was no exception.
The OUN conducted terrorist attacks in pre-war Poland, was persecuted by the Soviets after they occupied Eastern Poland in 1939, allied itself with the Nazis after they invaded the Soviet Union in 1941, and slaughtered thousands of Poles and Jews in a drive to “purify” Ukraine.
But Bandera’s men were nationalists with no allies.
The OUN broke with the Nazis after they denied Ukraine independence, and finally fought the Soviets and both the army of Communist Poland and the anticommunist Polish underground after the Germans were routed.
In western Ukraine, they remain the incarnation of a heroic myth.
In eastern Ukraine, with its large Russian population, they are widely seen as traitors to the Soviet motherland.
So it is no surprise that Putin is trying to place the Maidan movement beyond the pale by emphasizing the OUN’s passing, if bloody, alliance with the Nazis.
But the “Great Russia” nationalism that he has stirred up to mobilize popular support for his Ukraine policy is hardly more appealing.
It is true that 70 years ago Russian nationalism served Stalin’s totalitarianism in the righteous cause of defeating Hitler’s totalitarianism.
It is also true that Russia today is as free of overt manifestations of anti-Semitism as Ukraine is, largely because Putin’s hostility to Jew-baiting is well known and duly noted.
But Putin’s implied argument that in Ukraine he is refighting WWII, with Russia once again rescuing Jews and the world from nationalist pogromists and their European (read: German/ Nazi) sponsors is simply not credible.
On the contrary, his justification for seizing and occupying Crimea – the need to defend ethnic kin from a nonexistent threat – was precisely Hitler’s justification for annexing the Sudetenland.
Observers would do well, therefore, not to dredge up the past while ignoring the present.
The Maidan movement, for all of the nasty antecedents of some of its participants, began as a true popular uprising against a corrupt and despotic regime supported by an expansionist Russia.
Illiberal nationalism is one of the movement’s driving elements, owing to its widely shared and understandable anti-Russia appeal.
And, though that nationalism may yet be directed against Ukraine’s Russians, Poles, and Jews, as it was in the past, the rest of the movement would resist such a turn (which may well explain why it has not happened).
Putin’s claim that fascists have taken control in Kyiv is fundamentally bogus, while Russia’s despicable actions in Crimea and eastern Ukraine are all too real.
Russia does retain some support among Russophone Ukrainians of all ethnicities, including some Jews.
But Zissels is right: The battle is not about them; it is about the survival of a fledgling democratic nation-state.
Kosovo’s Original Sin
PRISTINA – Hundreds of people disappeared ten years ago in Kosovo, the former Serbian province that is now the world’s newest state.
These are not missing persons like the Albanians whom Serbian police executed and buried in secret graves during the Kosovo conflict of 1999.
These missing persons disappeared after the conflict, on NATO’s and the United Nations’ watch.
Most were Serb civilians.
Relatives of most report that they were abducted.
Kosovo, to a significant degree, owes its independence to a NATO military intervention undertaken in the name of human rights.
And in the name of human rights, it is time for the truth to come out about the people who went missing after the conflict, and about why, for a decade, UN officials ignored appeals by the victims’ families and launched no criminal investigation.
Kosovo’s Albanian authorities have for years made no significant decision without receiving the imprimatur of the European Union and, especially, the United States.
They should urge the Kosovo government to mount a credible investigation, for the sake of the victims and people in Kosovo who want their state to be ruled by law.
In 1999, the US led NATO into war against Slobodan Milosevic’s Serbia to end gross violations of the rights of Kosovo’s majority Albanians.
Serb nationalists had quashed Kosovo’s autonomy.
Serb troops beat, killed, and jailed Albanians, whose leaders, following cues from Western embassies, urged nonviolence.
After the rise of an Albanian insurgency, the Kosovo Liberation Army (UCK), Milosevic launched a violent, all-out campaign to expel Kosovo’s Albanians.
Villages were burned.
Serbian police forces killed Albanian civilians and cast out hundreds of thousands.
(The authorities in Belgrade have yet to come clean on the Albanians civilians executed and buried on police and military bases in Serbia.)
NATO forced Serb forces to withdraw in June 1999, and international peacekeepers, under NATO’s leadership, occupied Kosovo.
The UN sent a mission to help establish local institutions.
The US and West European countries began shepherding Kosovo’s new Albanian authorities toward statehood.
Last year, Kosovo gained independence.
Sixty countries, including the US and most EU states, have recognized it.
According to a recent BBC news investigation, however, UCK members abducted Serbs, Albanians, Roma, and others after NATO’s arrival.
UN missing-persons researchers – not criminal investigators – searched for them for years and found no trace of them in Kosovo.
They did, however, find Albanian witnesses who asserted that UCK members took captured Serbs, Albanians, and Roma into Albania, where they were killed.
Now the BBC has broadcast interviews with Albanians who were imprisoned in secret UCK camps in Albania.
These witnesses confirmed that the camps also held Serbs, Roma, and others.
The BBC located graves of some of the missing in Albania.
There have been leaks of UN documents citing Albanian sources who name people involved.
From June 1999, the UN and NATO contingents in Kosovo clearly calculated that stability trumped justice.
Despite the urging of staff members, leaders of these missions avoided launching criminal investigations into the missing.
Some UCK leaders, lionized by the local Albanian population as the victors of 1999, are now running Kosovo’s government.
Their blanket denials are no longer credible.
Kosovo will not convince many more countries to recognize its independence unless it investigates these disappearances and demonstrates a commitment to the rule of law.
(Likewise, no one should forget the Albanian dead that Serbia has not returned.)
A credible investigation requires full backing from the authorities in Albania and from the newly established EU mission in Kosovo.
On the record, leaders of this mission say that they are interested in investigating.
Diplomats speaking off the record, however, tell a different tale, even as some Albanian political leaders, including some former UCK officials, call for an inquiry.
The US should use its political clout with Albania and Kosovo to press for a thorough investigation, and for criminal proceedings against anyone who is implicated.
Albanians are outraged at the allegations that UCK members engaged in war crimes and other criminal behavior.
Some Albanians claim that these allegations are Serbian propaganda.
But there are many Albanians who know differently, and believe that Kosovo is failing to come to terms with the acts of a few rogue fighters.
These Albanians fear that, in the absence of the rule of law, Kosovo will become a mafia state.
Kosovo’s Albanians do not want a state tainted by such allegations.
They want a state that respects human rights for everyone.
And they want answers about the fate of hundreds of people who have been missing for a decade.
India’s Deadly Entrance Exams
NEW DELHI – In late April, a 17-year-old girl named Kriti Tripathi leaped to her death in Kota, India, shortly after passing the country’s examination for admission to the prestigious Indian Institutes of Technology (IIT).
A week later, another Kota student, Preeti Singh, hanged herself, succumbing to her injuries after a few days.
Singh’s was the ninth suicide by a student in Kota this year alone, and the 56th in the last five.
All attended Kota’s “coaching institutes,” whose sole purpose is to prepare high-school students for the IIT Joint Entrance Examination (JEE).
In a five-page suicide note, Tripathi expressed her frustration at having been compelled to study engineering, when her real ambition was to become a NASA scientist.
She also described the pressure she had faced at the coaching institution.
Tripathi implored the Human Resource Development Ministry to shut down such institutes, which force their students to endure unbearable stress and depression.
The story is all too common, but should the blame really be laid on the coaching institutes?
In fact, Kota’s coaching institutes are a symptom of a larger problem, hinted at by the city’s senior administrator, District Collector Ravi Kumar Surpur, in an emotional letter he wrote in response to the latest deaths.
Addressing parents directly, Surpur pleaded with them not to subject their children to excessive stress in an attempt to live vicariously through them.
Indian parents are known for demanding academic excellence from their children.
They know that a professional degree in the right field is a passport to social and economic advancement, so they push hard to ensure that their children get one – something that India’s higher-education system does not make easy.
Given this deeply entrenched culture of academic ambition, the planned administrative inquiry into conditions at the Kota coaching institutes is unlikely to result in remedial action.
The toll this culture takes on young people is obvious.
Students are forced to pass brutally difficult examinations – only about 10,000 of the 500,000 who take the IIT-JEE each year score high enough to be admitted – in subjects they often detest.
And Indian students are far more likely to push themselves until they crack than to drop out.
Engineering and medicine remain the subjects of choice for middle-class Indian parents.
The country graduates a half-million engineers every year, some 80% of whom end up in jobs that do not require an engineering degree.
But, in a throwback to the mid-twentieth century, Indian parents view engineering as the gateway to modernity, and continue pressing their children to study it.
Students who do not make it to an IIT end up in institutions of varying quality, many of which do not equip their graduates for today’s labor market.
But at least there are enough engineering colleges in India to meet demand.
Medicine, by contrast, is a frustratingly crowded field – and for no good reason.
India’s medical profession is controlled by the Medical Council of India, an opaque and self-serving cabal that has intentionally limited the supply of available medical college seats.
Medical colleges must be recognized by the MCI, which has seen fit to permit only 381 to exist.
That leaves only 63,800 slots each year in a country of 1.2 billion people – enough space for fewer than 1% of Indian students aspiring to attend medical school.
As if that were not bad enough, some of the seats are awarded against “donations,” with the wealthy essentially purchasing positions that their marks do not merit.
Meanwhile, high-achieving students who just barely missed the cutoff have to find alternatives – or pursue another field altogether.
Those whose families can afford it often end up studying medicine abroad.
Many do not return to India, depriving the country of their much-needed expertise.
Some return after having attended obscure colleges in countries like Georgia or China, only to have the MCI refuse to recognize their degrees and block them from practicing.
For those who cannot afford to go abroad – even bright students who barely missed the cutoff for a spot at an Indian university – studying medicine is no longer an option.
Yet India desperately needs doctors.
According to the World Health Organization, the country has just 0.7 doctors per 1,000 people.
In the United States and the United Kingdom – two countries to which Indian doctors often emigrate – the rate is 2.5 per 1,000 and 2.8 per 1,000, respectively.
The crippling lack of capacity means that lives are lost every day – particularly in rural areas – for want of medical attention.
India could be graduating four or five times as many capable doctors as it does each year.
Yet the MCI has been allowed to pursue its restrictive approach, depriving poor Indians of adequate health care, while augmenting the already-huge pressure on students to gain a seat in a medical college.
It is in this context – with a huge population competing for a tiny number of seats in professional colleges – that coaching institutes like those in Kota thrive.
When succeeding in tough entrance examinations is the only way to fulfill one’s educational goals, test preparation becomes the be-all and end-all of schooling.
Eager to satisfy pushy parents, young people sacrifice their own interests at the altar of a false god.
The 56 pyres lit in Kota over the last five years are a tragic testament to how damaging this conception of academic excellence can be.
The Kremlin and the US Election
CAMBRIDGE – In early November, US President Barack Obama reportedly contacted Russian President Vladimir Putin personally to warn against cyber attacks aimed at the American presidential election.
The previous month, the Director of National Intelligence, James Clapper, and Jeh Johnson, the Secretary of Homeland Security, publicly accused Russia’s most senior officials of using cyber attacks to “interfere with the US election process.”
In the aftermath of the November 8 election, no firm evidence has emerged that hacking interfered with voting machines or other electoral machinery.
But in an election that turned on 100,000 votes in three key states, some observers argue that Russian cyber interference in the political process may have had a significant impact.
Can such Russian behavior be deterred in the future?
Deterrence always depends on who and what one is trying to deter.
Ironically, deterring states from using force may be easier than deterring them from actions that do not rise to that level.
The threat of a surprise attack such as a “cyber Pearl Harbor” has probably been exaggerated.
Critical infrastructures such as electricity or communications are vulnerable, but major state actors are likely to be constrained by interdependence.
And the United States has made clear that deterrence is not limited to cyber retaliation (though that is possible), but can target other sectors with any tools it chooses, ranging from naming and shaming and economic sanctions to nuclear weapons.
The US and others, including Russia, have agreed that the laws of armed conflict apply in cyberspace.
Whether a cyber operation is treated as an armed attack depends on its consequences, rather than on the instruments used.
It would have to result in destruction of property or injury or death to individuals.
But what about deterring operations that are not equivalent to an armed attack?
There are gray areas in which important targets (say, a free political process) are not strategically vital in the same way as the electrical grid or the financial system.
Destroying the latter could damage lives and property; interference with the former threatens deeply held political values.
In 2015, a United Nations Group of Government Experts (including the US, Russia, China, and most states with significant cyber capabilities) agreed to a norm of not targeting civilian facilities in peacetime.
This agreement was endorsed by the G20 countries at their summit in Turkey in November 2015.
When an anonymous cyber attack interfered with the Ukrainian electric grid the following month, some analysts suspected the Russian government of using cyber weapons in its continuing hybrid warfare against Ukraine.
If true, it would mean that Russia had violated the agreement it had just signed.
But how should one interpret Russian behavior in regard to the American election?
According to US officials, Russian intelligence agencies hacked into the email accounts of important Democratic Party officials and provided the materials to WikiLeaks to dribble out over the course of the campaign, thereby ensuring a continuous steam of news stories that were unfavorable to Hillary Clinton.
This alleged Russian disruption of the Democratic presidential campaign fell into a gray area that could be interpreted as a propaganda response to Clinton’s 2010 proclamation of a “freedom agenda” for the Internet or retaliation for what Russian officials saw as her critical comments about Putin’s election in 2012.
Whatever the motive, it looked like an effort to skew the US political process – precisely the type of nonlethal political threat that one would want to deter in the future.
The Obama administration had previously made efforts to rank the seriousness of cyber attacks, but without sorting out the ambiguities of these gray areas.
In 2016, Obama faced difficult choices in estimating the escalatory potential of responding with cyber measures or with a cross-sector response such as sanctions.
The administration did not want to take steps that might themselves disrupt the election.
So, eight days before the vote, the US sent Russia a warning about election meddling over a hotline – created three years earlier to deal with major cyber incidents – that connects the Nuclear Risk Reduction Centers in both countries.
Because Russian hacking activity seemed to slow or halt, the Obama administration cited the warning as a successful exercise in deterrence.
But some critics say the Russians had already achieved their main goals.
Three weeks after the election, the administration said that it remained confident in the overall integrity of America’s electoral infrastructure, and that the election was free and fair from a cyber-security perspective.
But intelligence officials continued to investigate the impact of a broader Russian information-warfare campaign, in which fake stories about Clinton appeared intended to influence voters.
Many of the false reports originated from RT News and Sputnik, two state-funded Russian outlets.
Should this be treated as traditional propaganda or something new?
A number of critics believe that the level of official Russian state involvement in the 2016 US election process crossed a line and should not be dismissed as a form of tolerable gray-area behavior.
These critics have urged the Obama administration to go further in naming and shaming, by providing a fuller public description of what US intelligence knows about Russia’s behavior, and by imposing financial and travel sanctions against high-level Russian officials who are identified.
Other US officials, however, are reluctant to jeopardize the intelligence means used for attribution, and are wary of escalation.
Russia’s involvement in the 2016 US election was a watershed.
With important elections coming in a number of Western democracies, analysts will be watching closely to see what lessons the Kremlin draws from it.
Kremlin Musical Chairs
It’s that time again – Russia’s pre-election season when prime ministers are changed as in a game of musical chairs.
The last one seated, it is supposed, will become Russia’s next president.
As the end of his rule approached, Boris Yeltsin went through at least a half-dozen prime ministers, looking for the one who would ensure the security not only of Russia’s new democracy and market economy, but also of his “family” and the wealth that it had accumulated during his rule.
The last man seated then was, of course, Vladimir Putin.
Now it is Putin’s turn to call the tune, dismissing Mikhail Fradkov and dissolving the government that had served him throughout his second term in order to prepare for the parliamentary elections looming in December and the presidential ballot in March 2008.
In 1999, Yeltsin picked Putin, who was then the little-known head of the FSB (formerly the KGB).
Putin chose to elevate the equally mysterious Victor Zubkov, head of the Federal Financial Monitoring Service (also known as the “finance espionage” agency).
Despite that similarity, the reasoning behind these choices appears to be somewhat different.
Yeltsin’s choice of Putin – encouraged, ironically, by Boris Berezovsky, the prominent Russian oligarch and Yeltsin advisor who is now exiled in London as Putin’s mortal enemy – was based on his belief that the quiet apparatchik, even if a former KGB spy, was a democrat at heart.
After all, Putin had been a protegé of Anatoly Sobchak, the liberal mayor of St. Petersburg as communism collapsed.
A security services insider, Putin was seen as well placed to protect Yeltsin and his oligarchic allies.
Indeed, Berezovsky intended to continue ruling the country from behind the scenes, first as Yeltsin’s health failed in the final months of his presidency, and then by controlling the successor he had helped to choose.
In Russia, however, the KGB is famous for turning the tables in any struggle with the Kremlin apparat.
So no one but Yeltsin and Berezovsky was surprised when Putin, their supposed marionette, began pulling the strings.
And pull them he did, turning Berezovsky into an international villain, exiling former media mogul Vladimir Gusinsky, jailing the oil magnate Mikhail Khodorkovsky, and eventually imposing a new authoritarian regime behind the façade of Yeltsin’s democratic institutions.
Putin’s own game of prime ministerial “musical chairs” does not reflect a desire to secure for himself a quiet position behind the scenes while someone else rules, for he knows all too well that the path from the Kremlin leads only to inner exile and the grave.
Stalin replaced the dying Lenin, Khrushchev denounced Stalin, Brezhnev banished Khrushchev to his dacha, and Gorbachev buried Chernenko.
Only Yeltsin did things differently.
He disliked his predecessor, Mikhail Gorbachev, as much his predecessors disliked their predecessors.
But all the same he treated Gorbachev in a more decent manner because Yeltsin fundamentally believed in democracy.
So he left Gorbachev a private life that could also be lived in public.
Putin, of course, did not accost the retired Yeltsin, but he didn’t have to.
He simply ignored him while reversing his achievements in building a free Russia.
Before Zubkov’s nomination, reports swirled that the next prime minister would become Putin’s presidential successor, with Sergei Ivanov, a current deputy prime minister, dubbed the most likely candidate.
But Ivanov, who is perceived as “strong,” would provide unwelcome competition to Putin, who, after all, remains a “strong” president.
Had he anointed Ivanov now, Putin’s power would already begin seeping away.
The outgoing Fradkov, surprisingly, put the matter best when he explained why he had resigned: with elections approaching, Putin needed a free hand.
So Zubkov’s nomination allows Putin to continue to keep his cards – and thus ultimate power in Russia – close to his chest.
Of course, Zubkov will continue Fradkov’s “Yes, whatever you say Mr. President” management style.
Moreover, his former position as head of the Federal Financial Monitoring Service will allow him to draw on a wealth of information to keep tabs on all possible enemies and competitors, perhaps turning them into new model Berezovsky’s, Gusinsky’s and Khodorkovsky’s, if necessary.
The only question now is whether Zubkov, or his successor, will eventually succeed in turning Czar Vladimir into the same sort of non-person that Putin’s rivals have become.
Paul Krugman and the Obama Recovery
NEW YORK – For several years, and often several times a month, the Nobel laureate economist and New York Times columnist and blogger Paul Krugman has delivered one main message to his loyal readers: deficit-cutting “austerians” (as he calls advocates of fiscal austerity) are deluded.
Fiscal retrenchment amid weak private demand would lead to chronically high unemployment.
Indeed, deficit cuts would court a reprise of 1937, when Franklin D. Roosevelt prematurely reduced the New Deal stimulus and thereby threw the United States back into recession.
Well, Congress and the White House did indeed play the austerian card from mid-2011 onward.
The federal budget deficit has declined from 8.4% of GDP in 2011 to a predicted 2.9% of GDP for all of 2014.
And, according to the International Monetary Fund, the structural deficit (sometimes called the “full-employment deficit”), a measure of fiscal stimulus, has fallen from 7.8% of potential GDP to 4% of potential GDP from 2011 to 2014.
Krugman has vigorously protested that deficit reduction has prolonged and even intensified what he repeatedly calls a “depression” (or sometimes a “low-grade depression”).
So much for Krugman’s predictions.
Not one of his New York Times commentaries in the first half of 2013, when “austerian” deficit cutting was taking effect, forecast a major reduction in unemployment or that economic growth would recover to brisk rates.
On the contrary, “the disastrous turn toward austerity has destroyed millions of jobs and ruined many lives,” he argued, with the US Congress exposing Americans to “the imminent threat of severe economic damage from short-term spending cuts.”
As a result, “Full recovery still looks a very long way off,” he warned. “And I’m beginning to worry that it may never happen.”
I raise all of this because Krugman took a victory lap in his end-of-2014 column on “The Obama Recovery.”
The recovery, according to Krugman, has come not despite the austerity he railed against for years, but because we “seem to have stopped tightening the screws: Public spending isn’t surging, but at least it has stopped falling.
And the economy is doing much better as a result.”
That is an incredible claim.
The budget deficit has been brought down sharply, and unemployment has declined.
Yet Krugman now says that everything has turned out just as he predicted.
In fact, Krugman has been conflating two distinct ideas as if both were components of “progressive” thinking.
On one hand, he has been the “conscience of a liberal,” rightly focusing on how government can combat poverty, poor health, environmental degradation, rising inequality, and other social ills.
I admire that side of Krugman’s writing, and, as I wrote in my book The Price of Civilization, I agree with him.
On the other hand, Krugman has inexplicably taken up the mantle of crude aggregate-demand management, making it seem that favoring large budget deficits in recent years is also part of progressive economics.
(Krugman’s position is sometimes called Keynesianism, but John Maynard Keynes knew much better than Krugman that we should not depend on mechanistic “demand multipliers” to set the unemployment rate.)
But we should pay for this through higher taxes on high incomes and high net worth, a carbon tax, and future tolls collected on new infrastructure.
We need the liberal conscience, but without the chronic budget deficits.
There is nothing progressive about large budget deficits and a rising debt-to-GDP ratio.
After all, large deficits have no reliable effect on reducing unemployment, and deficit reduction can be consistent with falling unemployment.
Krugman is a great economic theorist – and a great polemicist.
But he should replace his polemical hat with his analytical one and reflect more deeply on recent experience: deficit-cutting accompanied by recovery, job creation, and lower unemployment.
This should be an occasion for him to rethink his long-standing macroeconomic mantra, rather than claiming vindication for ideas that recent trends seem to contradict.
Krugman’s Anti-Cameron Contradiction
NEW YORK – It is truly odd to read Paul Krugman rail, time and again, against the British government.
His latest screed begins with the claim that “Britain’s economic performance since the financial crisis struck has been startlingly bad.”
He excoriates Prime Minister David Cameron’s government for its “poor economic record,” and wonders how he and his cabinet can possibly pose “as the guardians of prosperity.”
Hmm.
In recent months, Krugman has repeatedly praised the US economic recovery under President Barack Obama, while attacking the United Kingdom’s record.
But when we compare the two economies side by side, their trajectories are broadly similar, with the UK outperforming the United States on certain indicators.
Consider, first, the unemployment rate.
In the fourth quarter of 2007, the UK’s rate was 5.2%.
When Cameron’s government took office in May 2010, it was 7.9%.
In the most recent reporting period (November 2014-January 2015), it was 5.7%.
In the US, the unemployment rate in the fourth quarter of 2007 was 4.8%, 9.8% in March-May 2010, and 5.7% in November 2014-January 2015.
In the US, by contrast, the employment rate was 62.8% at the end of 2007, 58.6% in March-May 2010, and then only slightly higher, at 59.2%, during November 2014-January 2015 – still below the pre-crisis level.
This suggests that there are more discouraged workers in the US than in the UK.
Finally, there is output growth.
In the UK, real (inflation-adjusted) GDP fell by 3.8% from the fourth quarter of 2007 to the second quarter of 2010.
It then rose by 8.1% from that point until the fourth quarter of 2014.
In the US, real GDP fell by 1.6% from the fourth quarter of 2007 to the second quarter of 2010, and then rose by 10.5% from then until the fourth quarter of 2014.
Thus, both countries have experienced moderately high and broadly similar growth rates since May 2010, when Cameron’s government took power.
Yes, UK growth has been slightly slower, but the British economy has also faced, among other factors, the headwind of sharply falling North Sea oil production during this period, whereas the US benefited from a shale-oil boom.
Obviously, neither of these long-term trends can be fairly attributed to the governments currently in office.
In any case, during the past two years, from the fourth quarter of 2012 to the fourth quarter of 2014, the US economy grew by a cumulative 5.6% while the UK economy grew by 5.4%, essentially the same as the US.
Krugman seems to make much of the fact that the UK did not bounce back even more strongly from a larger output decline between the fourth quarter of 2007 and the fourth quarter of 2010 – a fall that occurred before the Cameron government took office.
That is true, and measured UK productivity growth has remained low, but nobody can be sure why.
Perhaps the unsustainable pre-2008 bubble was larger in the UK; perhaps the UK’s structure (particularly the larger share of finance in its GDP and the continued decline in energy output) made the initial downturn less reversible.
The UK has been more vulnerable than the US to the eurozone’s prolonged crisis.
Moreover, subtle differences between the US and UK in national income accounting should be taken into account in comparing productivity trends.
The International Monetary Fund’s estimate of the output gap (see its World Economic Outlook database) for both countries does not suggest that, as of 2014, the UK is more cyclically depressed than the US.
So, if Krugman praises the Obama recovery, he should also praise the Cameron recovery.
They are very similar.
Perhaps more notable is that both the US and UK economies have cast considerable doubt on Krugman’s oft-repeated view that a robust recovery would require further fiscal stimulus, a position that he maintained at least until 2013.
The truth is that – barring another Greek tragedy – both the UK and US are finally out of the post-2008 crisis.
It is time for both countries to move beyond short-term macroeconomic policy and turn their attention to the challenges of long-term sustainable development.
Kuwaiti Democracy in Action
Kuwait has just held its eleventh parliamentary election since independence in 1961.
Though Kuwait is a monarchy, its parliamentary history has not been placid, and the election campaign reflected ongoing tensions between the royal family and segments of the electorate.
Originally scheduled for October 2007, the election was brought forward to break a deadlock between the parliament and the government over the number of electoral districts in the country.
Currently, there are 25 constituencies nationwide, but reformers have long argued that a smaller number – each with a larger number of voters – would be less susceptible to manipulation by the political elite.
The 29 members of parliament who support a reduced number of constituencies were unable to agree with the government on a new number.
As a result, the issue became the focus of a vigorous campaign by discontented Kuwaitis, who gathered in front of the National Assembly building and in universities to voice their criticisms.
In response, the government removed the redistricting issue from parliament to the constitutional court – an attempt, reformers argued, to hold back change.
When three members of parliament sought to question the Prime Minister – their right, under the Kuwaiti constitution – the parliament was dissolved and elections called.
The rules of political life in Kuwait have been worked out over 300 years – since this small area emerged as a self-contained polity.
The royal family is empowered to administer the country, but through consultation with different sectors, like tribal leaders or merchant groups.
With independence in 1961, these rules were codified in a constitution.
Compared to other Gulf monarchies, the Kuwaiti royal family has limited powers, though constitutional rules have frequently been ignored—charges of vote-rigging marred the 1967 election, and parliamentary dissolution has dotted the country’s history.
Nevertheless, since the liberation of Kuwait following the Iraqi occupation in August 1990, the parliament has gained both stability and respect.
The dissolutions of parliament in 1999 and 2006 were carried out according to the law, and elections were called in a timely fashion.
Parliament retains the right to interrogate cabinet members, including the Prime Minister, on financial and administrative questions and has frequently passed motions of no confidence in government.
More recently, with the death last December of the Emir, Sheikh Jabir al-Ahmad, parliamentary approval was exercised a key role in resolving succession debates among factions of the royal family.
Overall, Kuwait has always had a lively political life, with freedom of expression and less political repression than is common in neighboring countries.
Political violence has been rare.
Though political parties have not been legalized, they are nonetheless active in parliament and public life.
Kuwaiti life reflects the many and varied political trends throughout the Arab world, as the country’s open society and economic opportunities have made it a powerful magnet and regional safety valve since the 1950’s.
With this election, Kuwaiti women participated in choosing the parliament for the first time.
Though women have long been active in Kuwaiti public life, they were enfranchised only last year, after a long and difficult campaign, by a parliamentary vote, and accounted for just 28 of the 249 candidates.
The voting population is 57% female, and commentators speculated on the likely impact of women voters.
Would they follow the voting patterns of their husbands?
Clearly, women demonstrated tremendous enthusiasm for the political process, taking part in large numbers in political rallies, and as campaign workers in a range of election activities.
In the end, while a loose opposition coalition of pro-reform ex-MP’s, Islamists, and liberals gained a majority of the National Assembly’s 50 seats, no woman won.
The electorate focused almost entirely on the question of electoral districts.
Most Kuwaitis see this very technical issue as a way of pressing for reform.
Major regional issues, like the war in Iraq or Iran’s nuclear program, were barely mentioned.
Islamist candidates were in an awkward position, seeking support from women voters, whose participation they opposed.
However, Islamists were among the 29 parliamentary deputies pushing for constituency reform, a fact that eased their dilemma.
Support for reform comes from across the political spectrum, whereas opponents of reform paid a high price.
Indeed, three members of the outgoing parliament who supported the government were not candidates, having lost in their tribal primaries.
With pro-reform forces strengthening their position in the new parliament, the government has two choices. It can go along with electoral reform, or it can find an opportunity to dissolve the new parliament.
Already, commentators are speculating on how long the new parliament will survive.
Whatever the immediate outcome, the Kuwaiti government is facing a new political horizon.
Women are now the majority of voters, and young people have taken up the reform banner.
Democracy will push the ruling family forward.
Kuwait’s Parliamentary Revolution
The world has been transfixed by the victory of Hamas in the Palestinian election.
But a different assertion of democratic and parliamentary power, this time in the Gulf sheikdom of Kuwait, which possesses 10% of world oil reserves, may prove to be equally important.
Every sign indicates that the wave of democratization in Kuwait is irreversible, and the impact of these changes extends beyond Kuwait to all the other oil-rich Gulf countries, which are also ruled by emirs and sheikhs.
Indeed, these rulers now have much to ponder.
The death of Kuwait’s ruler, Sheikh Jaber al-Sabah, on January 15, 2006, was followed by unprecedented national disquiet, which led to the rapid abdication of his designated successor, Saad Al Sabah.
Nothing like this had ever hit the Al Sabah family, which has ruled Kuwait for two centuries.
Traditionally, the role of ruling Emir alternated (according to a tacit agreement) between two rival branches of the Al Sabah family – the Al Jaber and the Al Salem.  The succession was always strictly a family affair, and any disputes remained behind closed doors.
However, with Sheikh Jaber al-Sabah’s death, the succession was not only subjected to feverish public debate, but the Kuwaiti press and Parliament were key actors in determining the outcome.
Kuwait’s political system is considered the most modern among the Arab Gulf sheikhdoms and monarchies, because all citizens – men and women alike – elect its parliament.
Elections with universal suffrage, combined with a relatively free press, meant that the succession became a public issue, debated in the media and by academics for months as Sheikh Jaber was dying.
With the taboo on discussing the succession broken, talk about the physical and mental fitness of the likely successor became commonplace, and members of the ruling family who objected to the prospect of being ruled by a severely incapacitated crown prince came to enjoy broad support.
Thus, immediately after Jaber al-Sabah’s death, the succession became a national concern.
What in the past would have been a straightforward palace coup spilled out onto the pages of newspapers and into the corridors of Parliament.
The Crown Prince’s abdication became inevitable.
The cabinet of ministers affirmed the succession of Sabah al-Sabah as the new ruler, which was then ratified by Parliament.
For the region, this was an historic moment.
For the first time, an Arab parliament had voted a head of state out of office and asserted its will in choosing the successor.
Parliamentary supremacy, one of the key issues in any country’s democratization, appears to be at hand in Kuwait.
Of course, Kuwait will now experience enormous tensions between the transparent rule of a true parliamentary government and the still powerful legacy of a hidden family-run state.
But the old rule of force and intrigue by which other Gulf rulers have been deposed has been replaced by a modern principle – the Al Sabah have surrendered their exclusive control over the succession to the will of Parliament, which alone could provide the legitimacy that the new Emir needs.
The consequences of this assertion of parliamentary authority will be enormous.  Parliamentary ratification did not simply provide a rubber stamp to a palace coup; its approval was conditional.
Emirs who need parliamentary approval to secure popular legitimacy must now reckon with the need to share power.
In exchange for voting to pass over the unfit Crown Prince, Kuwait’s parliament is now demanding even more political and economic reforms, including the formal legalization of political parties, the separation of the position of prime minister from that of crown prince, and even for the prime minister to be chosen from outside the Al Sabah family.
This was a true parliamentary revolution.
According to Kuwait’s constitution, the new ruler has one year to appoint a crown prince, but he has to appoint a prime minister immediately.
The delay is significant, because Kuwaitis are unaccustomed to this kind of parliamentary influence.
The new Emir will now need to navigate skillfully between the Al Sabah family’s factions and a newly empowered parliament.
The victory of Parliament in Kuwait’s succession crisis is likely to lead some of Kuwait’s neighboring autocracies to stiffen their resolve against modernity and democracy.
But Kuwait shows that there will invariably come a moment when holding onto feudal ways is no longer an option.
The Kuwaiti model may be risky, but the alternative – simply ignoring the need for change – could prove fatal.
Kyoto’s Misplaced Priorities
When the Kyoto treaty enters into force on February 16, the global warming community will undoubtedly congratulate itself: to do good they have secured the most expensive worldwide treaty ever.
They have succeeded in making global warming a central moral test of our time.
They were wrong to do so.
Global warming is real and is caused by emissions of carbon dioxide (CO2).
But existing climate models show we can do little about it.
Even if everyone (including the United States) applied the Kyoto rules and stuck to them throughout the century, the change would be almost immeasurable, postponing warming for a mere six years in 2100 while costing at least $150 billion a year. 
Global warming will mainly harm developing countries, because they are poorer and therefore less able to handle climate changes.
However, by 2100, even the most pessimistic forecasts from the UN expect the average person in the developing countries to be richer than now, and thus better able to cope.
So Kyoto is basically a costly way of doing little for much richer people far in the future.
We need to ask ourselves if this should be our first priority.
Of course, in the best of all worlds, we would not need to choose our priorities.
We could do all good things.
We could win the war against hunger, end conflicts, stop communicable diseases, provide clean drinking water, improve education and halt climate change.
But we can’t do everything.
So we must ask the hard question: what should we do first? 
Some of the world’s top economists – including three Nobel Laureates – answered this question at the Copenhagen Consensus last May.
They found that dealing with HIV/AIDS, hunger, free trade, and malaria were the world’s top priorities, where we could do the most good for our money.
Moreover, they put urgent responses to climate change at the bottom of the list.
In fact, the panel called these ventures – including Kyoto – “bad projects,” because they cost more than the good they do.
As the economics of climate change has become ever clearer, warnings from the global warming community have become shriller.
For example, the head of the UN Climate Panel says, “We are risking the ability of the human race to survive.” 
Such statements make headlines, but they are nonsense.
For example:
· At a recent meeting at Exeter in the UK, some participants warned of a 50-50 chance that the Gulf Stream winds could collapse within a century.
Such a scenario looks great in the movie “The Day After Tomorrow,” but it is unsubstantiated.
As one presenter at the conference summarized: “No models have shown a complete shutdown, or a net cooling over land areas.
Hence a shutdown during the twenty-first century is regarded as unlikely.”
· Recently, a coalition of prominent environmental and development organizations claimed that malaria would increase in a warmer world.
This has some theoretical validity, but ignores malaria’s dependence on poor infrastructure and health care.
Indeed, throughout the cold 1500-1800’s, malaria was a major disease in Europe, the US, and far into the Arctic Circle.
Malaria infections didn’t end because it got colder (it actually got warmer), but because Europe and the US got rich and dealt with the problem.
With developing countries getting richer over the century, malaria is similarly likely to decrease rather than increase.
· We are told that sea levels will rise – by roughly 50 centimeters by 2100 in some scenarios.
This is correct, and it will clearly cause problems in low-lying countries like Bangladesh.
But the alarmists neglect to mention that sea levels rose through the twentieth century by 10-25 centimeters.
It makes us believe that we only have one choice.
Yet the reality is that we can choose to make climate change our first priority, or we can choose to do a lot more good first.
To say this is not to suggest laissez faire.
Far from it.
With Kyoto, the world will spend $150 billion a year on doing little good a century from now.
In comparison, the UN estimates that half that amount could buy clean drinking water, sanitation, basic health care, and education for every single person in the world.
Which is better?
Global warming really is the moral test of our time, but not in the way its proponents imagine.
We need to stop our obsession with global warming and start dealing with more pressing and tractable problems first.
Labor in a World of Financial Capitalism
The traditional hostility between labor unions and the world of finance should not obscure their common interest in using financial tools in an expansive and creative way.
We live in an age of financial capitalism, and the only intelligent way forward – for unions and other workers’ associations – is for these bodies to help their members make increasingly sophisticated use of the tools of risk management.
The traditional boundaries between labor and capital are becoming blurred.
For example, companies increasingly augment standard wage packages with stock options, even for rank-and-file employees.
In the United States, the Labor Department reports that in 2003, 14% of US workers in firms with 100 or more employees were offered stock options.
Expect more such packages in the future.
The problem is, most employees do not fully understand options or stocks and do not know how to evaluate them.
A recent paper by MIT Professors Nittai Bergman and Derk Jenter suggests that management tends to award employee options when employees are excessively optimistic about the outlook for company stock – thereby in effect opportunistically substituting overpriced options for full pay.
Unions and workers associations are the natural vehicles to monitor such behavior, but they must invest in the expertise to do so effectively.
They should not stand in the way of compensation that includes stock options, or that otherwise create financial risks for their employees.
But they should make sure that such programs are administered in employees’ interest, because companies that encourage their employees to hold options or to invest directly in the company’s stock are asking them to take on some of the company’s risks.
To be sure, sharing ownership can help employee morale. But it also creates an unhealthy concentration of risks: not only the employee’s job, but also his assets now depend on the company’s fate.
The scandal at Enron, in its final days, was that management prevented employees from selling their Enron shares while executives unloaded their own shares.
Obviously, unions need to be alert to such bad behavior.
More generally, they must examine employee ownership programs both sympathetically and analytically, in order to suggest ways to hedge the risks they create.
The same is true of other financial tools.
Labor unions have long pointed with satisfaction at hard-won contracts that specified a defined-benefit pension for their members.
But these unions were often without the financial sophistication to judge whether the firm set aside sufficient capital to meet its commitments decades later.
In the US, union failure to represent members’ interests adequately contributed to a major pension default at the Studebaker Corporation in 1963.
The AFL-CIO, the United Auto Workers and the United Steel Workers then petitioned Congress – against strong opposition from business interests – to establish the Pension Benefit Guaranty Corporation in 1974 to insure private pensions against companies’ failure to honor them.
Gradually, many countries, with prodding from their trade unions, now have some form of benefit protection plan for private pensions.
The latest example is the United Kingdom, where trade unions have spurred the creation of the Pension Protection Fund, which will begin operating next year.
But it is not clear that these plans will succeed fully.
Declining stock markets and low interest rates, have left private pensions around the world more vulnerable than was expected.
Moreover, the risks to pension funds may correlate with risks to other economic factors affecting specific groups of workers.
This means that unions should not leave the complex financial problems of designing pensions entirely to governments.
Local unions need to be involved, for the issues involved are specific to companies and workers, and cannot be resolved by governments alone.
Indeed, the essence of labor unions is that they know the unique problems of a distinct group of workers, bring focused expertise on these problems, and thus intelligently represent their interests.
In today’s complex financial economies, representing workers’ interests is not so simple as battling with management for a bigger share of the pie.
Unions should instead negotiate with management the same way top executives do with their boards of directors when their complex compensation packages are worked out.
Unfortunately, instead of learning how to think in financially sophisticated ways, we still see labor unions in Europe and elsewhere dwelling too much on job security for working people.
But making it difficult for firms to lay off workers provides only an illusory benefit for workers, for it compromises companies’ ability to compete, and weakens their incentive to create jobs.
The alignment of incentives lies at the heart of modern financial theory.
Labor unions should negotiate with management about providing appropriate risk management to their employees in financial forms: the right kinds of insurance, options, and other investments to protect them realistically without guaranteeing their employment and without jeopardizing the productivity of the firm.
Complex financial capitalism is here to stay, and we all have to learn to live with it.
Union leaders must study finance rather than condemn it as evil.
They must develop a cadre of professionals with a sophisticated understanding of risk management, and must work to educate their members in the financial subtleties of their specific circumstances.
Labor in the Age of Robots
DAVOS – Fears about the impact of technology on the labor market are nothing new.
In the early nineteenth century, a group of English textile workers known as the Luddites worried that new technologies like power looms and spinning frames would cost them their jobs.
They protested by smashing the machines.
Today, anxiety that new technologies could destroy millions of jobs is as high as ever.
In the midst of a major employment crisis, technology continues to reduce the labor needed for mass production, while the automation of routine legal and accounting tasks is hollowing out that sector of the job market as well.
The science of robotics is revolutionizing manufacturing; every year, an additional 200,000 industrial robots come into use.
In 2015, the total is expected to reach 1.5 million.
Adapting the labor market to a world of increasingly automated workplaces will be one of the defining challenges of our era.
Yet no country can afford to ignore the transformation.
Globally, some 200 million people are unemployed, up 27 million since 2008.
There is a critical need to anticipate coming technological changes and provide the global workforce with the education and skills needed to participate in the modern labor market.
Worldwide, one-third of employers surveyed complain that they are unable to find workers with the right skills for existing vacancies.
Efficient paths from training and education programs to the world of work must be built, so that skills can be matched to market demand.
Government programs must be strengthened, and employers and trade unions must assume greater responsibility for investing in skills.
They also must consult more closely with educators and policymakers – discussions that should be informed by labor-market information, performance reviews, and the availability of employment services.
Whatever a country’s development level, investment in education and skills will increase the ability of its workforce to innovate and adapt to new technologies.
Such investment can determine whether a country’s economic growth is broadly inclusive or leaves large segments of society behind.
An abundant supply of workers who have been appropriately trained and can continue to learn boosts investor confidence and thus job growth.
In addition to training the labor force for an age of further automation, sustainable economies must offer protections for workers in good times and bad.
The nature of a worker’s relationship with his or her employer is changing.
People entering the labor market are increasingly finding only short-term or temporary contracts; often, they are forced to take informal work or emigrate for a job.
These trends are exacerbating income inequality.
As a result, mitigation policies are necessary.
Along with a robust system of unemployment benefits, social protections such as healthcare and pensions are essential for overall worker security and to ensure a healthy economy.
And yet only 20% of the world’s population has adequate social-security coverage; more than half lack any coverage at all.
That is why the work of the International Labor Organization, which was established in 1919, is still relevant today.
In a world of increasingly automated workplaces and eroding employee-employer relationships, the values encoded in the ILO’s labor standards are more necessary than ever.
The complex challenges facing workers worldwide will require complex solutions.
In 2013, the ILO launched its Future of Work initiative, which seeks to identify and analyze incipient trends and provide a forum for discussion about what must be done to adapt to rapidly changing labor-market conditions.
Our world has changed vastly over the past century – and not only because of technology.
By 2050, the global population will surpass nine billion.
The number of people aged 60 years and over will have tripled.
Three-quarters of the elderly will be living in what are now developing countries, and the majority of them will be women.
These demographic shifts will further revolutionize the labor market, social-security systems, economic development, and the world of employment.
For all of the progress human society has made since the era of the Luddites, a simple truth persists: machines must strengthen, not weaken, our prospects for inclusive growth and broadly shared prosperity.
We must ensure that the modern economy is a sustainable one, built on the principles of human dignity and the opportunity for decent work.