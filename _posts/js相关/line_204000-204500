I might not do it, but I would nevertheless defend the right of someone who decides otherwise.
It is debatable whether recent incidents of this kind require a “dialogue between religions.” Public debate making clear cases one way or the other seems more appropriate than conciliation.
The gains of enlightened discourse are too precious to be turned into negotiable values.
Defending those gains is the task that we now face.
To Err is Science
Until now, discovery was often considered the main goal of medical science.
But nowadays discovery is almost too easy.
Anyone with a little funding and a few biological specimens in a refrigerator can make thousands of postulated “discoveries.”
Indeed, the number of research questions that we can pose is increasing exponentially.
Medical kits the size of a thumbnail can measure a million different biological factors on an individual with an infinitesimal amount of blood.
A million research questions can be asked on the spot.
But even with proper statistical testing, many tens of thousands of these biological factors may seem to be important due to mere chance.
Only a handful of them really will be.
The vast majority of these initial research claims would yield only spurious findings.
So the main issue nowadays is to validate “discoveries” by replicating them under different settings.
Several different teams of researchers need to see them “work” again and again using common rules.
Moreover, all the teams should agree not to select and report only the data that seem most impressive.
With selective reporting, we would end up with a long list of all the false discoveries made across all research teams, with only a few true findings buried among this pile of non-replicated waste.
In fact, empirical data suggest the significance of this danger.
In a paper in the Journal of the American Medical Association published in July 2005, I showed that refutation is very common, even for the most prestigious research findings.
I examined the 45 clinical research findings that had received the greatest recognition in the scientific world, as documented by the number of times other scientists had cited them over the last 15 years.
Even with the most robust types of research – for example, randomized clinical trials – one of four of these results had already been found to be wrong or potentially exaggerated within a few years after publication.
For epidemiology (e.g. studies on vitamins, diet, or hormones in terms of their association with health outcomes in the general population), four-fifths of the most prestigious findings were rapidly contradicted.
For molecular research, in the absence of extensive replication, the refutation rate may occasionally exceed 99%.
But we should not panic.
It is to be expected that the majority of research findings are rapidly contradicted and refuted; indeed, it is part of how progress of science occurs.
However, we need to adapt to this situation.
Instead of taking scientific evidence as dogma, we should consider it as tentative information that should be ascribed a level of credibility.
There is nothing wrong with disseminating scientific information that has a credibility of 10%, or even 1%.
Sometimes, this will be the best evidence we have.
But we should get used to understanding that some research findings have very low credibility, while others may be more likely to stand the test of time.
Scientists themselves may be able to ascribe these levels of credibility to their own work in fairness, if they describe in detail what they set out to do, and how they did it.
Science is a noble pursuit, but genuine progress in scientific research is not easy to achieve.
It requires a lot of time, continuous effort, uncompromising integrity, appropriate funding and material support, and unwavering commitment.
Proposed scientific advances require careful validation and replication by independent scientists.
Scientific knowledge is never final, but it evolves continuously.
This is part of the great fascination of science, and it fosters liberty of thought.
While these principles are probably well known to serious scientists, they are often forgotten when scientific information is disseminated.
Our society is awash with inflated information, which is inherent to efforts in many human activities – entertainment, law courts, stock markets, politics, and sports, to name but a few – to gain greater public attention in the framework of mass civilization.
But it would be a damaging to expect science to “show off” in this way.
Exaggeration contradicts the key hallmarks of scientific reasoning: critical thinking and careful appraisal of the evidence.
The Nigerian Kidnappers’ Ideology
LONDON – The abduction of more than 240 Nigerian girls has shocked the world.
But, unfortunately, their case is not an isolated one in Nigeria.
Indeed, Nigeria’s torment is shared by many other African countries, and the motivation behind the kidnapping derives from an ideology that is global.
That ideology is based on a warped and false view of religion.
It is taught in formal and informal school settings worldwide.
Of course, the hideous and crazed words of the leader of Boko Haram, the group that kidnapped the girls, are representative only of the most extreme fringe of this ideology.
But, until we clean the soil in which this poisonous plant takes root, it will continue to blight the life chances of millions of young people around the world – and jeopardize our own security.
Across Sub-Saharan Africa, this problem is now vast.
Mali, Chad, Niger, the Central African Republic, Somalia, Kenya, and even Ethiopia have all suffered or face acute anxieties about the spread of extremism.
Many other countries have now identified extremism as their single most important challenge.
Governments are often confronting the challenge with courage and determination, and the use of African forces in many countries to try to keep peace is a tribute to that resolve.
But the fact is that the problem is continuing to grow.
This is not by accident.
When I became Prime Minister of the United Kingdom in 1997, Nigeria served as an example of productive cooperation between Christians and Muslims.
The destructive ideology represented by Boko Haram is not part of the country’s traditions; it has been imported.
As the population grows, so will the problem.
Nigeria has approximately 168 million people today, with some estimates putting the population at 300 million by 2030, split roughly equally between Christian and Muslim.
Without a climate of peaceful coexistence, the consequences for the country – and the world – will be enormous.
Poverty and lack of development play a huge part in creating the circumstances in which extremism incubates.
But poverty alone does not explain the problem.
And a major factor now holding back development is terrorism.
Who would invest in northern Nigeria under current conditions?
How can local economies thrive in such an atmosphere?
This challenge is not confined to Africa.
The Middle East, as we know, is immersed in a process of revolution and upheaval that has been immensely complicated by Islamism and its extremist offshoots.
In Pakistan, more than 50,000 people have lost their lives in the terror attacks of the last decade.
Violence linked to the same ideology has taken innocent lives and destroyed communities in India, Russia, Central Asia, and the Far East as well.
What is that ideology?
Here is the crux of the issue.
Because misrepresentation follows any pronouncement on this question, let me state some things very clearly.
This ideology does not represent Islam.
The majority of Muslims do not agree with it.
They are repulsed by it.
This should give us hope about the future.
But this ideology is a strain within Islam that represents an organized, substantial, powerful, and funded minority.
What might loosely be called Islamism is based on a politicization of religion that is fundamentally incompatible with the modern world, for it assumes that there is one true religion, only one interpretation of that religion, and that this interpretation should prevail and dominate all countries’ politics, government institutions, and social life.
Those who do not share this view must be overcome.
This Islamist ideology is a spectrum.
At one extreme are groups like Boko Haram.
Other groups may not advocate violence (though sometimes they do) but still preach a view of the world that is dangerous and hostile to those who disagree.
To see what I mean, read the Muslim Brotherhood’s statement in 2013 denouncing the UN women’s declaration for, among other things, defending women’s right to travel or work without their husbands’ permission.
It is the ideology, not just the acts of extremism, that must be confronted.
My foundation, which provides practical support to help prevent religious prejudice, conflict, and extremism, has been active in Nigeria for several years, bringing together Christian and Muslim clerics to foster mutual understanding.
In more than 20 countries worldwide, we have schools programs that connect children of different faiths to learn about each other.
Even in the most challenging places, the results are clear and powerful.
In Sierra Leone, where we are part of the campaign against malaria, we mobilize churches and mosques to work in their local communities and help families use bed nets effectively to protect themselves against a disease that still kills 750,000 pregnant women and children each year in Africa.
We have reached two million people in an act of compassion and care, with results that are as remarkable as the interfaith cooperation that produces them.
So the battle is not lost.
But it has to be seen for what it is.
Every year, the West spends billions of dollars on defense relationships and on fighting terrorism.
Yet the very thing we are fighting is given license to grow in the education systems of many of the countries with which we are engaged – even in our own.
Education today is a security issue.
The G-20 should agree that open-minded education that promotes religious tolerance should be a responsibility of all countries.
We should insist upon it in our own school systems – and then insist upon it in others’ systems.
Nigeria’s kidnapped girls are victims not just of an act of violence but of a way of thinking.
If we can defeat that ideology, we will begin to make progress toward a more secure world.
Education is a Security Issue
LONDON – In November, I spoke at the United Nations Security Council for the first time in 13 years.
It struck me how different the mood is now.
In September 2000, the world seemed very different.
We were trying to articulate the new security order in the decade after the fall of the Berlin Wall.
Of course there were challenges.
But the atmosphere was light, positive even, as we discussed eradicating poverty in the developing world.
This time, the mood was dark.
And the first days of 2014 have made it darker still.
Scroll down any day’s news summary and you find stories of terrorism and violence perpetrated in the cause of a false view of religion.
Some of it is committed by non-state actors, and some of it by state actors; but all of it is committed in the context of division and conflict defined by differences of religious faith.
This is the new struggle of the twenty-first century.
We will not win it unless we fight its root causes as well as its ghastly consequences.
Today, in an arc that stretches from the Far East through the Middle East to the streets of cities in Europe and the United States, we face a scourge that has taken innocent lives, scarred communities, and destabilized countries.
It is a threat that is constantly evolving, growing, and mutating to counter our fight against it.
The extremists propagating this violence have networks of outreach to young people and know the power of education, whether formal or informal.
Extremists are filling young minds with the belief that anyone who disagrees is an enemy – and not just their enemy, but God’s enemy.
The security debate has understandably often focused on the consequences.
After an attack, states consider immediate security measures.
Terrorists are hunted down.
Then we get back to our daily lives, until the next time it happens.
But lasting change depends on dealing with the root causes of extremism.
Of course politics plays its part.
And the extremists are good at jumping on the back of political grievances.
But the soil in which they plant the seeds of hate is fertilized with ignorance.
That is why we need to start thinking of education as a security issue.
The extremists justify killing in the name of God.
This is an obscene perversion of proper religious faith.
And it is a menace, both for the harm that it does directly and for the damaging division and sectarianism that it nurtures indirectly.
Every killing is a human tragedy.
But it also causes a chain reaction of bitterness and hatred.
There is real fear in the communities plagued by such extremism, fear that paralyzes normal life and pushes people away from each other.
Globalization is intensifying and multiplying this extremism.
Not limited by borders, it can spring up anywhere.
We are more connected than at any point in human history, and more and more people come into contact with those who are different from them.
So the need to respect a neighbor who is not like you is much greater; but the scope to identify him or her as an enemy is also greater.
And this is not only about Islamic extremism.
There are extremist acts perpetrated against Muslims because of their religion, and today there are fanatical Christians, Jews, Hindus, and Buddhists who disfigure the true nature of their faith.
That is why education in the twenty-first century is a security issue for all of us.
The challenge is to show young people who are vulnerable to appeals from terrorists that there is a better path to having their voice heard, a more meaningful way to engage with the world.
The good news is that we know how to do this.
I use my Faith Foundation only as one example.
Our schools program promotes cross-cultural dialogue among students aged 12-17 around the world.
Reaching students in more than 20 countries, our program connects students via a secure Web site, where they interact from their classrooms under the guidance of trained teachers.
Through facilitated videoconferences, students discuss global issues from a variety of faith and belief perspectives.
They gain the dialogue skills required to prevent conflict by breaking down religious and cultural stereotypes.
For schools in the poorest areas, we use special arrangements, because they cannot access the Internet themselves.
To be sure, we are only a drop in the ocean.
But we now have experience in more than a thousand schools; over 50,000 students have been taught, and we are working in countries as diverse as Pakistan, India, the US, Jordan, Egypt, Canada, Italy, the Philippines, and Indonesia.
I have been privileged to witness these students becoming at ease with the cultures, faiths, and beliefs that inspire so many people around the world.
There are many other fantastic examples of this type of work.
But they lack the resources, weight, and recognition that they need.
We need to mobilize to defeat extremism.
And we need to act globally.
All governments must take seriously their responsibility to educate young people to accept and respect people of different faiths and cultures.
There is no issue that is more pressing.
There is a real danger that religious conflict replaces the ideologically based struggles of the last century in an equally devastating form.
It is up to all of us to show people that we have a better idea than the extremists have – to learn from each other and live with each other.
And this needs to be a core part of young people’s education.
Tony Blair’s Long Goodbye
In early 1999, Paddy Ashdown, then the leader of Britain’s Liberal Democratic Party (and since then, as Lord Ashdown, Europe’s envoy in Bosnia), was found with a woman not his wife and forced to resign his post.
In his diaries, he describes calling on Prime Minister Tony Blair to inform him in advance of his intention to quit:
“Blair said: ‘Going is the most difficult thing to do in politics.
Too many people stay for too long.
I would rather stop when people said, “Why is he going?” than when they said, “Why isn’t he going?”
Or, even worse, “When is he going?”
I hope I will be able to do it the same way.’”
This leaves us with an enduring mystery.
Britain’s most adept and skillful politician has evidently known for years exactly what not to do about arranging his departure, and yet he has chosen to ignore his own advice.
The mystery deepens when we recall that this consideration has been a part of Blair’s calculations ever since he became leader of the Labour Party in 1994.
At a dinner in a London restaurant named Granita, in what has since become the best-known coffee-stage chat in British history, Blair made a proposal to Gordon Brown, his rival for the leadership.
That proposal fell in two parts.
He, Blair, was demonstrably more “electable,” and should lead Labour in deposing the ramshackle Tory regime of John Major.
Then, with Labour in power, Brown could expect in due time to receive the mantle.
On this condition, Brown agreed to give Blair a clear run.
That was three elections ago.
What has kept Blair going?
When I called on him in January this year, his press officer advised me not to bring up the obvious question.
(I readily agreed, since an unanswerable question is a waste of time.)
But no sooner had I asked the Prime Minister how he was than he replied with a grin: “It’s nice to know one doesn’t have to fight another election.”
So there was the topic, inescapably, right in the middle of the room.
For the rest of the conversation, and on the trip to the outskirts of London that I also took with him, Blair talked and acted as if he had a full Prime Ministerial agenda on everything from global warming to the reconstruction of Afghanistan.
He also behaved, when talking to voters and citizens, as if he was tirelessly running for office for a fourth time.
Some of the motivations for this are purely human: he likes being Prime Minister and is good at it.
Moreover, next year he will have been Prime Minister for a decade, longer than any previous Labour leader.
A little longer, and he would outlast Margaret Thatcher’s record-breaking tenure, which must have been a temptation.
But Blair inexplicably chose to compound the mistake he had made with Brown, by announcing publicly, after having defeated the Tories for the third straight time, that he would not stand again.
From then on, there was really only one question on peoples’ minds, and it was the third – the worst – of the three questions he had mentioned to Ashdown: “When is he going?”
Blair ought to have known that politics is a pitiless business.
For years, his backbench members of parliament kept quiet because they knew that they owed their seats, and their majority, to him.
Now, with the country insisting on an answer to the question he posed, they see him as a liability.
And the trade unions, whose power he has done so much to reduce, have been open in saying that they want a new party leader.
Thus, his announcement that he will leave Downing Street next year is no more than a reluctant acceptance of what has been reality for some time.
No politician is free from a sense of destiny, and I think that Blair’s got the better of him.
In the decision to send British forces to defend Sierra Leone from a barbaric invasion from Liberia, he faced down all those who warned of disaster and won great moral credit.
In deploying soldiers to Afghanistan and Iraq, he was convinced that he was both morally correct and politically right to stand by Britain’s main historic ally, the United States.
(It is reasonably certain that he would not have trusted Brown to do any of these things in the face of any serious opposition, and also reasonably certain that he was correct to think so.)
When I first interviewed Blair, as newly elected Labour leader in 1994, he answered my question about the role of his Christianity in his politics by saying, “I can’t stand politicians who go on about religion.”
If I had to date the moment when my own misgivings about him began, it would be the time – starting after September 11, 2001 – when he began to emphasize his own “faith” as a motivating factor in his moral stand.
A saving element in British politics is that such appeals are usually considered embarrassing.
They may also suggest a slight tendency, on the part of those uttering them, to believe in some kind of supernatural endorsement.
So Blair’s concession that he must leave office, a decision so long postponed and so disastrously protracted, represents among other things a triumph of the mundane over the permanent temptation to believe that politics is about anything else.
Tony Blair’s Palestine
LONDON –Former British Prime Minister Tony Blair has many positive attributes, including great charm.
He will need all his skills to address the bewildering range of global tasks that he has taken on since being shoehorned out of office by his dour successor, Gordon Brown.
His initial daytime job, after running Britain, was to bring peace to the Middle East by helping establish the governing institutions of a Palestinian state.
Since then, Blair has become an adviser to banks (which need all the advice they can get these days), is touring the world to promote a sensible policy on global warming and climate change, has created a foundation to help bridge the divide between different faiths, and will lecture on religion at Yale.
All that is left is to restore the fortunes of England’s national football and cricket teams.
Perhaps he could fit that in on weekends.
But Blair has just made a useful comment on Palestine and Israel, which deserves to be taken seriously.
Throughout the long years of this bloody tragedy, we have tried to inch our way to a settlement through confidence-building measures or, in the case of the long dead “Road Map,” through pushing both parties to take parallel steps towards an agreement.
Some observers, not least hard-headed Israeli peace campaigners, have suggested a different approach.
You will never succeed, they say, if you try to bob and weave your way slowly towards an end game.
Instead, you should jump straight into a final deal.
And, since you won’t get the two sides to agree to it, you’ll have to impose it from the outside.
But that ambitious outcome is easier described than achieved.
While Israeli public opinion has usually appeared to run well ahead of its political leaders in the approach to peace, it is difficult to see how one could act over their heads.
They need to be pushed and shoved into a successful negotiation.
What would it mean to go straight to Palestinian statehood?
Presumably, Blair is not proposing to the Palestinians the creation of a state before an agreement is reached on final borders.
There cannot be a Palestinian state without dealing with West Bank settlements.
If you don’t believe me, just visit the West Bank and see, for example, how the proposed suburban Israeli development of East Jerusalem stabs through the heart of Palestinian territory towards the Dead Sea.
How can you have a viable state carved up by fences, military roads, and barbed wire?
Any Palestinian state should be within the 1967 borders (as adjusted through negotiation).
Peace activists on both sides solved that in the Geneva initiative.
Yasser Arafat and Ehud Barak came close to doing so at Camp David almost eight years ago.
Moreover, a Palestinian state would not only comprise the West Bank and Gaza, but presumably would also have to accommodate the principal political parties in each area.  Attempts to destroy Hamas – whether politically or physically – have not worked and cannot work.
The Americans and Europeans committed a major error in conspiring to destroy the Fatah-Hamas national unity government, which was created largely thanks to the diplomacy of Saudi Arabia and other Arab League countries.
I hope that Blair is saying that to his American friends.
His greatest achievement was the peace deal in Northern Ireland.
That historic triumph depended on bringing in Sinn Fein politicians – leaders of the Irish Republican movement who in many cases could not be distinguished from the IRA, which bombed, shot, and maimed civilians in pursuit of its political goals.
Why should what worked in Northern Ireland – indeed, what was pressed on Britain by the United States – be unthinkable in the Middle East?
Are we in the West guilty of double standards yet again?
I abhor any and every terrorist act, by Hamas or anyone else.
I have had friends killed by terrorists.
But since when were sentiment and moral denunciation sufficient ingredients of a policy?
And when did a disproportionate military response to terrorism ever work?
The third challenge in establishing a Palestinian state is to create the institutions of statehood: hospitals, ports, airports, roads, courts, police stations, tax offices, and government archives.
When I was a European Commissioner, we provided funds from European taxpayers to pay for these things.
Then we saw them systematically trashed by Israel’s response to the second intifada .
How did destroying driving licenses in Palestine preserve Israeli security?
How was it preserved by digging up runways, uprooting olive trees, and fouling wells?
A Palestinian state will need to be built from the bottom up.
And what is built should not be destroyed.
I want to see – and I’m sure Blair does, too – a peaceful Palestinian state next door to a secure Israel in a region united in prosperity and stability.
Maybe Blair has hit on how this can be achieved.
But he should dwell upon the implications of such an approach between board meetings, lectures, and photo calls.
Tony Blair’s Poisoned Legacy
Tony Blair has a powerful claim to being one of the most successful British politicians of any recent generation, at least in domestic economic and social policy.
But history will remember him mainly for his strategic error in going to war in Iraq.
During his ten years in power, Blair and his Chancellor of the Exchequer, Gordon Brown, gave Britain one of the longest periods of economic stability, relatively high growth, and low unemployment that it had ever known.
In this respect, Blair’s premiership marked a fundamental break with the Labor Party’s tax-and-spend tradition.
It also established a new tradition of stability in economic policy, continuing and reinforcing the previous Conservative government’s commitment to fiscal discipline and low inflation.
Stable economic policy and rapid growth, in turn, enabled Blair’s government to pour extra money into education and the National Health Service.
Yet Blair’s domestic legacy is a mood of disillusion and mistrust, especially of Blair himself.
One reason is that a significant portion of Blair’s party (which he renamed “New Labor”) never reconciled itself to the primacy that he gave to free-market principles over its old Socialist or Social Democratic values.
Another is that Blair consistently seemed to pay much less attention to Parliament than to the right-wing tabloid press: the spin and media manipulation to which his office devoted so much effort worked wonders at first, but soon generated deep skepticism and mistrust.
But the main reason for Britain’s popular disillusion with Blair comes down to his role in the Iraq war, which was launched with the ostensible aim of pre-empting Iraq’s use weapons of mass destruction (WMD).
Of course, as we now know, WMD were never found, and, worse, evidence came to light showing that Blair was aware that the Bush administration was committed to regime change, regardless of their existence.
The infamous Downing Street memorandum of July 23, 2002, eight months before the outbreak of war, stated explicitly that “The intelligence and the facts were being fixed around the policy.”
In short, when Blair took Britain to war, he deliberately misled Parliament and the electorate about the ostensible rationale for it.
When no WMD were found, Blair fell back on the previously unavowed justification that the removal of Saddam Hussein was the “right thing to do.”
Unfortunately, the Bush administration did not foresee that Saddam’s removal would precipitate a multi-sided insurgency and civil war.
These dangers were predictable, and the world now knows that the war has proved a terrible failure of American strategy, and may yet have even more catastrophic consequences.
So why did Blair support it?
Part of the answer goes back to Blair’s first big foreign policy adventure, NATO’s intervention in Kosovo in 1999.
In 1998-1999 Serbia had embarked on violent repression of Kosovo’s ethnic Albanian majority, driving roughly 400,000 Kosovars from their homes. Diplomatic efforts by the outside world proved ineffective, so in March 1999, NATO began a 78-day bombing campaign against the Serbs.
Blair was at the forefront among Western leaders in pressing for NATO action, and to justify this innovation in outside intervention he proclaimed “a new doctrine of international community” that made it a “just war,” because it was based on superior Western values.
One of Blair’s character flaws is self-righteousness, an excessive confidence – perhaps an extrapolation of his Christian faith – that he knows what is correct, and is therefore entitled to act accordingly.
In the case of the Iraq war, Blair has never conceded that the war was mistaken in conception or disastrous in practice: for him, it was still “the right thing to do.”
Blair leaves office before the full consequences of the Iraq war have been played out.
There is no sign of an end to the sectarian violence, and little prospect that Iraq can become a peaceful, unitary democracy.
Moreover, the convulsions in Iraq may well have incalculable repercussions throughout the Middle East.
One consequence, from a British perspective, is that it is now virtually impossible to imagine that any future prime minister could engage in another big military adventure primarily out of loyalty to an American president.
Indeed, it is possible that Blair, through his complicity in the Iraq war, has inflicted major damage on the very idea of a “special relationship” between Britain and the United States.
This relationship was always more special for the British than for the Americans, who scarcely recognize its existence.
But if it has now been discredited in the minds of the British people, the result may be a new element of independence into British strategic thinking.
Another consequence is that Britain’s moral standing has been damaged alongside that of the US. But the damage to America’s moral position is more serious for the health of the world.
It is uncertain whether Tony Blair could have dissuaded George Bush from waging war in Iraq. Nevertheless, by enthusiastically backing Bush’s war, his legacy will forever remain deeply compromised.
Still Too Big to Fail
WASHINGTON, DC – Nearly seven years after the global financial crisis erupted, and more than five years after the passage of the Dodd-Frank financial-reform legislation in the United States, the cause of the crisis – the existence of banks that are “too big to fail” – has yet to be uprooted.
As long as that remains the case, another disaster is only a matter of time.
The term “too big to fail” dates back several decades, but it entered wide usage in the aftermath of the collapse of Lehman Brothers in September 2008.
As problems spread throughout the financial system, the US authorities decided that some banks and other financial companies were so large relative to the economy that they were “systemically important” and could not be allowed to go bankrupt.
Lehman failed, but AIG, Goldman Sachs, Morgan Stanley, Citigroup, Bank of America, and others were all rescued through various forms of massive – and unprecedented – government support.
The official line at the time was “never again,” which made sense in political and economic terms.
These large financial firms were provided a scale of assistance that was not generally available to the nonfinancial corporate sector – and certainly not to families who found that the value of their assets (their homes) was below the value of their liabilities (their mortgages).
If large, complex financial institutions continue to have an implicit government guarantee, many people – on both the right and the left – would agree that this is both unfair to other parts of the private sector and an inducement for big banks to engage again in excessive risk-taking.
In the jargon of economics, this is “moral hazard.”
But no special training is needed to know that it is unwise and dangerous when bank executives get the upside (huge bonuses) when things go well and everyone else bears the downside risks (bailouts and recession).
At the heart of the Dodd-Frank law is a two-pronged approach to the too-big-to-fail problem.
The first section of the legislation, Title I, stipulates that all firms must be able to go bankrupt without causing large-scale damage to the broader financial system or the real economy.
Regulators are instructed, in no uncertain terms, to make sure that all large financial firms are structured in such a way that bankruptcy, using the standard rules and procedures of the court system, can happen without repeating the catastrophic post-Lehman cascade.
In Title II of Dodd-Frank, Congress created a back-up authority through which the Federal Deposit Insurance Corporation (FDIC) can take over and manage a failing financial firm and impose appropriate losses on shareholders and some creditors without creating widespread systemic damage or a global panic.
The good news is that, over the past half-decade, the FDIC has made some progress formulating the design of a workable Title II.
The bad news is that there has been almost no progress in terms of ensuring that large financial firms actually can go bankrupt.
In a hearing this week before a part of the Senate Banking Committee, there was complete agreement across the political spectrum on this point.
The disagreement concerns what must be done to finish this important piece of Dodd-Frank business.
The Republican proposal is to modify the bankruptcy code, creating special provisions for large, complex financial institutions.
There are three problems with this approach.
First, all companies in the US should be able to fail under the same rules.
Privileged treatment for anyone perpetuates the perception that it is safer to lend to some large financial firms – and further strengthens their unfair advantage.
Second, it is fanciful to believe that the private sector would want to get involved in providing funding to a huge financial firm under court supervision, particularly during a systemic crisis.
The definition of such a crisis is precisely that moment when private-sector loans are not readily available.
And a large loan – in the tens of billions of dollars – provided by the US Treasury to a bankruptcy court judge is unlikely to be politically acceptable or economically sensible.
Finally – and most fatally – the bankruptcy of any large US financial firm today would induce a scramble for assets by regulators around the world.
Some foreign regulators – such as the Bank of England – have agreed not to act preemptively in a resolution process run by the FDIC.
But such agreements do not apply to a court-run bankruptcy process; authorities everywhere would move to protect local creditors and taxpayers by seizing assets in their jurisdiction.
The only reasonable alternative is to make large, complex financial institutions smaller and less complex so that it is possible for them to fail under standard bankruptcy rules.
This is the intent of Dodd-Frank.
The FDIC has pushed hard in this direction, whereas the Federal Reserve Board of Governors has been less enthusiastic.
But the law is the law, and it is time to implement it.
Too Big to Jail
WASHINGTON, DC – Among the fundamental principles of any functioning justice system is the following: Don’t lie to a judge or falsify documents submitted to a court, or you will go to jail.
Breaking an oath to tell the truth is perjury, and lying in official documents is both perjury and fraud.
These are serious criminal offenses, but apparently not if you are at the heart of America’s financial system.
On the contrary, key individuals there appear to be well compensated for their crimes.
As Dennis Kelleher of Better Markets has argued, the recent so-called “robo-signing” settlement – in which five large banks “settled” their legal liability for carrying out fraudulent foreclosures on mortgages – is a complete sell-out to the financial industry.
First, there was no serious criminal prosecution – meaning that no one will be charged with a felony, and no one will go to jail.
In terms of affecting executives’ incentives, this is the only thing that matters.
Even the terminology used to frame the discussion is wrong.
Kelleher, an attorney with extensive experience in private practice and the public sector, tells it like it is: “‘robo-signing’ is massive, systematic, fraudulent, criminal conduct.”
Alternatively, as he points out, we could just call it “lying, cheating, and stealing.”
Second, the civil penalties in this settlement – a form of fine – are minuscule relative to the size of the companies involved.
As Shahien Nasiripour, one of the best reporters on this issue, dryly put it: “None of the five lenders have said they expect to incur a material charge due to the settlement.”
In other words, from a corporate perspective, the penalty is a trifling affair.
Third, such fines are, in any case, paid by the companies’ shareholders, not by their executives or board members (all of whom carry insurance).
In the rare cases in which fines have been levied on individuals, either their insurance policies picked up most of the bill, or the penalties were trivial relative to the cash compensation that they received while committing their crimes – or both.
As if all of this weren’t bad enough, the banks reportedly will be able to use government money to write down the value of mortgages, which amounts to subsidizing them to pay their own meaningless fines.
The Obama administration and its allies have worked hard to sell its roughly $20 billion settlement with the banks as one that will have a meaningful impact on the housing market.
But nothing could be further from the truth.
As Kelleher points out, the United States has “more than 10 million homes under water” (the outstanding mortgage exceeds the house’s value).
“Twenty billion dollars doesn’t make a dent in that: one million homes at $20,000 loan forgiveness is it.”
In fact, the Obama administration’s settlement with the mortgage lenders is consistent with its track record on all of its policies related to the financial sector, which has been abysmal.
But it is also puzzling.
Why would the administration continue to bend over backwards to be lenient towards top bankers under these circumstances?
I honestly do not believe that the administration’s stance reflects any form of corruption – payments made to individuals or even to political campaigns.
And, in this case, it does not even appear to reflect the lobbying power of big financial players.
That power certainly explains why the Dodd-Frank financial reforms enacted in 2010 were not stronger, and why there is now so much opposition to effective implementation of that legislation (for example, there is currently a huge fight around the “Volcker rule,” which would limit proprietary trading by megabanks).
But mortgage lenders’ criminal activities are another matter.
Indeed, at stake in the mortgage settlement are fundamental and systemic breaches of the rule of law – perjury and fraud on an economy-wide scale.
The Justice Department has, without question, all of the power that it needs to prosecute these alleged crimes fully.
And yet America’s top law-enforcement officials have consistently – and now completely – backed off.
The main motivation behind the administration’s indulgence of serious criminality evidently is fear of the consequences of taking tough action on individual bankers.
And maybe officials are right to be afraid, given the massive size of the banks in question relative to the economy.
In fact, those banks are bigger now than they were before the crisis, and, as James Kwak and I documented at length in our book 13 Bankers, they are much larger than they were 20 years ago.
Top bankers want to make a lot of money. They also want to stay out of prison.
Political leaders can huff and puff as much as they want, but, without a credible threat of poverty and time behind bars, bankers have no reason to comply with the law.
For them, it’s all about the trade – and you can be the sucker in public policy as easily as you can be the sucker in an individual loan agreement.
The message to bank executives today is simple: build your bank to be as big as possible – and then keep growing.
If you manage to become big enough, you and your employees are not just too big to fail, but also too big to jail.
The Obama administration has just made everyone else the sucker.
Too European to Fail
BERLIN – Among investment bankers, there is renewed speculation about the possibility of a country leaving European Monetary Union – or being pushed out.
Rating agencies have downgraded Portugal, Greece, and Spain, owing to their poor prospects for economic growth and weak public finances.
Ireland has been assigned a negative outlook and could soon suffer a downgrade as well.
With fears mounting that one or another euro-zone country may default, yield spreads on government bonds between EMU countries have reached record highs.
For some time now, Greek ten-year government-bond yields have been about 300 basis points above German yields.
This is a sign that investors now see a significant risk of a Greek default or of Greece exiting from EMU and redenominating its government bonds. 
But the panic that EMU may disintegrate is overdone.
Rather than a default and subsequent exit from the euro zone, the member states are more likely to overrule a fundamental principle of EMU and bail out a fellow member state.
Leaving EMU would be a costly option for weak-performing countries.
Of course, regaining the exchange rate as an instrument for competitive devaluation could help overcome competitiveness losses due to soaring unit-labor costs.
As Argentina showed after its default and devaluation in the winter of 2001/2002, such a move can reignite exports and economic growth.
But Greece, Portugal, Italy, and Ireland are not Argentina.
The European Union treaty does not provide for an exit from EMU.
As a default would demand a change in the currency involved in private contracts, business partners from other EU countries would surely sue, legal uncertainties would drag on and on, and an exiting country’s trade with its main trading partners would be impaired for years to come.
So none of the countries at risk would seriously consider reintroducing a national currency.
It is possible that an EMU government will come close to insolvency or illiquidity.
Indeed, even something like a self-fulfilling debt crisis is thinkable: if market participants believe that a default or an exit from EMU is imminent, this would drive up government bond yields for the country in question.
At some point, yields might become so high that the government concerned could not refinance its maturing debt or finance its current expenses.
In this situation, a country would be faced with the sole option of default.
But even if this scenario is possible in theory, it is unlikely to occur in practice.
Letting one member fail would create speculative pressure on other EMU governments with weak fiscal positions or shallow domestic bond markets.
A widespread default of several countries in EMU would lead to serious disruptions of trade within the EU and to new problems in the banking system, which would have to write down their holdings of government bonds.
The large EU governments are well aware of this problem and would act accordingly.
With its loans to Hungary and Latvia, the European Commission has already revived a credit facility which was dormant since the European Monetary System crisis in 1992 – and for countries outside EMU. The support is huge.
Together with the International Monetary Fund’s contribution, the EU loan to Latvia amounts to more than 33% of Latvian GDP.
By supporting to such a degree two new EU members that are not part of EMU, the EU countries have demonstrated a much greater commitment to mutual aid than was thinkable only a few years ago.
And more is expected to come: in November 2008, the Ecofin Council increased the ceiling for possible balance-of-payment loans to non-EMU countries to €25 billion.
Against this background, it seems inconceivable that the EU should refuse to support an EMU member in a situation similar to that of Hungary and Latvia, especially as all the countries that are currently on market watch lists are long-time EU members.
In case of real distress, the rest of the EU will offer a bailout package.
Indeed, the no-bailout clause in Article 103 of the EU Treaty – according to which neither the European Central Bank, the EU, nor national governments “shall […]amp#160; be liable for or assume the commitments” for other national governments – has lost its credibility.
The political and economic costs of letting a fellow member government fail are simply too high in the closely interconnected EMU.
Because there is no current blueprint for this move, one can only speculate about what it would look like.
Too Much Saving, Too Little Investment
Talk abounds of a global savings glut.
In fact, the world economy suffers not from too much saving, but from too little investment.
To remedy this, we need two kinds of transitions.
How well the world makes them will determine whether the strong global growth of the last few years will be sustainable.
This is the central message of the IMF’s World Economic Outlook, which will be released this week [editors: on Wednesday, September 21st 2005] on the eve of the Fund’s 2005 Annual Meeting.
First, consumption has to give way smoothly to investment, as past excess capacity is worked off and as expansionary policies in industrial countries normalize.
Second, to reduce the current account imbalances that have built up, demand has to shift from countries running deficits to countries running surpluses.
Within this second transition, higher oil prices mean consumption by oil producers has to increase while that of oil consumers has to fall.
The current situation has its roots in a series of crises over the last decade that were caused by excessive investment, particularly the bursting of the Japanese asset bubble, crises in emerging Asia and Latin America, and the collapse of the IT bubble in industrial countries.
Investment has fallen off sharply since, and has since staged only a very cautious recovery.
The policy response to the slowdown in investment differs across countries.
In the industrial countries, expansionary budgets, coupled with low interest rates and elevated asset prices, has led to consumption- or credit-fueled growth, particularly in Anglo-Saxon countries.
Government savings have fallen, especially in the United States and Japan, and household savings have virtually disappeared in some countries with housing booms.
By contrast, the crises were a wake-up call in many emerging-market countries.
Historically lax policies have become far less accommodative.
Some countries have primary fiscal surpluses for the first time, and most emerging markets have brought down inflation through tight monetary policy.
With corporations cautious about investing and governments prudent about expenditure – especially given the grandiose investments of the past – exports have led growth.
Many emerging markets have run current-account surpluses for the first time.
We should celebrate the implicit global policy coordination that enabled the world to weather the crises of recent years. However, the fact that rich countries are consuming more, and are being supplied and financed by emerging markets, is not a new world order; it is a temporary and effective response to crises.
Now it needs to be reversed.
Indeed, it is misleading to term this situation a “savings glut,” for that would imply that countries running current-account surpluses should reduce domestic incentives to save.
But if the problem is weak investment, then a reduction in such incentives will lead to excessively high real interest rates when the factors holding back investment dissipate.
Policy, instead, should be targeted at withdrawing excessive stimulus to consumption and loosening the constraints that are holding back investment.
There are reasons to worry whether the needed transitions will, in fact, occur smoothly.
First, with asset prices like housing fueled by global liquidity, goods prices kept quiescent by excess capacity and global trade, and interest rates held down by muted investment, domestic and external imbalances have been easily financed.
The traditional signals provided by inflation expectations, long-term interest rates, and exchange rates have not started flashing.
Instead, bottlenecks are developing elsewhere, as in oil.
It may well be that easy financing has given economies a longer leash.
The worry, then, is that when the signals change – as they must – they will change abruptly, with attendant harsh consequences for growth.
Alternatively, prices such as that of oil will have to move more in order to effect the most pressing transitions, creating new imbalances.
Policymakers should not see higher oil prices as an aberration to be suppressed, but should focus on underlying causes.
Second, more investment is needed, particularly in low-income countries, emerging markets, and oil producers (though less in China, the exception that proves the rule).
But the answer is not a low-quality investment binge led by government or fuelled by easy credit; we know the consequences of that.
Instead, product, labor, and financial markets must be reformed so that high-quality private-sector investment emerges.
It is here that the good may have been the enemy of the perfect.
Strong exports and decent government policies have enabled some countries to generate growth without the reforms that can create the right incentives for investment.
These countries are overly dependent on demand elsewhere, which in turn is unsustainable.
With the right reforms, adopted in concert, the world can make the needed transitions.
But one of the risks associated with the large current-account imbalances we now face is that politicians may decide to blame one or another country and espouse protectionist policies.
That could precipitate the very global economic downturn that we all want to avoid.
If, instead, countries see the transitions as a shared responsibility, each country’s policymakers may be able to guide the domestic debate away from the protectionism that might otherwise come naturally.
Each country should focus on what it needs to do to achieve sustained long-term growth.
In that possibility lies the well-being of us all.
Too Much “Too Big to Fail”?
LONDON – Obviously, the global financial crisis of 2008-2009 was partly one of specific, systemically important banks and other financial institutions such as AIG.
In response, there is an intense debate about the problems caused when such institutions are said to be “too big to fail.”
Politically, that debate focuses on the costs of bailouts and on tax schemes designed to “get our money back.”
For economists, the debate focuses on the moral hazard created by ex ante expectations of a bailout, which reduce market discipline on excessive risk-taking – as well as on the unfair advantage that such implicit guarantees give to large players over their small-enough-to-fail competitors.
Numerous policy options to deal with this problem are now being debated. These include higher capital ratios for systemically important banks, stricter supervision, limits on trading activity, pre-designated resolution and recovery plans, and taxes aimed not at “getting our money back,” but at internalizing externalities – that is, making those at fault pay the social costs of their behavior – and creating better incentives.
I am convinced that finding answers to the too-big-to-fail problem is necessary – indeed, it is the central issue being considered by the Standing Committee of the Financial Stability Board, which I chair.
But we must not confuse “necessary” with “sufficient”; there is a danger that an exclusive focus on institutions that are too big to fail could divert us from more fundamental issues.
In the public’s eyes, the focus on such institutions appears justified by the huge costs of financial rescue.
But when we look back on this crisis in, say, ten years, what may be striking is how small the direct costs of rescue will appear.
Many government funding guarantees will turn out to have been costless: liquidity support provided by central banks at market or punitive rates will often show a profit, and capital injections will be partly and sometimes wholly recovered when stakes are sold.
Emerging estimates of the total fiscal costs of rescue vary by country, but are usually just a few percentage points of GDP.
As a result of this crisis, however, government debt-to-GDP ratios in the United Kingdom and the United States will likely rise by 40 to 50 percentage points, and more important measures of economic harm – foregone GDP growth, additional unemployment, and individuals’ wealth and income losses – will rise as well.
All of this implies that the crucial problem is not the fiscal cost of rescue, but the macroeconomic volatility induced by precarious credit supply – first provided too easily and at too low a price, and then severely restricted.
And it is possible – indeed, I suspect likely – that such credit-supply problems would exist even if the too-big-to-fail problem were effectively addressed.
In the US, this crisis has seen over-exuberant commercial real-estate lending by regional banks.
If such banks fail, they are resolved by the Federal Deposit Insurance Corporation (FDIC) in the normal fashion, and this has always been the market’s ex ante expectation.
In the UK, similarly, we have had problems with mid-sized mortgage banks, and with poor commercial real-estate lending by mutual building societies, as well as problems with two large banks.
Ireland faces huge economic problems as a result of a commercial real-estate boom driven by banks that are relatively small by global standards.
There is therefore a danger that excessive focus on “too big to fail” could become a new form of the belief that if only we could identify and correct some crucial market failure, we would, at last, achieve a stable and self-equilibrating system.
Many of the problems that led to the crisis – and that could give rise to future crises if left unaddressed – originated elsewhere.
The Closed Marketplace of Economic Ideas
MILAN – Imagine that you fell asleep in 2006 and woke up today.
The world economy would be barely recognizable.
While you were dreaming of real-estate riches, the United States and Europe were hit by the most crippling financial crisis in almost 80 years, and China’s statist economy swiftly overtook Germany and Japan to become the world’s second largest (and, despite its recent slowdown, is poised to surpass the US).
Given such massive, unexpected shifts, you might be even more surprised by what didn’t change: the way economists think about themselves and their discipline.
To see this, one need look no further than the Ideas.RePEc.org website.
RePEc (Research Papers in Economics) arguably provides the closest thing to a credible hierarchy of economists, not unlike the ATP’s rankings of professional tennis players.
The site, entirely open and free (thanks to hundreds of volunteers in 82 countries), maintains a decentralized online database of around two million items of economic research, including working papers, journal articles, books, and software.
Its index of influence assesses the number of citations for each author, weighted by impact and discounted by citation age (otherwise, Adam Smith and Karl Marx would likely still top the list).
Because the ranking is updated every month, RePEc enables one to track which economists are viewed by their peers as the most influential over time.
So I compared the rankings from December 2006 and September 2015 to see whether the RePEc index had evolved along with economic reality.
It had not.
Despite the profound – and largely unpredicted – financial and economic turmoil of the intervening decade, the intellectual influence of those whose theories suffered the most evidently remains undented.
After a succession of bursting multi-trillion-dollar credit bubbles, you might wonder what to make of Robert Lucas’s view that rational expectations enable perfectly calculating “agents” to maximize economic utility.
You might also want to rethink Eugene Fama’s efficient markets hypothesis, according to which prices of financial assets always reflect all available information about economic fundamentals.
You must not be an economist.
In fact, Lucas and Fama both moved up in the RePEc rankings during the period I examined, from 30 to nine and from 23 to 17, respectively.
And the persistence at the top is striking across the board.
Among the top ten economists in September 2015, six were already there in December 2006, and another two were ranked 11 and 13.
Mobility in the RePEc rankings remains subdued even after widening the sample.
For example, of the top 100 economists in September 2015, only 14 were absent from the much wider top 5% in 2006, and only two others had advanced more than 200 spots over the previous decade.
Among those recently ranked from 101 to 200, just 24 were not in the top 5% in 2006, and only ten others had moved up by more than 200 places.
The rate of renewal among the 200 most influential economists was as low as 25% – and just 16% among the top 100 – during a decade in which the explanatory power of prevailing economic theory had been found severely wanting.
What is remarkable about this is the difference between the pace of change in the ranking of economists and in the economy itself.
Entry barriers among the world’s ten richest people and ten most valuable companies seem to be far lower than among the top ten economists.
According to Forbes, only two of the ten wealthiest individuals in 2015 (Bill Gates and Warren Buffett) were in the top ten in 2006.
And just three companies – ExxonMobil, General Electric, and Microsoft – made the top ten in terms of market capitalization in both 2006 and 2015.
In the rankings of economists, by contrast, criteria such as gender or geographic origin confirm the overall inertia.
Only four women made the RePEc top 200 in September 2015, compared to three in December 2006, and two were included on both lists.
Likewise, emerging countries – which represent more than 90% of the world’s population, three-quarters of global GDP growth over the last decade, and nearly half of total income in current dollar terms – supplied just 11 of the top 200 economists in September 2015, up from ten in December 2006.
And ten of those 11 – three Iranians, four Indians, two Turks, and one Chinese – have lived and worked in the US or the United Kingdom since their student days.
The rest of the RePEC top 200 tend to be Caucasian men in their 60s and older – roughly three decades past the age when an economic or scientific author is generally most innovative, according to research by the economist Benjamin Jones.
No black person, American or otherwise, is in the top 200.
How surprised should we be that, even after the Great Recession cast grave doubt on the rational-market theories so dominant a decade ago, the top tier of academic economics remains largely unchanged?
After all, many of these scholars have made tremendous, lasting contributions to understanding how markets and societies work.
And ideas tend to advance and retreat slowly, like glaciers, not precipitously, like armies.
But replace the names of the leading economists with products in any other market – cars, for example, or semi-conductors – and most people probably would agree that the RePEc ranking looks like a closed, inefficient market with high entry barriers.
Might the world’s leading economists be so keen to protect their own ideas that they ignore (or, worse, stifle) innovation from unexpected quarters?
For a group of people so committed to free markets and so enamored of “creative destruction,” that is a question that urgently needs to be addressed.
The answer may hold enormous implications not only for intellectual growth, but also for human welfare.
To the Victors Go the Foils
NEW YORK – A surprising number of elections and political transitions is scheduled to occur over the coming months.
An incomplete list includes Russia, China, France, the United States, Egypt, Mexico, and South Korea.
At first glance, these countries have little in common. Some are well-established democracies; some are authoritarian systems; and others are somewhere in between.
Yet, for all of their differences, these governments – and the individuals who will lead them – face many of the same challenges.
Three stand out.
The first is that no country is entirely its own master.
In today’s world, no country enjoys total autonomy or independence.
To one degree or another, all depend on access to foreign markets to sell their manufactured goods, agricultural products, resources, or services – or to supply them.
None can eliminate economic competition with others over access to third-country markets.
Many countries require capital inflows to finance investment or official debt.
Global supply and demand largely set oil and gas prices.