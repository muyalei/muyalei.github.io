It is a familiar story, one that is consistent with the global pattern of this terrible disease, which claims a life every two minutes: those most in need of protection have the least access to it.
Of the 275,000 women and girls who die of cervical cancer every year, 88% live in developing countries, where mortality rates can be more than 20 times higher than in France, Italy, and the United States.
That is not just because vaccines are not readily available; it is also because women in these countries have limited access to screening and treatment.
Without prevention, they have no options if they get sick.
Alarmingly, in some of the wealthy countries, where both screening and treatment should be readily available, vaccine coverage now appears to be declining, raising a real danger that socioeconomically disadvantaged girls there will face a similar fate.
If it turns out that girls at risk of not receiving all three doses of the HPV vaccine are also those with an elevated risk of being infected and missing cervical screenings as adults, they may be slipping through not one but two nets.
It is still not clear why this is happening.
What we do know is that HPV is a highly infectious sexually transmitted virus, which is responsible for almost all forms of cervical cancer.
HPV vaccines can prevent 70% of these cases by targeting the two most common types of the virus, but only if girls have not yet been exposed to the virus, which means vaccinating them before they become sexually active.
Yet efforts to communicate this to the public have been met with skepticism from some critics, who argue that the vaccine gives young girls tacit consent to engage in sexual activity, ultimately leading to an increase in promiscuity.
However, quite apart from the evidence to the contrary, intuitively this makes no sense.
To suggest that giving girls aged 9-13 three injections over six months gives them a green light to engage in sex and sets them on a path to promiscuity is utter nonsense.
It is like saying that people are more likely to drive dangerously if they wear a seat belt; in fact, the opposite is more often the case.
Whether such attitudes and misinformation account for poor vaccine coverage in places like France and the US is still not known.
It may simply be that some parents or girls mistakenly believe that one shot of the HPV vaccine is enough to provide protection, or that some socially disadvantaged girls lack sufficient access to in-school vaccination services.
Or perhaps the cost of the vaccine is a barrier in some of these countries.
Whatever the reason, unless coverage for all three doses increases, cervical cancer and pre-cancer rates will increase.
In countries like Rwanda, people know this only too well, which is why they have been so eager to tighten the net on HPV.
They have seen the horrors of cervical cancer, with women in the prime of their lives presenting with late-stage disease and suffering slow and painful deaths.
Without changes in prevention and control, deaths from cervical cancer worldwide are projected to rise almost two-fold by 2030, to more than 430,000 per year.
And now, with help from my organization, the GAVI Alliance, a public-private partnership created to improve access to new vaccines for the world’s poorest children, other low-income countries are following Rwanda’s lead.
As of this year, Ghana, Kenya, Laos, Madagascar, Malawi, Mozambique, Niger, Sierra Leone, Tanzania, and Zimbabwe have all taken steps to introduce HPV vaccines, with more countries expected to follow.
G-8 countries’ generous contributions to organizations like mine show that they understand the importance of childhood immunization.
But, while HPV infection rates may be falling in some of these countries, are they falling fast enough?
In the US, for example, the G-8 country for which we currently have the most data, infection rates have halved in the six years since the vaccine was first introduced.
Yet failure to reach the 80% coverage mark means that 50,000 American girls alive today will develop cervical cancer, as will another 4,400 girls with each year of delay.
So it is worth remembering that even in wealthy countries, there is an urgent need to overcome challenges in protecting the hardest-to-reach girls, who often are at high risk of HPV infection.
Overcoming these challenges is essential to reducing cervical cancer and pre-cancer rates in the coming years.
Rwanda’s success should be the norm, not the exception.
Set the IMF Free
BRUSSELS – The International Monetary Fund is back in business.
During the bubble years, neither its advice nor its money seemed to be needed.
But now more and more countries need balance-of-payments support, and there is general agreement that the global monetary system needs a body to oversee its overall stability.
The IMF is the only candidate for this task, but experience has shown that the Fund can fulfill this role only if its governance is reformed.
Granting balance-of-payments support has important fiscal implications, and it is natural that there should be continuing close oversight by those who ultimately provide the capital – the IMF’s member states.
But looking after the stability of the global financial system, including the assessment of exchange-rate policies and global payment imbalances, is a different responsibility.
For these analytical functions, there is no need for close oversight.
On the contrary, independence and professional expertise should be decisive.
Thus, a key change should be to distinguish between the IMF’s financial measures and its analytical functions, especially the surveillance of exchange rates and other sources of global financial risk.
The IMF’s Executive Board, which consists exclusively of representatives of member countries, currently runs the Fund’s daily business.
The Board thus does not perform oversight functions, but rather serves essentially as an extended Management Board, which delegates the execution of its decisions to the Managing Director and the staff.
This modus operandi needs to be changed in order to give the IMF the independence it needs if it is to become a credible, impartial judge of balance-of-payments disequilibria and sources of risk to global financial markets.
This requisite independence of the IMF staff can be achieved by stipulating that the Board oversees only the work of the Fund’s analytical functions and, more importantly, that its composition and decision-making mode are overhauled in the following ways:
- The Executive Board should be enlarged by the addition of several (possibly 3-5) independent members (as in the private sector), and the voting principle should be one person, one vote.
The independent board members would constitute only a small minority, but their presence and professional expertise would give them disproportionate weight.
- Management would be free to take a position on all issues not involving the use of Fund resources, unless it is was explicitly overruled by the expanded board.
This would give management considerable de facto independence, since under the one person, one vote principle the larger member countries could no longer block issues just because they are politically inconvenient.
For all decisions involving the resources of the Fund (lending to member countries or the size of quotas, etc.), the existing decision-making procedure could remain unchanged.
All financial decisions would thus continue to be taken by the existing Executive Board, with weighted voting reflecting the financial contributions of member countries.
The European Central Bank provides an interesting analogy, because it has two voting procedures, depending on the issue at hand.
The ECB’s Governing Council comprises the six members of the Executive Board plus the governors/presidents of the 16 national central banks of the eurozone countries.
The Governing Council takes the really important decisions on monetary policy based on one person, one vote.
When the Maastricht Treaty was negotiated, the Germans considered this provision an important concession.
But it is indispensable if the ECB is really to be independent, and if one expects that all members of the Governing Board, particularly the governors of national central banks, base their decisions only on the interests of the entire euro area (and not of their home country).
However, on financial matters (in particular the distribution of profits and losses), the voting rules are different: the Executive Board does not participate and the votes of the national central bank presidents in the Governing Council are weighted by their respective capital shares.
When the G-20 leaders meet in London, they should consider giving the IMF the independence it needs to become an effective guardian of global financial stability.  Putting independent experts on this Board will be a key step in that direction.
Our Debt to Stalingrad
BERKELEY – We are not newly created, innocent, rational, and reasonable beings.
We are not created fresh in an unmarked Eden under a new sun.
We are, instead, the products of hundreds of millions of years of myopic evolution, and thousands of years of unwritten and then recorded history.
Our past has built up layer upon layer of instincts, propensities, habits of thought, patterns of interaction, and material resources.
On top of this historical foundation, we build our civilization.
Were it not for our history, our labor would not just be in vain; it would be impossible.
And there are the crimes of human history.
The horrible crimes. The unbelievable crimes.
Our history grips us like a nightmare, for the crimes of the past scar the present and induce yet more crimes in the future.
And there are also the efforts to stop and undo the effects of past crimes.
So it is appropriate this month to write not about economics, but about something else.
Seventy-nine years ago, Germany went mad.
There was delinquency.
There was also history and bad luck.
The criminals are almost all dead now.
Their descendants and successors in Germany have done – and are doing – better than anyone could have expected at grappling with and mastering the nation’s unmasterable past.
Seventy years ago, 200,000 Soviet soldiers –&nbsp;overwhelmingly male and predominantly Russian – crossed the Volga River to the city of Stalingrad.
As members of Vasily Chuikov’s 62nd Army, they grabbed hold of the nose of the Nazi army and did not let go.
For five months, they fought.
And perhaps 80% of them died in the ruins of the city.
On October 15 –&nbsp;a typical day – Chuikov’s battle diary records that a radio message was received from the 416th Regiment at 12:20 PM: “Have been encircled, ammunition and water available, death before surrender!”
At 4:35 PM, Lieutenant Colonel Ustinov called down the artillery on his own encircled command post.
But they held on.
And so, 70 years ago this November –&nbsp;on November 19 to be precise –&nbsp;the million-soldier reserve of the Red Army was transferred to General Nikolai Vatutin’s Southwestern Front, Marshal Konstantin Rokossovsky’s Don Front, and Marshal Andrei Yeremenko’s Stalingrad Front.
They went on to spring the trap of Operation Uranus, the code name for the planned encirclement and annihilation of the German Sixth Army and Fourth Panzer Army.
They would fight, die, win, and thus destroy the Nazi hope of dominating Eurasia for even one more year – let alone of establishing Hitler’s 1,000-year Reich.
Together, these 1.2 million Red Army soldiers, the workers who armed them, and the peasants who fed them turned the Battle of Stalingrad into the fight that, of any battle in human history, has made the greatest positive difference for humanity.
The Allies probably would have eventually won World War II even had the Nazis conquered Stalingrad, redistributed their spearhead forces as mobile reserves, repelled the Red Army’s subsequent winter 1942 offensive, and seized the Caucasus oil fields, thus depriving the Red Army of 90% of its motor fuel.
But any Allied victory would have required the large-scale use of nuclear weapons, and a death toll in Europe that would most likely have been twice the actual World War II death toll of perhaps 40 million.
May there never be another such battle.
May we never need another one.
The soldiers of the Red Army, and the workers and peasants of the Soviet Union who armed and fed them, allowed their dictatorial masters to commit crimes –&nbsp;and committed crimes themselves.
But these crimes fall short by an order of magnitude of the great service to humanity –&nbsp;and especially to western European humanity –&nbsp;that they gave in the rubble along the Volga River 70 years ago this fall.
We are the heirs to their accomplishments.
We are their debtors.
And we cannot repay what we owe to them.
We can only remember it.
But how many NATO leaders or European Union presidents and prime ministers have ever taken the time to visit the battle site, and perhaps lay a wreath to those whose sacrifice saved their civilization?
Seven Years in Afghanistan
KABUL –We began a journey in Afghanistan seven years ago with the war that ousted the Taliban from power.
Much has been accomplished along the way, for Afghanistan and for the world.
In less than 45 days in 2001, we Afghans were freed from the menace of terrorism and the Taliban.
Back then, Afghanistan’s people held great hopes for an immediately wonderful future.
Some of those hopes were fulfilled. Our children are back in school.
Roughly 85% of Afghans now have access to some health care, up from 9% before 2001.
Child mortality – among the worst in the world in 2001 – has dropped by 25%.
Democracy, a free press, economic gains, and better livelihoods – all of that is there.
But, sadly, we are still fighting the Taliban and Al Qaeda.
What is it that we have not done right that makes us – and the rest of the world – less secure?
After the liberation in 2001, the international community concentrated on Afghanistan alone as the place to fight extremism and terrorism, while we Afghans argued that our country is not the right place to fight.
The war on terrorism cannot be fought in Afghan villages.
Instead, a regional approach was and is needed.
It must be concentrated on the sanctuaries of those who train, equip, and motivate the extremists and send them out to hurt us all.
But we were not heard.
Regardless of whether that was the result of a lack of knowledge or a lack of will, events have proven us right.
Unfortunately, for the past two years, Pakistan has been suffering as much or perhaps more than Afghanistan has suffered.
Almost the entire tribal belt along the Pakistan/Afghanistan border is suffering.
Just as schools were burned in Afghanistan from 2004 onwards, for the past year schools – especially for girls – have been burned there, leaving 80,000 children without facilities.
Bridges have been blown up, soldiers and police killed.
Bombs have exploded from Karachi to Lahore to Islamabad.
The violence has spread to India as well, with bombings in Gujarat, Bangalore, and Delhi.
So the problem is regional, and it concerns institutional support for extremism that incites terrorism.
Unless we collectively address the roots of the problem by ending that support, as well as financial support for radicalism in all forms, we will not defeat terrorism.
This has not been properly understood in the West, which has been fighting the symptoms of terrorism, but has failed to attack its underlying causes.
Fortunately, today I see signs of recognition of this malaise.
And democratic change in Pakistan is good news for Afghans, Pakistani people, and, by extension, many others around the world.
Pakistan’s new president, Asif Ali Zardari, has suffered from terrorism as we have suffered.
His wife, Benazir Bhutto, was killed by terrorists.
I visited Pakistan for President Zardari’s inauguration, and for the first time I saw a dim ray of hope.
If we can all work together – Afghanistan, Pakistan, India, the United States, and our allies – I see a possibility of moving beyond the days when a government thinks it needs extremism as an instrument of policy.
When all governments in the region reject extremism, there will be no place for extremists, and terrorism will wither away.
But this also requires helping those people who out of desperation have fallen prey to extremist forces.
Last year, I pardoned a 14-year-old boy from the Pakistan tribal area in Waziristan who had come to Afghanistan to blow himself up as a suicide bomber.
Only utter hopelessness can drive so young a man to such an act.
We must rescue these people by giving them a better future, which only more education and new opportunities can bring.
Desperation and poverty are the tools used by evil forces to raise their terrorist cadres.
But that environment will not change if political will is lacking, and if there is no action by the US and the governments of the region to get our economies to create jobs that offer hope.
Moreover, in order to deny terrorists institutional support, we must bring institutional strength to Afghanistan.
We must enable Afghans to look after themselves and defend their country, to have a future in Afghanistan, to have hope of raising their children in Afghanistan.
Recently, I spoke to an Afghan man very close to me.
He has a son who works in the Afghan Foreign Office.
That young man was born in the US but returned to Afghanistan four years ago.
The father asked, “Do you think I should take my son back to the US?”
I said, “Why?
Let him live here, let him work here, let him be an Afghan.”
He said, “Yes, but will he have a future?”
A viable future means security as well as bread.
We have started to bring hope by educating young Afghans, but we have not yet succeeded in bringing them a secure life, free from the danger of bombs and aerial bombardment.
Only when that happens will Afghanistan be secure.
And if the two other conditions are fulfilled – removal of political backing for radicalism and help for the desperate – we will have a safer life not only in Afghanistan, but in Pakistan, India, and the rest of the world.
Sex, Berlusconi, and Putin’s Bed
ROME – Italian Prime Minister Silvio Berlusconi’s political and sexual exploits make headlines around the world, and not just in the tabloid press.
These stories would be no more than funny – which they are certainly are – if they were not so damaging to Italy and revelatory of the country’s immobile politics.
For, despite the rampant scandals, “national Silvio” (“Il Silvio Nazionale”) remains by far Italy’s most popular and successful politician (though his approval ratings have now dipped below the 50% mark in opinion polls for the first time since his second return to the premiership in 2008).
Part of the reason for Berlusconi’s longevity despite his many stumbles is cultural.
As in other Latin or Mediterranean countries with a strong Catholic tradition, Italian society long ago learned to accept serenely a life of duplicity: on the one hand, a strong attachment to church and family values, and on the other a second life – often lived in plain sight – composed of mistresses and other “dubious” connections.
Today’s Italian Catholic political leaders often embrace such a lifestyle.
In recent years, aside from Berlusconi himself, other divorcés like Centrist Catholic Party Leader Pier Ferdinando Casini and Parliament Speaker Gianfranco Fini could easily deliver passionate speeches in the morning on the importance of the traditional family unit and the sacredness of marriage, attend a touching audience with the Pope in the afternoon, and then rush off in the evening to their unmarried partners and mothers of their latest offspring.
Italian society’s tacit acceptance of such behavior has become more openly acknowledged in recent years, thanks perhaps to Berlusconi and his vast media holdings.
In the 1970’s, the average Italian working-class family’s major ambition for its children was for them to study, go to university, and become a doctor or a lawyer.
Since the late 1970’s, and especially during the 1980’s and 1990’s, Berlusconi's three private TV channels have portrayed a false and illusory model of quick success, as seen in American soap operas such as “Dallas.”
Since the 1990’s, his channels broadcast “Big Brother” and Italian variety shows dominated by male comedians, musclemen, and scantily clad young girls, popularly known as “veline.”
In the space of just 30 years, Berlusconi's TV stations managed to impose this illusory portrait of success on Italian society.
And today, the ambition of many working-class Italian mothers is to see their daughters become a successful scantily clad “velina” who, in turn, manages to hit the gossip columns by flirting with the latest muscleman-turned-TV heartthrob or some budding young football player.
Graduating as a doctor or a lawyer is no longer a mark of success.
Despite his lack of muscles and hair, Berlusconi is the embodiment of this form of success.
The former cabaret singer who became one of the richest businessman in the world, has also managed to become Italy’s most powerful politician – and one of the world’s most colorful.
Until a few weeks ago, the average Italian viewed him as a role model, someone who had succeeded in many spheres of life.
That has now changed.
People have become less admiring of Berlusconi, because the hypocrisy has gone too far.
It may be trendy for an Italian politician to flaunt his Mediterranean macho image, but that image becomes hard to stomach when the prime minister launches a campaign to eradicate street prostitution, with possible jail sentences for clients, while sleeping with paid escorts.
Nor are Italians reassured to learn that Berlusconi fielded a number of candidates during the recent European Parliament elections whose only discernible qualification was that they were pretty young girls who had possibly spent some time in the prime minister’s company at his Sardinian Villa or Roman Palazzo.
Today, it seems all but certain that Berlusconi will never be elected President of Italy, the post to which he has always aspired.
Moreover, rumors are rife that he is now being attacked for his behavior by members of his own party.
Indeed, some maintain that Berlusconi will be forced to resign as prime minister by the end of the year.
Such rumors may well turn out to be true, for the heart of the scandal now concerns the taped conversations between a paid escort and Berlusconi during their romps in his Sardinian villa on the big bed given to him by his Russian counterpart, Vladimir Putin.
A downfall plotted to happen on a Kremlin-supplied bed would be a denouement that not even one of Berlusconi’s TV channels could dream up.
Sexy Culture
Why is there culture?
What motivates people to write poems, paint, or sing?
Most people engaged in these activities would respond with answers like, “because I like to” or, “because it fulfills me,” if not, “I feel obliged not to waste my talent.”
They tend to believe that culture reflects the existence of a soul type, or that it's an expression of humans’ intelligence and creativity.
Natural science – as so often – has a more mundane answer, one that has to do with natural selection.
In his seminal work on evolution, On the Origin of Species by Means of Natural Selection, or the Preservation of Favored Races in the Struggle for Life, Charles Darwin used the much-cited expression “survival of the fittest.”
Most people find it easy to understand that being especially strong or fast, or able to withstand hunger, heat, or cold, can increase the chances of survival.
Intelligence also falls into that category.
But to squeeze cultural excellence into the group of characteristics defining “the fittest” is not so easy and requires some leap of faith.
In his later work, Darwin introduced another selection criteria that may be just as important, but which has received much less attention: mating preference, or sexual selection.
His reason for doing so was to explain male peacocks’ obviously hindering tail feathers and male lions’ apparently useless manes.
These characteristics would reduce rather than enhance the bearer’s chances of survival, but obviously they prevailed in generation after generation.
Thus, Darwin argued, they must increase the probability of more offspring by making the males more attractive to female mates.
As Darwin did not believe that pure aesthetics would guide female peacocks and lions in their choice of mating partners, he had to find a rational reason for females’ preference for males with hindering characteristics.
The very fact, he reasoned, that these features make life harder signals to prospective partners that individuals who can do reasonably well with them have an especially good genetic set-up and are thus likely to produce strong offspring.
They should therefore be preferred mates.
Evolutionary biologists have since taken the concept further.
If someone can do difficult things, not only carrying peacock tail feathers or a long dark lion mane, but also things that require much practice without contributing to physical fitness and survival, and yet stay alive, that individual must have especially good genes.
They are therefore sexually attractive.
Culture – at least the culture we are proud of and don’t sneer at – is highly elitist.
We admire the best, and only the best, according to some cultural and time-dependent standard.
It does not help much to sing pop songs or opera arias in the bathroom.
You must be able to draw a listening and cheering crowd to qualify for the elite.
Likewise, the amateur painter does not increase her or his attractiveness much compared to a van Gogh or a Picasso.
The same goes for writers.
A vanity press autobiography does not bring you to the top.
For that, you have to be a Nobel laureate or at least the author of a couple of well regarded books.
The bottom line is that while many are called, few are chosen.
Reaching the top requires not only talent and luck, but also a lot of practices – that is, time wasted from the point of view of survival.
Sport is in this respect also a form of culture, albeit one that is often sneered at by those engaged in pursuits like classical music or serious literature.
Most sports certainly contribute to physical fitness – as do some other expressions of culture, such as ballet – but what we admire in a player who can do extraordinary things with a ball is a technique that is utterly useless outside the playing field and has taken thousands of hours of practice to bring to perfection.
Here, of course, it’s only the best that become local or national heroes.
To be a devoted football or basketball player in the lowest series brings ridicule rather than fame.
It must be hard and require enormous effort to acquire the unique skills that mark the superstar and earn societies’ respect and admiration.
Following this reasoning, what makes the poet, the painter, and the singer attractive is uselessness combined with the difficulty of their activity.
The harder and more futile the activity, the better and more sexually attractive is the performer.
Naturally, self-awareness of this underlying wish to be sexually attractive is not required.
The mechanism works all the same.
The poet, painter, and singer may think they do what they do for more high-minded reasons, but scientists know otherwise.
Hey, science is hard, too!
But, in line with the logic of natural selection, to become sexually attractive, the scientist only has make sure that the results of his or her work are useless.
Shades of Gray
It takes a very close look at the results of the recent elections in the German state of North-Rhine-Westphalia to find among the list of “Others” the tally for “The Greys”: they got 0.1% of the vote.
In other words, one in a thousand voted for them, although they claimed to speak for that state’s retired and elderly people – over 30% of the population.
“Generation consciousness,” unlike the “class consciousness” of old, is obviously not a defining factor in people’s political preferences.
Many more “Greys” voted for the Greens than for their “own” party.
This is an important fact.
Most Europeans – and many in other parts of the world – live in rapidly aging societies.
Nurseries and schools are closed while retirement homes and hospices spring up everywhere.
Rising life expectancy coupled with low birth rates shape the demography of almost all prosperous countries.
By the middle of the century – unless there is a dramatic turnaround – about half the population will be economically inactive for reasons of age.
This trend will have many consequences, most obviously for the welfare state, notably pensions and healthcare.
While expenditure for both is rising rapidly, the offsetting revenues are coming from ever fewer people in employment.
As a result, the “generational contract” looks less and less viable, because we can no longer rely on today’s workers to pay for today’s pensioners.
Insurance-based systems of entitlements created by personal contributions are increasingly taking the place of national health and pension services. This is a profound change that creates much friction in the transitional phase.
For example, the transitional phase is a period in which public debt inevitably increases.
Governments must continue to pay benefits to today’s pensioners, and they can do so only by borrowing money to replace the revenues that tomorrow’s pensioners are now diverting to personal insurance schemes.
Debt, however, is a burden imposed by current citizens on future generations.
Understandably, there are signs that younger politicians of all parties are resisting this development.
Indeed, a party of the young might well have better prospects than the “Greys.”
Shifting political interests are, however, but one part of the changes in an aging society.
More visible are changes in lifestyle.
People who live longer while not being in employment want to enjoy their lives. A plethora of magazines tell pensioners what they can do.
Advice ranges from sex in old age to tourism.
Tourism, in particular – cruises as well as more adventurous trips – has become a favorite pastime for the elderly.
At the same time, images of life in our own societies are shifting.
No one is surprised to see grey-haired people dancing and singing and petting.
For many of them, life seems to become an almost permanent holiday.
To be sure, the impression is deceptive.
Old people who have families are often found in a new role as educators of the young.
While their sons and daughters go to work, they become the real parents of the next generation.
Today’s young children often see more of their grandmother than of their mother.
One may well wonder what this means in terms of social values.
The impression of a generation on permanent holiday is deceptive in another respect. Much public debate is devoted to child poverty and how to relieve it.
However, there is at least as much old-age poverty. The point is that it is less visible.
Occasionally, newspapers report that an old person has been found dead after days, having suffered a lonely demise from illness or even starvation.
For the most part, old-age poverty is hidden, often by the victims themselves, who are too proud to talk about their condition.
This is in part the explanation for the failure of the “Greys,” that is, of political groups catering to the interests of the elderly.
While such interests do exist, they do not lead older people to close ranks and organize themselves.
If they are poor or otherwise disadvantaged, they are most likely to regard this as their personal, almost private destiny.
Like the unemployed, they do not want a political party devoted to their plight.
Beyond that, the key feature of an aging society is the independence of the elderly.
Their vote is precisely not an expression of group interest.
They make up their own minds and contribute to the unpredictability of modern elections.
In that sense, too, an aging society adds to the responsibilities of the young.
Pakistan’s War at Home
SINGAPORE – Last month, after years of indecision, Pakistan’s military launched a full-scale military operation in the North Waziristan Tribal Agency aimed at eliminating terrorist bases and ending the region’s lawlessness.
In particular, the army wants to clear out foreign fighters who are using the territory as a base for various jihads around the Muslim world.
But, by triggering yet another refugee crisis, the operation risks spreading the terrorist threat to other parts of Pakistan, including its largest city and commercial center, Karachi.
Operating from sanctuaries established in the tribal agency, various terrorist groups, in association with organizations elsewhere in the country, have already attacked Pakistan’s four neighbors – Afghanistan, China, India, and Iran.
Of the region’s foreign fighters, Uzbeks belonging to the Islamic Movement of Uzbekistan have recently become the most visible threat, taking responsibility for the June 8-9 attack on Karachi’s Jinnah International Airport, in which 30 people, including all ten of the militants, were killed.
In launching the North Waziristan operation, General Raheel Sharif, Pakistan’s new Chief of Army Staff, stated that his forces would draw no distinction between supposedly “good” and “bad” Taliban.
The former, including the Haqqanis – named after Jalaluddin Haqqani, who led the Islamic resistance against Soviet forces in Afghanistan – had been trained and equipped by the Inter-Services Intelligence (ISI), Pakistan’s main security agency.
Following the United States’ invasion of Afghanistan in 2001, the Haqqanis created a sanctuary in the North Waziristan Tribal Agency.
The ISI countenanced this in the hope that the Pashtun group would later act as Pakistan’s proxies in Afghanistan after US combat troops depart at the end of 2014.
But the Haqqanis, it appears, did not keep to any such bargain, and allowed their Uzbek guests in North Waziristan to launch the Karachi airport attack.
This conflict, however, will not be easy to contain or manage.
Pashtuns, the main ethnic group on both sides of the Afghanistan-Pakistan border, are engaged in a bitter struggle in both countries to assert what they consider to be their legitimate political and economic rights.
Karachi, hundreds of miles to the south, will not escape the fallout from the North Waziristan operation.
The military, which planned to flush out the main militant hideouts with air strikes, and then send in ground troops, instructed residents to leave the area beforehand.
Some 350,000 people have already fled, creating a humanitarian crisis on a scale similar to that in 2009 when the military broke the Taliban’s grip on the Swat Valley.
The movement of so many people is likely to have a profound effect on Pakistan.
According to a report by the United Nations High Commissioner for Refugees, released just five days after the assault, at the end of 2013 there were 51.2 million forcibly displaced people in the world, six million more than the year before, and the largest number since World War II.
Pakistan plays host to more refugees than any other country, with 1.5 million registered in the country, in addition to an estimated 3.5 million internally displaced people.
As has been the case on previous occasions, internally displaced people from North Waziristan are unlikely to remain in the camps set up for them in adjoining districts.
Many will head for Pakistan’s large cities, particularly Karachi.
The city’s population of 20 million already includes around six million Pashtuns, more than the number in Kabul and Peshawar combined.
Indeed, Karachi is sometimes called an “instant city,” having grown 50-fold as a result of several waves of migration since Pakistan gained independence in 1947.
The first wave, of around two million people, arrived in Karachi when eight million Muslims fled India for Pakistan.
The second wave included Pashtun construction workers who helped build the new commercial capital.
The third wave comprised refugees displaced during Afghanistan’s war against Soviet occupation.
And the fourth began in the early 2000s, following the US invasion of Afghanistan, which also gave rise to Pashtun resistance on both sides of the border, and contributed in turn to Islamic extremism in the tribal areas.
The current displacement from North Waziristan, therefore, can be seen as part of this fourth wave.
Even if the army succeeds in clearing out the militants, some of the internally displaced people, bearing battle scars, will end up in Karachi.
They will be in no mood to lay down their arms if the municipal authorities fail to develop inclusive political institutions that give minority ethnic groups a fair political voice.
In that case, the long-term consequence of the military’s North Waziristan campaign may well be more violence where it can cause the most damage.
Pakistan’s Political Renaissance
LAHORE – Pakistani institutions are evolving rapidly.
With executive authority increasingly in the hands of elected representatives, rather than dispersed among various competing institutions, the political establishment has been revitalized – and it has taken three important steps toward strengthening democracy and the rule of law.
Is Pakistan, a country long prone to military coups, finally developing a well-functioning political system?
On November 27, Pakistani President Mamnoon Hussain – acting on the prime minister’s advice, as the constitution dictates – announced that General Raheel Sharif would succeed General Ashfaq Parvez Kayani as Chief of Army Staff, even though Sharif was not among the military establishment’s favored candidates.
Unlike Kayani – who has directed the Directorate-General of Military Operations and the Inter-Services Intelligence (Pakistan’s spy agency) – Sharif has not served in any of the positions that typically prepare someone to lead Pakistan’s best-funded and most influential institution.
This was not Prime Minister Nawaz Sharif’s first act of defiance against the military.
Just days earlier, he asked the Supreme Court to appoint a three-judge special tribunal to investigate charges of treason against Pakistan’s former president, General Pervez Musharraf, for imposing emergency military rule and suspending the constitution in November 2007.
The decision, which Musharraf claimed was intended to stabilize the country and stem the tide of Islamist extremism, facilitated the removal of dozens of senior judges from the Supreme Court and the provincial high courts – including Chief Justice Iftikhar Muhammad Chaudhry, Pakistan’s highest-ranking judge.
Chaudhry’s suspension the previous March, following his refusal to bow to government pressure to resign, had incited relentless protests by Pakistan’s legal community and made him a symbol of the people’s desire for a fairer, more independent judicial system.
In a sense, this movement, which contributed to Musharraf’s electoral defeat the following February and the return of democracy to Pakistan, prefigured the 2010-2011 revolutions in Tunisia and Egypt that sparked the Arab Spring.
Musharraf will be tried under Article Six of Pakistan’s constitution, according to which “any person who abrogates or subverts or suspends or holds in abeyance…the Constitution by use of force or show of force or by any other unconstitutional means shall be guilty of high treason.”
Parliament has defined high treason as a capital offense.
By appointing a special tribunal to try Musharraf, the Sharif government is sending a strong signal to the military – particularly its senior commanders – that they are not above the law.
This message is especially important now, given doubts about the government’s resolve stemming from its decision last June to drop high-treason charges against Musharraf for leading, while serving as Chief of Army Staff, the 1999 coup against the elected government, headed by Sharif himself.
The Pakistani government’s third move to tame the military was the announcement that Justice Tassaduq Hussain Jillani, the Supreme Court’s second most senior judge, would succeed Chaudhry, who was reinstated in 2009, after his mandatory retirement this year.
(Jillani will serve for only seven months before he, too, retires.)
By establishing the seniority rule for the most important Supreme Court appointment, Sharif has depoliticized the process.
These three moves promise, at long last, to establish civilian control over the military and ensure judicial independence.
This would put Pakistan on sounder political footing than several other large Muslim countries, which are currently engaged in similar – but far less successful – efforts to institute more accountable governance.
For example, in Bangladesh, the executive – namely, Prime Minister Sheikh Hasina and her Awami League government – is attempting to monopolize political power.
The judiciary already does the executive’s bidding; if current plans succeed, the ruling party will soon dominate the legislature as well.
Although the Bangladeshi military is watching events unfold with some apprehension, it lacks the will to install a caretaker government, as it did in 2007, when it brought peace and stability to the country by establishing a nonpartisan technocratic administration.
Unfortunately, unlike Sharif’s actions in Pakistan, this effort did not spur the development of a more stable political order.
In Egypt, supporters of the democratically elected President Mohamed Morsi of the Muslim Brotherhood are still battling the army and security forces, which deposed him in July.
With the liberal opposition apparently unable to create a political organization capable of challenging the Brotherhood within a recognized framework – and, indeed, largely supporting the military coup – Egypt’s fate is increasingly being decided on the street.
Even in Turkey, where the rule of law is relatively strong and the constitution well established, the opposition has failed to organize a credible political party with broad public support.
Thanks to the efforts of Sharif’s government, Pakistan now has reasonably well-developed political parties, which compete in regularly scheduled elections; an autonomous judiciary capable of defending the constitution; and a military that appears to have accepted civilian control.
After nearly seven decades of tumult, Pakistan may soon serve as a model for other large Muslim countries.
Shaky Social Contracts
LONDON – “Enrich yourselves,” China’s Deng Xiaoping told his fellow countrymen when he started dismantling Mao Zedong’s failed socialist model.
In fact, elites everywhere have always lived by this injunction, and ordinary people have not minded very much, provided that the elites fulfill their part of the bargain: protect the country against its enemies and improve living conditions.
It is this implied social contract that is now endangered by economic collapse.
Of course, the terms of the contract vary with place and time.
In nineteenth-century Europe, the rich were expected to be frugal.
Conspicuous consumption was eschewed. The rich were supposed to save much of their income, as saving was both a fund for investment and a moral virtue.
And, in the days before the welfare state, the rich were also expected to be philanthropists.
In the opportunity culture of the United States, by contrast, conspicuous consumption was more tolerated.
High spending was a mark of success: what Americans demanded of their rich was conspicuous enterprise.
Societies have also differed in how wealthy they allow their elites to become, and in their tolerance of the means by which wealth is acquired and used.
One dividing line is between societies that tolerate self-enrichment through politics, and those that demand that the two spheres be kept separate.
In Western countries, politicians and civil servants are expected to be relatively poor.
In most of the rest of the world, a political career is regarded as a quasi-legitimate road to wealth.
But the broad conclusion remains: wealth is conditional on services.
When the services fail, the position of the wealthy is threatened.
In the current crisis, popular anger is – no surprise – directed against bankers.
Their speculative frenzies ruined shareholders, customers, and the economy.
Anger has come to focus on banking executives’ huge compensation packages, composed largely of bonuses.
Rewarding success is acceptable; rewarding failure is not.
Governments face a dilemma. Big banks cannot be allowed to fail; but the public expects bankers to be punished.
Few will be ruined or imprisoned.
But the banking system is sure to be re-regulated, as it was after the Great Crash of 1929-1932, when President Franklin Roosevelt promised to drive the money changers from the temple.   
The global economy’s downturn increases countries’ political risk to varying degrees, depending on the severity of the shock and the nature of the implied social contract.
Political systems in which power is least controlled, and the abuse of wealth greatest, are most at risk.
The more corrupt the system of capitalism, the more vulnerable it is to attack.
The general problem is that all of today’s versions of capitalism are more or less corrupt.
“Enrich yourselves” is the clarion call of our era; and in that moral blind spot lies a great danger.
Despite efforts to give it precision, estimating political risk is not an exact science.
It requires political theory, not econometrics.
Forecasting models, based on “normal distributions” of risk over short slices of recent time, are notoriously incapable of capturing the real amount of risk in a political system.
One of the “safest” political systems of recent times was President Suharto’s regime in Indonesia.
Suharto came to power in 1966, establishing a quasi-military dictatorship and encouraging Indonesians to “enrich themselves.”
Despite the depredations of his family, enough Indonesians did so over the next 30 years to make his rule seem exceptionally stable – until the East Asian financial crisis of 1997-1998 sent the Indonesian economy into a tailspin, triggering violent riots that forced Suharto out.
Similarly, few regimes seemed more stable than that of the Shah of Iran, another long-term ruler, who, having bankrupted his country, was forced to flee the fury of a mob in 1979.
The lesson is clear.
Autocracies, which are much praised for their decisiveness, and for guaranteeing “law and order,” are paper tigers.
They appear immovable until the moment they are evicted by popular anger.
In face of economic failure or military defeat, their emperors turn out to be naked.
In such situations, the great advantage of democracies is that they allow a change of rulers without a change of regime.
Failure discredits only the party or coalition in power, not the entire political system.
Popular anger is channeled to the ballot box.
In such countries, there may be “New Deals,” but no revolutions.
In estimating political risk today, analysts must pay particular attention to the character of the political system.
Does it allow for an orderly transition?
Is it competitive enough to prevent discredited leaders from clinging to power?
Analysts also must pay attention to the nature of the implied social contract.
Broadly speaking, the weakest contracts are those that allow wealth and power to be concentrated in the same few hands, while the strongest are built on significant dispersal of both.
Deepening economic recession is bound to catalyze political change.
The Western democracies will survive with only modest changes. But strongmen who rely on the secret police and a controlled media to maintain their rule will be quaking in their shoes.
Even Venezuela’s Hugo Chávez, who built his power on populist anti-Americanism, must be praying for the success of US President Barack Obama’s stimulus package to lift his falling oil revenues.
The big countries with the highest political risk are Russia and China.
The legitimacy of their autocratic systems is almost entirely dependent on their success in delivering rapid economic growth.
When growth falters, or goes into reverse, there is no one to blame but “the system.”
Igor Yurgens, one of Russia’s most creative political analysts, has been quick to draw the moral: “the social contract consisted of limiting civil rights in exchange for economic well-being.
At the current moment, economic well-being is shrinking. Correspondingly, civil rights should expand.
It’s just simple logic.”
The rulers in Moscow and Beijing would do well to heed this warning.      
Shards of Europe
PRINCETON – As European leaders struggle after another failed summit, they should think hard about what their continent – and the world – might look like if they continue to produce unsatisfactory solutions to Europe’s financial and economic problems.
What would follow the disintegration of the eurozone and – almost certainly with it – that of the European Union?
The best place to consider that question would not be Brussels, but Tiraspol, the capital of the entity that calls itself the Pridnestrovian Moldavian Republic, or Trans-Dniestr.
This territorial sliver with a population of a half-million emerged in the early 1990’s, after the dissolution of the Soviet Union (population almost 300 million), when it broke away from the Republic of Moldova (population four million), which had separated in the 1940’s from Ukraine (population 50 million).
Trans-Dniestr has its own government and parliament, army, constitution, flag, and a rousing Soviet-style national anthem; of course, its nationhood would be incomplete without its own currency.
This political entity is a precise counterpart in the political world to a well-known physical phenomenon of splintering or fissuring.
When stressed, a big surface bifurcates in big chunks, but then the disintegration continues into smaller and smaller fragments.
Of the six larger EU states, only France has a really well-defined centralized political system.&nbsp; Poland’s centralism comes close, but strong regional differences persist – a legacy of the three large and quite different imperial systems that encompassed today’s Poland in the nineteenth century.
Italy and Germany were nineteenth-century amalgamations of a colorful variety of small and medium-size political units.
The United Kingdom looks older and more stable, but Scotland today is controlled by a political party that wants to repeal the 1707 Act of Union, with the future to be determined by a Scottish referendum in 2014.
Spain after the Franco dictatorship stabilized itself by granting autonomy to its regions, which in many ways now behave like independent units.
In these fragmented political areas, the logic of integration in the past depended on areas that were dissatisfied with political outcomes appealing to new allies in larger units.
Franconians in southern Germany disliked the fact that the Napoleonic Wars subjected them to Bavarian rule; they saw German nationalism as a way to use Prussia and Berlin as a counterweight to Munich’s hegemony.
But, once Germany was united, Bavarians did not like the outcome, and then thought of a united Europe as a counterweight to the German state.
Indeed, Bavaria became adept at using European Community resources to bolster its own political system.
Integration had its own historical momentum; if and when it goes into reverse, that process will have a counter-momentum.
The argument against European structures depends on hostility to a transfer union that might lead to some redistribution of resources.
Why should our money be taken away and given to people in a very different area?
What sort of claim do those people have?
Germans thinking about the likelihood of transfers to southern Europe doubtless recall their country’s reunification after the collapse of communist East Germany in 1989-1990.
There were massive transfers, and national resources were devoted to gigantic infrastructure projects.
That was not enough to halt the hollowing out of the eastern Länder, as many of the ablest and most entrepreneurial people left – an experience that put enormous strain on national solidarity.
Problems of transfers in a large political unit are at the heart of federalism.
The United States’ early history was dominated by a passionate debate about the issue of solidarity.
In 1790, when Alexander Hamilton argued that the new federal government should assume the states’ debts from the War of Independence, he encountered fierce hostility.
The only way to sustain such a new political order, James Madison argued in The Federalist Papers, was to ensure that federal powers were few and limited.
Europe is confronting a similar moment of destiny.
It is now mired in an existential crisis more profound than at any point since 1945.
And, while muddling through is a characteristic response of complex political systems, it is deeply destructive.
If Europe’s political center is widely perceived to be arbitrary and overweening, its authority will be rejected and resisted.
While adopting a new treaty may look like an unwieldy process, ill-suited to managing a fast-moving modern financial crisis, it is the only way to generate legitimacy for the institutions that are needed to address that crisis – in particular to provide reassurance that transfers will not be indefinite and unlimited.
If European integration shifts into reverse, the outcome will not be a series of happy and prosperous nation-states, living in a sort of replica of the 1950’s or 1960’s.
Southern Germans would wonder whether they were not transferring too much to the north’s old industrial rustbelt; northern Italians who support the anti-EU Lega Nord in the self-styled unit of “Padania” would want to escape from the rule of Rome and the south.
Setting the clock back would thus not simply return Europe to the mid-twentieth century.
The small states of the mid-nineteenth century, with no fiscal transfers out of a relatively limited area, might be recreated.
But the dynamic might go further: the German territories had around 350 independent political entities in the mid-eighteenth century, and more than 3,000 before the middle of the seventeenth century.
Watch out, Trans-Dniestr.
Share the Work
BERKELEY – The United States today is facing a crisis of long-term unemployment unlike anything it has seen since the 1930’s.
Some 40% of the unemployed have been out of work for six months or more, which, as US Federal Reserve Board Chairman Ben Bernanke noted in a recent speech, is far higher than in any other post-World War II recession.
This crisis of long-term unemployment is having a profoundly damaging impact on the lives of those bearing the brunt of it.
We know this thanks to a series of careful studies of the problem conducted in the depths of the 1930’s Great Depression.
The most famous such study, of the long-term unemployed in New Haven, Connecticut, was conducted by E. Wight Bakke, a graduate student and subsequently a professor of economics at Yale University.
Through participant interviews, personal observation, time diaries, and longitudinal studies, Bakke showed how extended spells of unemployment caused workers’ skills to deteriorate and made it difficult for them to acquire new ones.
The long-term unemployed also experienced a variety of physical and psychological problems, among them demoralization, apathy, and a sense of social isolation.
For those unfortunate enough to experience it, long-term unemployment – now, as in the 1930’s – is a tragedy.
And, for society as a whole, there is the danger that the productive capacity of a significant portion of the labor force will be impaired.
What is not well known, however, is that in the 1930’s, the United States, to a much greater extent than today, succeeded in mitigating these problems.
Rather than resorting to extensive layoffs, firms had their employees work a partial week.
The average workweek in manufacturing and mining fell from 45 hours in 1929 to 35 hours in 1932.
We know this from a 1986 article by my Berkeley colleague James Powell and his co-author, none other than – wait for it – Ben Bernanke.
The 24% unemployment reached at the depths of the Great Depression was no picnic.
But that rate would have been even higher had average weekly hours for workers in manufacturing remained at 45.
Cutting hours by 20% allowed millions of additional workers to stay on the job. They continued to earn an income.
They continued to acquire skills. They had hope and the possibility of advancement.
Why was there so much work-sharing in the 1930’s?
One reason is that government pushed for it.
In his memoirs, President Herbert Hoover estimated that as many as two million workers avoided unemployment as a result of his efforts to promote work-sharing.
Second, legislation encouraged it.
The industrial codes of the New Deal set ceilings on the workweek for specified industries and workers.
The Fair Labor Standards Act provided financial incentives by requiring overtime pay for employees working long hours.
Third, there was no unemployment insurance to discourage it.
An individual today, faced with the option of working 20 hours a week or drawing unemployment benefits, might be tempted by the latter.
But, back in the 1930’s, before unemployment insurance, 20 hours was better than nothing.
Of course, unemployment insurance replaces only a fraction of most workers’ previous wages, which suggests that its effect in this regard is not very strong.
But, even if unemployment insurance does not discourage work-sharing, it could be restructured to encourage it.
Partial benefits could be paid to workers on short hours, rather than limiting payments to those who are fully unemployed.
The program would at least partly pay for itself, with additional payments to workers on short hours offset by lower unemployment (and thus lower payments to those who are completely without work).
In fact, the US already has something along these lines: a program known as Short-Time Compensation.
Workers can collect unemployment benefits pro-rated according to their hours when their employer submits an approved work-sharing plan, while the federal government compensates the states for a portion of the set-up costs.
At last count, 24 states have begun adapting their unemployment-insurance systems to take advantage of the measure.
Unfortunately, the financial incentives that the federal government provides are mainly limited to helping the states to advertise and automate their programs.
And those programs, in turn, are too modest, especially for senior workers with a reasonable expectation of remaining in a full-time job, to make work-sharing an attractive option.
Other countries have gone further.
In Germany, for example, the federal government’s Kurzarbeit program makes up a significant fraction of the difference when, owing to short hours, a worker’s earnings fall by more than 10%.
The US federal government could emulate this example by compensating the states more generously for their Short-Term Compensation programs.
Its failure to do so not only inflicts avoidable pain and suffering on the unemployed, but also threatens to inflict long-term costs on American society.
Man Bites Shark
BONN – Sharks have long been portrayed as man-eaters, a menace to any swimmer brave (or foolish) enough to share the water with them.
But this depiction could not be further from reality.
In fact, sharks are extremely vulnerable, and their dwindling populations – largely owing to human behavior – urgently need global protection.
To be sure, there have been many laudable shark-protection efforts in the last few years. A campaign spearheaded by the United States-based organization Wild Aid has contributed to a major decline in demand for shark fin across Asia.
China, for example, has banned shark-fin soup, a traditional delicacy, at official government dinners and functions – a move that contributed to a 30% drop in shark-fin sales from last December to April.
In the southern city of Guangzhou, the center of China’s shark-fin trade, vendors have reported an 82% decline in sales over the last two years.
It should not be difficult to spur countries to take action to protect their shark populations, given that a shark’s economic value plummets when it is killed.
A study by the Australian Marine Institute found that Palau’s shark-diving industry is worth far more than its shark-fishing industry.
A single reef shark that frequents major dive sites in Palau is worth roughly $179,000 annually, or $1.9 million over its lifetime; the same shark would be worth about $108 dead.
Similarly, the shark-diving industry has brought in an estimated $110 million annually to Thailand, $22 million to the Canary Islands, and a massive $800 million to the Bahamas over the last 20 years.
It is not difficult to see why allowing fisherman to decimate these countries’ shark populations would be counter-productive.
Developing protected areas and corridors for shark migration is also important.
Last month, US President Barack Obama extended the maritime boundary of the Pacific Remote Islands Marine National Monument from 50 miles to the 200-mile limit, effectively merging the region’s national parks into one massive protected area, in which commercial fishing is prohibited.
Similar protected areas have been created over the last couple of years in Palau, Micronesia, Indonesia, and the Maldives.
But the new sanctuary in the Pacific – with an area equivalent in size to Texas, California, Montana, and Arizona combined – could be the largest such zone in the world.
Though these measures are certainly positive steps, no country acting alone can ensure adequate protection of shark populations.
After all, sharks are global wanderers; they have no regard for the boundaries of economic exclusion zones, protected areas, or countries’ territorial waters.
Shark-protection efforts cannot truly be effective without coordinated international action.
Such action must not only aim to protect shark populations directly; it must also address major threats to sharks, such as illegal trade, bycatch, and overfishing.
Consider climate change.
A recent study by the University of Lisbon’s Center for Oceanography reports that acidification from CO2 emissions could cause the global shark population to decline by as much as 40% by 2100.
Add to that the more than 73 million individual sharks that are already lost annually, and the situation is clearly unsustainable.
As it stands, there is no global treaty to protect sharks.
But frameworks are in place for the development of international conservation measures.
The Convention on Migratory Species, for example, has a track record of working with countries – as well as international organizations, NGOs, media, and the private sector – to spur coordinated action that meets international standards.
The Convention’s memorandum of understanding on shark conservation, which lists seven endangered species, has attracted 35 signatories so far.
Next month, representatives from these countries will meet in Quito, Ecuador, to discuss extending the list to include the great and scalloped hammerheads, the silky shark, thresher sharks, sawfish, and mobula rays.
Given how many shark species have been proposed, some have begun referring to the Convention’s Conference of the Parties as the “shark COP.”
But proposals must give way to action.
The threats facing sharks are well documented.
It is time to launch concerted global efforts to protect some of the oldest, most vulnerable – and most valuable – species in our seas.
Sharm el-Sheikh Redux
Driven by a common fear of Islamic fundamentalism, and by a false assumption that it is an illegitimate political force, the Middle East’s so-called “moderates” have once again gathered at the Egyptian seaside resort of Sharm el-Sheikh, the traditional venue for emergency regional summits, to rally “moderates” against “extremists.”
In the spring of 1996, the so-called “moderates” – Egyptian President Hosni Mubarak, Jordan’s King Hussein, Yasser Arafat, and even some representatives of the Gulf dynasties – convened in Sharm el-Shekh with President Bill Clinton and United Nations Secretary-General Kofi Annan in a desperate bid to block radical Islam’s emergence.
They were also expected to give an electoral boost to Prime Minister Shimon Peres of Israel who, severely undermined by Hamas’s devastating campaign of suicide terrorism, was on the verge of electoral defeat at the hands of Benjamin Netanyahu.
But Islamic fundamentalism remained unimpressed.
Both its jihadist and political identity have, indeed, only gained momentum ever since.
In October 2000, Sharm el-Sheikh was the scene of another summit, with most of the same actors.
This time, the call went out for an end to the Palestinian Intifada and for Israelis and Palestinians to reach a final peace agreement.
Both objectives were endorsed by all the participants; neither was implemented.
As one of the players at that summit, I understand the reason for the gap between what was agreed by the “moderates” and the harsh realities that have driven the “extremists” forward.
The only way Arafat could stop the Intifada and stem the advance of Hamas was through an especially generous peace deal from Israel.
But such a deal became impossible precisely because the Intifada had raised Palestinian expectations to such heights that it became impossible for Israel to meet them.