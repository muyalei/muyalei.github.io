News headlines screamed about online dangers that we could barely understand, alerting swarms of digital pirates to a bounty of new criminal opportunities.
Companies worldwide scrambled to secure their online safety.
The story of the so-called “Heartbleed” bug, however, is all too real.
It points to a stark truth: In a shockingly short time, we have become completely dependent on the Internet, a frontier we are only beginning to comprehend, let alone map and regulate.
Important debates – such as freedom versus security, privacy versus piracy, and cyberspace’s impact on democracy – are far from being resolved.
Yet fears over Heartbleed and similar such threats, and the furor surrounding the aggressive American surveillance tactics revealed by former intelligence contractor Edward J. Snowden, have already put many countries in a defensive posture.
In many places, efforts to protect Internet freedom have been stymied.
But, as my home country, Brazil, has shown, a different path is possible.
Brazil’s House of Representatives has passed a genuine Internet “Bill of Rights,” which was unanimously approved by the senate and signed into law by President Dilma Rousseff last week – much to the delight of civil-society advocates.
The legislation, widely described as an Internet constitution, seeks to safeguard online freedom of expression and limit government collection and usage of Internet users’ metadata.
The bill ensures “net neutrality” (meaning that Internet service providers must treat all information and users equally), and subjects global companies, such as Google and Facebook, to Brazilian law and precedent in cases involving its own citizens.
Inspiration for Brazil’s embrace of Internet freedom stemmed in part from US President Barack Obama’s historic election campaign in 2008.
By tapping millions of small donations through social networking sites, he revolutionized election fundraising and seemed to open up new horizons for democracy and civic engagement.
But not everyone was convinced.
A wave of legislation – criminalizing various types of Internet use, such as music sharing, and allowing greater government surveillance – appeared to be turning back the liberal tide.
In Brazil, too, the authorities acquired greater control over the Internet, as the country moved to the forefront of the international fight against cybercrime.
The situation started to change in 2009, after Brazil’s then-president, Luiz Inácio Lula da Silva, participated in an Internet freedom event, and vowed afterwards to block any legislation restricting online freedoms.
He also pledged to submit the bill that resulted in the newly adopted law.
Interestingly, the bill was not drafted by the usual cast of bureaucrats.
Instead, the process was opened up to the public, with ordinary citizens allowed to contribute via a government blog.
Advocates of free software mixed it up with law enforcers and lobbyists from the likes of Google and Yahoo, in a pioneering example of “wiki-democracy.”
In 2011, after two years of debate and discussion, a draft bill was sent to Congress.
Opposition was fierce.
Lobbyists sought to water down the proposed legislation, with telecommunications companies particularly opposed to its net-neutrality provisions.
Nevertheless, the coalition of forces backing the bill – civil-society groups supported by a 350,000-strong petition organized by legendary Brazilian musician and former Minister of Culture Gilberto Gil – overcame traditional political interests.
In the process, they seem to have ushered in a new model of lawmaking.
Elsewhere in the world, however, developments have moved in a different direction.
Obama’s electoral afterglow dimmed.
The Internet-inspired uprisings in Iran and then in much of the Arab world were brutally crushed.
New revelations about government surveillance, together with Heartbleed-type security scares, continue to test the world’s faith in online freedom.
Yet, despite the fears – and the dangers inherent in a free Internet – the world must not give up exploring the transformative potential of cyberspace.
Those who are losing their resolve need to only look south, where Brazil is leading the way.
People Power in the Caucasus
For the past month, women in Georgia who were displaced from Abkhazia during the 1993 conflict have witnessed history moving backwards; everything they lived through 15 years ago is repeating itself.
These women are now hosting a new flood of displaced civilians from Abkhazia and South Ossetia after Russia’s aggression in those regions, as well as within the Georgian territories that Russian forces have occupied since the invasion.
In Tbilisi alone, there are more than 500 camps for internally displaced people, many of them women and children living with shortages of food and medical supplies.
Georgians today hardly feel supportive of their president, Mikheil Saakashvili, who, in a foolish attempt to regain control over South Ossetia, provoked Russia to drop its peacekeeping mission in the region and turn its full military might to pushing Georgian troops out of South Ossetia and Abkhazia, and then to occupying much of Georgia.
The Russians bombed numerous strategic and civilian targets in Georgia, destroying infrastructure and producing growing shortages of food, fuel, and medicine.
People are in despair; they are angry at Russia for its aggression and at their own government for provoking this uneven conflict.
People of different nationalities and ethnicities have been living in this region side by side for centuries, sharing customs, traditions, bread and wine, and mutual respect for each another’s cultures and languages.
But, going back to the Russian, British, and Ottoman Empires that once battled here, they have been continually exploited by politicians and generals.
Women and children suffer the most in times of conflict.
Add to this centuries-old patriarchal traditions, 15-year-old post-war traumas, a 20-year economic crisis, and current Russian aggression, and you may begin to grasp what women in South Ossetia, Abkhazia, and Georgia are enduring these days.
Besides the general devastation that modern warfare brings, impoverished and angry Russian soldiers were wreaking havoc on civilians by stealing belongings left behind and raping women.
In addition, lawlessness was enticing bandits to cross the border and vandalize and rob properties left by fleeing refugees.
News reports and “analysis” by state-controlled channels in both Russia and Georgia that promote negative images of “the enemy” serve only to widen the gap between ethnic groups.
Over the past month, concerned citizens in both Russia and Georgia have started to make attempts to build alliances and reach out to each other outside of the government-controlled media and structures.
There have been action calls and statements circulated on the Web calling on the people of the region to unite and not allow governments to build bigger walls between them.
Despite government propaganda, the region’s people must remember that Russians are not superior to Georgians, Georgians to Ossetians or Abkhazians, and so on.
We need to stop these territorial battles based on national pride and desire to control and rule.
Saakashvili must be pressured to abandon his effort to wield full control over Abkhazia and South Ossetia.
At the same time, the Russian government must be pressured to pull out of the Caucasus and let people there decide their future for themselves.
Now is the time for Georgian, Russian, Abkazian, and Ossetian civilians who are bearing the brunt of the conflict to come together to stop imperial chess games that kill thousands of people and leave thousands more displaced and emotionally wounded.
It is time to help civil society in this area build a world where peace, not warfare, is the rule.
Women’s rights activists in the region should not fall into a brainwashing trap of nationalism and territorial disputes, becoming another tool in the hands of politicians.
They should demonstrate to their governments that they will not succumb to divisive ideology.
Peril or Promise in North Korea?
MADRID – Two days after Kim Jong-il, North Korea’s leader, died in a train in his country, South Korean authorities still knew nothing about it.
Meanwhile, American officials seemed at a loss, with the State Department at first merely acknowledging that press reports had mentioned his death.
The South Korean and US intelligence services’ inability to pick up any sign of what had happened attests to the North Korean regime’s opaque character, but also to their own deficiencies.
American planes and satellites watch North Korea day and night, and the most sensitive intelligence-gathering equipment covers the frontier between the two Koreas.
Nonetheless, we know very little of that country, because all vital information is restricted to a small group of leaders obsessed with secrecy.
The leadership change is occurring at the worst possible time.
It is known that Chinese leaders had hoped that Kim Jong-il would survive long enough to consolidate support among the country’s various factions for the succession of his son, Kim Jong-un.
All of the symbolic attributes of power have been transferred to Kim Jong-un – reflected in his official position in the funeral ceremonies, his presidency of the Military Commission, and his assumption of the ruling party’s highest rank – with remarkable speed.
But such trappings will not make the transition process any easier for a young man of less than 30 in a society where veteran military chiefs retain so much power.
The economic situation, which is still very precarious, with many people living close to starvation, constitutes another key challenge.
Two examples suffice to illustrate the impact: the price of rice has tripled while consumption of electricity is down by two-thirds from two decades ago.
My personal memories of North Korea, now almost ten years old, are of a poor and depressed country.
Pyongyang, the capital, was dark and deserted, illuminated by the cavalcade taking us from the official housing to the opera house, only to return to darkness behind us.
Kim Jong-il was greeted with the same fervor when he entered the opera house that today marks public mourning of his death.
My trip took place in April 2002, a somewhat optimistic time.
The European Union had joined an agreement initiated by the two Koreas and the US within the Korean Energy Development Organization program, the objective being to persuade North Korea to freeze and later dismantle its nuclear program.
In exchange, two light-water nuclear reactors would be built to generate electric energy, and 500,000 metric tons of oil would be supplied annually until the first reactor began operating.
In turn, the EU initiated an extensive humanitarian aid project.
The talks with Kim Jong-il and his collaborators seemed promising.
Unfortunately, the agreement did not last long.
In 2003, North Korea abandoned the Nuclear Non-Proliferation Treaty.
From that moment, all optimism was lost, until contacts were subsequently reinitiated in a complex six-party format (China, Russia, the US, Japan, and the two Koreas) that continued, with ups and downs, until the end of 2007.
Since the maritime incidents of 2009 and 2010, in which North Korean forces attacked South Korean assets, there has been virtually no contact at all between the two sides.
Given North Korea’s behavior over the last decade, the sudden change of leadership increases the threat of unexpected incidents.
In order to limit the risk, it is essential to keep relations with China as transparent as possible.
It is China that has the most direct contact with North Korea, and that could best catalyze resumption of the six-party talks.
China recognizes that North Korea cannot continue in its present form, and would like to see its leaders transform the economy without undertaking substantial political change.
Is that possible?
Could it be done quickly enough to boost other regional players’ confidence that the country’s evolution will be predictable?
For China, problems are judged according to the country’s own history and from the standpoint of domestic policy – all the more so the closer the problem is to its borders.
For the West, especially the US, every problem should have a solution within a finite period of time.
While the US breaks down problems and tries to find solutions for each part, China considers political problems unhurriedly, as an extended process that might have no resolution.
Beyond the six-party talks, it is necessary to create a framework from which a cooperative dialogue between the US and China might emerge.
In the case of Korea – as Christopher Hill, one of the most effective US negotiators on these matters, remembers – the US should make it clear that no possible solution for the divided peninsula would mean a strategic loss for China.
After the armistice that ended the Korean War in 1953, the 38th parallel was established as the limit for US forces’ presence; the importance of that war for China should not be forgotten.
This approach could be one way to stabilize the region during this period of heightened uncertainty.
There might be others.
The ongoing opening in Myanmar (Burma) shows that potentially significant political change does not need to be accompanied by regional instability.
In the case of North Korea, where nuclear arms are in play, it cannot afford to be.
The Perils of Fed Gradualism
NEW HAVEN – By now, it’s an all-too-familiar drill.
After an extended period of extraordinary monetary accommodation, the US Federal Reserve has begun the long march back to normalization.
It has now taken the first step toward returning its benchmark policy interest rate – the federal funds rate – to a level that imparts neither stimulus nor restraint to the US economy.
A majority of financial market participants applaud this strategy.
In fact, it is a dangerous mistake.
The Fed is borrowing a page from the script of its last normalization campaign – the incremental rate hikes of 2004-2006 that followed the extraordinary accommodation of 2001-2003.
Just as that earlier gradualism set the stage for a devastating financial crisis and a horrific recession in 2008-2009, there is mounting risk of yet another accident on what promises to be an even longer road to normalization.
The problem arises because the Fed, like other major central banks, has now become a creature of financial markets rather than a steward of the real economy.
This transformation has been under way since the late 1980s, when monetary discipline broke the back of inflation and the Fed was faced with new challenges.
The challenges of the post-inflation era came to a head during Alan Greenspan’s 18-and-a-half-year tenure as Fed Chair.
The stock-market crash of October 19, 1987 – occurring only 69 days after Greenspan had been sworn in – provided a hint of what was to come.
In response to a one-day 23% plunge in US equity prices, the Fed moved aggressively to support the brokerage system and purchase government securities.
In retrospect, this was the template for what became known as the “Greenspan put” – massive Fed liquidity injections aimed at stemming financial-market disruptions in the aftermath of a crisis.
As the markets were battered repeatedly in the years to follow – from the savings-and-loan crisis (late 1980s) and the Gulf War (1990-1991) to the Asian Financial Crisis (1997-1998) and terrorist attacks (September 11, 2001) – the Greenspan put became an essential element of the Fed’s market-driven tactics.
This approach took on added significance in the late 1990s, when Greenspan became enamored of the so-called wealth effects that could be extracted from surging equity markets.
In an era of weak income generation and seemingly chronic current-account deficits, there was pressure to uncover new sources of economic growth.
But when the sharp run-up in equity prices turned into a bubble that subsequently burst with a vengeance in 2000, the Fed moved aggressively to avoid a Japan-like outcome – a prolonged period of asset deflation that might trigger a lasting balance-sheet recession.
At that point, the die was cast.
No longer was the Fed responding just to idiosyncratic crises and the market disruptions they spawned.
It had also given asset markets a role as an important source of economic growth.
The asset-dependent economy quickly assumed a position of commensurate prominence in framing the monetary-policy debate.
The Fed had, in effect, become beholden to the monster it had created.
The corollary was that it had also become steadfast in protecting the financial-market-based underpinnings of the US economy.
Largely for that reason, and fearful of “Japan Syndrome” in the aftermath of the collapse of the US equity bubble, the Fed remained overly accommodative during the 2003-2006 period.
The federal funds rate was held at a 46-year low of 1% through June 2004, before being raised 17 times in small increments of 25 basis points per move over the two-year period from mid-2004 to mid-2006.
Yet it was precisely during this period of gradual normalization and prolonged accommodation that unbridled risk-taking sowed the seeds of the Great Crisis that was soon to come.
Over time, the Fed’s dilemma has become increasingly intractable.
The crisis and recession of 2008-2009 was far worse than its predecessors, and the aftershocks were far more wrenching.
Yet, because the US central bank had repeatedly upped the ante in providing support to the Asset Economy, taking its policy rate to zero, it had run out of traditional ammunition.
And so the Fed, under Ben Bernanke’s leadership, turned to the liquidity injections of quantitative easing, making it even more of a creature of financial markets.
With the interest-rate transmission mechanism of monetary policy no longer operative at the zero bound, asset markets became more essential than ever in supporting the economy.
Exceptionally low inflation was the icing on the cake – providing the inflation-targeting Fed with plenty of leeway to experiment with unconventional policies while avoiding adverse interest-rate consequences in the inflation-sensitive bond market.
Today’s Fed inherits the deeply entrenched moral hazard of the Asset Economy.
In carefully crafted, highly conditional language, it is signaling much greater gradualism relative to its normalization strategy of a decade ago.
The debate in the markets is whether there will be two or three rate hikes of 25 basis points per year – suggesting that it could take as long as four years to return the federal funds rate to a 3% norm.
But, as the experience of 2004-2007 revealed, the excess liquidity spawned by gradual normalization leaves financial markets predisposed to excesses and accidents.
With prospects for a much longer normalization, those risks are all the more worrisome.
Early warning signs of troubles in high-yield markets, emerging-market debt, and eurozone interest-rate derivatives markets are particularly worrisome in this regard.
The longer the Fed remains trapped in this mindset, the tougher its dilemma becomes – and the greater the systemic risks in financial markets and the asset-dependent US economy.
It will take a fiercely independent central bank to wean the real economy from the markets.
A Fed caught up in the political economy of the growth debate is incapable of performing that function.
Only by shortening the normalization timeline can the Fed hope to reduce the build-up of systemic risks.
The sooner the Fed takes on the markets, the less likely the markets will be to take on the economy.
Yes, a steeper normalization path would produce an outcry.
But that would be far preferable to another devastating crisis.
Education Cannot Wait
LONDON – “Recall the face of the poorest and weakest man you have seen, and ask yourself if this step you contemplate is going to be any use to him.”
These words, spoken by Mahatma Gandhi in 1948, should be taken as a test of our sincerity, and as a challenge to our complacency, when considering the fate of the 30 million children displaced from their homes by civil wars and natural disasters.
More boys and girls have been uprooted by crisis than at any time since 1945.
They are likely to spend their school-age years without entering a classroom, their talents undeveloped and their potential unlocked.
There are now 75 million young people whose education has been interrupted by conflict and crisis.
Yet urgency – and international law, which mandates the education of all displaced children – fails to inspire action.
Displaced children are more likely to become the youngest laborers in the factory, the youngest brides at the altar, and the youngest soldiers in the trench.
Without opportunity, children are vulnerable to extremists and radicalization.
Every year, close to a half-million girls are trafficked and vanish.
The fate of these dispossessed is tethered to the thinnest of lifelines.
When disaster strikes, what can only be described as a begging bowl is handed around the world’s donor community.
Voluntary contributions are then administered by refugee agencies, charities, and NGOs, which heroically help refugees scrape by with the bare essentials – food, water, shelter, and protection.
In such circumstances, the right to education becomes an unaffordable “luxury.”
While UN Peacekeepers are funded by assessed contributions levied on member countries, the millions of children displaced by crises have no guarantee that anyone will fund their schooling.
Indeed, less than 2% of humanitarian aid reaches education.
Syria’s civil war, now entering its sixth year, and the first anniversary of Nepal’s devastating earthquake – two disasters that have forced millions more children into the streets – serve as painful reminders that we lack the means to return children to the classroom in the wake of such tragedies.
A permanent fund securing education for children in emergency situations is long overdue.
Instead of wasting months begging for aid, such a reserve fund would be capable of delivering it immediately.
At the onset of an emergency, the fund would swiftly assess where dispossessed children are, develop a strategy, and put in place a plan to provide education for them.
Now, for the first time, we are in a position to deliver on this promise.
Under the leadership of UNICEF Executive Director Anthony Lake, UNESCO Director-General Irina Bokova, UN High Commissioner for Refugees Filippo Grandi, Chair of the Global Partnership for Education Julia Gillard, and World Bank President, Jim Yong Kim – and with the support of the UN Secretary-General, Ban Ki-moon – an unprecedented humanitarian fund for education in emergencies is about to be established.
The new “Education Cannot Wait” fund will respond to new realities and requirements.
Most child refugees are likely to spend more than a decade out of their own countries, so their plight cannot be considered temporary.
The breakthrough fund will support the education of refugees for up to five years, instead of the mere months of patchwork provisions that are presently on offer.
No longer will humanitarian, security, and development needs be split into silos with their own addresses and agendas.
A single fund will now meet these needs at once.
After all, when refugee children find a place in school, they are more secure, their parents are more hopeful, and their states are more stable.
The fund will not be bound by old World Bank rules that, until recently, excluded the education of refugee children in middle-income countries from concessional loans.
The initiative will be the first official humanitarian fund for education.
And a parallel UN-led fund will have distinct windows through which businesses, foundations, and individuals can contribute.
To be sure, delivering an education to displaced children demands a departure from pure voluntarism: a shift toward assessed contributions collected by levies from wealthy countries.
But, until this shift occurs, we will ask individual philanthropists, corporations and charities – as well as new and old aid donors – to come together to catalyze the venture.
Our sense of urgency, coupled with the requisite funds, promises to have a significant impact.
When it comes to securing schooling, no price tag is too steep.
The fund will harness today’s passions and key innovations.
We want technology firms to play a central role in providing new ideas and disruptive thinking.
We want firms that already offer refugees an online education, internet access and IT hardware to drive the provision of education to displaced and isolated girls and boys.
In establishing this fund, we are ending the era of the formulaic crisis response.
It was Gandhi’s belief that reflection could motivate action by recalibrating our moral compass.
What makes this fund unique is not the series of new benchmarks it sets.
Rather, it signals a change in how we meet the challenges of an emergency.
Going beyond present demands, the fund is based on the needs of the future.
In providing resources to deliver education, the fund is a bold affirmation of a better tomorrow – an unwavering promise to unlock talent, develop potential, and secure futures for all children, wherever they are.
By establishing “Education Cannot Wait”, we are sending a message to the downcast and dispossessed everywhere.
Education, at its best, offers something that food, shelter, and healthcare can never themselves provide: hope, the chance to plan, and to prepare for the future.
Your Data or Your Life
LONDON – Apple’s new watch keeps track of your health.
Google Now gathers the information needed to compute the ideal time for you to leave for the airport.
Amazon tells you the books you want, the groceries you need, the films you will like – and sells you the tablet that enables you to order them and more.
Your lights turn on when you get close to home, and your house adjusts to your choice of ambient temperature.
This amalgamation and synthesis of digital services and hardware is designed to make our lives easier, and there is no doubt that it has.
But have we stopped asking fundamental questions, both of ourselves and of the companies we entrust to do all of these things?
Have we given sufficient consideration to the potential cost of all of this comfort and ease, and asked ourselves if the price is worth it?
Every time we add a new device, we give away a little piece of ourselves.
We often do this with very little knowledge about who is getting it, much less whether we share their ethics and values.
We may have a superficial check-box understanding of what the companies behind this convenience do with our data; but, beyond the marketing, the actual people running these organizations are faceless and nameless.
We know little about them, but they sure know a lot about us.
The idea that companies can know where we are, what we have watched, or the content of our medical records was anathema a generation ago.
The vast array of details that defined a person was widely distributed.
The bank knew a bit, the doctor knew a bit, the tax authority knew a bit, but they did not all talk to one another.
Now Apple and Google know it all and store it in one handy place.
That is great for convenience, but not so great if they decide to use that information in ways with which we do not proactively agree.
And we have reason to call into question companies’ judgment in using that data.
The backlash to the news that Facebook used people’s news feeds to test whether what they viewed could alter their moods was proof of that.
I do not recall checking a box to say that that was okay.
Recently, hackers misappropriated photos sent via Snapchat, a service used primarily by young people that promises auto-deletion of all files upon viewing.
Likewise, health-care data were always considered private, so that patients would be open and honest with health-care professionals.
As the lines between health care and technology businesses become hazy, some manufacturers of “wearables” and the software that runs on them are lobbying to have their products exempted from being considered medical devices – and thus from regulatory requirements for reliability and data protection.
Privacy is only one part of a larger discussion around data ownership and data monopoly, security, and competition.
It is also about control and destiny.
It is about choice and proactively deciding how people’s data are used and how people use their own data.
More mature firms have phased in formal protocols, with ethics officers, risk committees, and other structures that oversee how data are collected and used, though not always successfully (indeed, they often depend on trial and error).
Small new companies may have neither such protocols nor the people – for example, independent board members – to impose them.
If serious ethical lapses occur, many consumers will no longer use the service, regardless of how promising the business model is.
We like new applications and try them out, handing over access to our Facebook or Twitter accounts without much thought about the migration of our personal data from big companies with some modicum of oversight to small companies without rigorous structures and limits.
Consumers believe or expect that someone somewhere is keeping an eye on this, but who exactly would that be?
In Europe, legislation to protect personal data is not comprehensive, and much of the rest of the world lacks even rudimentary safeguards.
After exploring this issue with legislators in several countries over the past couple of months, it has become abundantly clear that many do not have a full grasp of the myriad issues that need to be considered.
It is a difficult subject to address, and doing so is impeded by lobbying efforts and incomplete information.
In the short term, young companies should view ethics not as a marketing gimmick, but as a core concern.
All organizations should invest in ethics officers or some sort of review process involving people who can assess all of the implications of a great-sounding idea.
Legislators need to educate themselves – and the public – and exercise more oversight.
For example, just as many countries did with car seatbelts a generation ago, a public-safety campaign could be paired with legislation to explain and promote two-step verification.
In the longer term, as we rightly move toward universal Internet access, we need to ask: How much of ourselves are we willing to give away?
What happens when sharing becomes mandatory – when giving access to a personal Facebook account is a job requirement, and health services are withheld unless a patient submits their historical Fitbit data?
If that is the future we want, we should stride toward it with full awareness and a sense of purpose, not meander carelessly until we fall into a hole, look up, and wonder how we got there.
Pervez Musharraf’s Long Goodbye
ISLAMABAD – Pervez Musharraf of Pakistan stands virtually alone today while facing the most serious challenge to his presidency: possible impeachment by the new democratically-elected government.
The potential charges are serious: conspiring to destabilize the government that was elected last February, unlawfully removing the country’s top judges in November 2007, and failing to provide adequate security to Benazir Bhutto before her assassination last December.
Allying himself with the Bush administration has increased his unpopularity, especially following missile attacks by the United States in Pakistan’s tribal areas.
Despite earlier differences on how to deal with Musharraf, Pakistan’s leading political parties are now united against him.
Feuding between the Pakistan People’s Party, led by Benazir’s widower, Asif Ali Zardari, and the Pakistan Muslim League (N), led by former Prime Minister Nawaz Sharif, had given Musharraf a chance to regain some standing after his allies were defeated in the February elections.
American reluctance to abandon Musharraf – together with prolonged electricity shortages, which made the new government appear incompetent – also raised his hopes.
Musharraf may be counting on the army, his primary constituency, to bail him out of this crisis.
Though such support remains a possibility, it is unlikely that the army leadership will extend itself on his behalf.
Though a protégé of Musharraf, the army’s chief of staff, General Ashfaq Kayani, is a professional soldier for whom the army’s institutional interests are more important than the political interests of his former army boss.
Kayani has repeatedly declared that the army will not interfere in political affairs, and that the parliament and constitution are supreme.
Even if the army is tempted to step in on Musharraf’s behalf, it has been chastened by political developments during the past year.
The entire legal community arose to demand restoration of the country’s judges and reinforcement of the rule of law. The public’s demand for free elections and the resulting creation of a democratic government have forced the military to accept the public will.
The army has also paid a heavy price for Musharraf’s approach to the war on terror.
Suicide bombers have struck repeatedly at military installations and personnel around the army’s headquarters in Rawalpindi.
An increase in deadly attacks on army convoys in the Pakistan-Afghanistan tribal areas has also pushed the army away from Musharraf.
Though the army has reaped a financial windfall from US military aid, and has targeted many foreign militants allied with al-Qaeda in the region, its performance against Pakistani militants has been mixed at best.
Consequently, the prestige of the Taliban and other militant groups operating in the area has grown.
In this context, the army, seeking to avoid sole responsibility for reverses, wants a popular government to take charge of policy.
No such government can emerge if the elected parties are unseated.
Nevertheless, there are signs of disagreement on important matters between the government and the army.
The military recently blocked a government move to place Pakistan’s infamous intelligence service, the ISI, under the control of the interior minister rather than the prime minister.
Musharraf backed the military’s opposition to this reform, gaining some gratitude from military commanders.
During Prime Minister Yousaf Raza Gilani’s recent visit to the US, President Bush repeatedly said that his administration supports Pakistan’s democracy, a policy since reiterated by Secretary of State Condoleezza Rice.
This indicates that the US will not back Musharraf in any confrontation between him and Pakistan’s democratic forces.
Most Pakistanis hope so.
Musharraf must assess what will be his legacy.
Rather than trying to face down impeachment and prolonging the crisis, he should recognize that Pakistan cannot afford more instability, and that giving up honorably will bring him some respect.
For the sake of argument, even if Musharraf faces impeachment and by some stroke of luck is saved from being thrown out of office, his future will be bleak.
In March 2009, the current ruling coalition will gain more seats in the Senate, and the government would almost certainly try to impeach him again.
Moreover, any attempt by Musharraf to dislodge the government by using his constitutional authority would trigger another election, the results of which would not be much different from the vote in February.
It is time for Musharraf’s friends in the West to press him to serve his country one last time, by avoiding confrontation with his country’s democratic forces and calling it quits.
The Case for Better Government
LONDON – A recent survey by the World Economic Forum’s Network of Global Agenda Councils rated government lower than either business or media in its ability to respond to global challenges.
On one level, this is understandable, given the plethora of challenges that governments face and the lack of long-term solutions to many problems that demand one.
But, on another level, the attempt to rate government alongside business and the media is fundamentally misguided: no sector operates at the scale of responsibility, accountability, and expectation that governments do.
Business decides for itself where to invest and grow.
Media indulge themselves in a fast-moving news cycle.
Government enjoys neither luxury.
It cannot simply pack up and move on when it faces a loss or is bored with a story.
Government must stay put – and must often clean up the messes left behind by those who do not.
On a good day, it may even get to make improvements.
The problem for governments, more often than not, is that in attempting to respond to and reconcile often conflicting individual, family, and national needs, their ability to deliver results efficiently and effectively has declined.
As a result, trust in government has plummeted.
Just before the WEF’s Summit on the Global Agenda in Abu Dhabi last month, I spent a week in India.
Most of the people with whom I spoke complained endlessly about government shortcomings.
Government at both the federal and state levels was invariably regarded as slow, indecisive, corrupt, unimaginative, and shortsighted – in general, worthless.
It is easy for business to want government just to get out of the way, and for the media to point fingers and sensationalize events without much depth of analysis – or even, sometimes, grasp of reality.
True, India may not be the best advertisement for democracy in some respects, given how hard it often seems there to make long-term decisions and implement them without being buffeted – and frequently derailed – by volatile public opinion and hard-nosed vested interests.
But the alternative – rule not by law but by dictatorship – is a far nastier prospect.
And there are not many problems in India’s governance that state funding of politics could not solve.
After all, when serving a vast democracy requires non-stop electioneering, and politicians are thus dependent on financial donations, governance is bound to go awry.
Governments’ ability to respond to global challenges is a more general problem.
Globalization – the breaking down of national boundaries and the integration of economies across continents – has resulted in burgeoning demands on governments at the same time that their ability to provide answers has been reduced.
In other words, demand for government is exceeding supply.
Globalization has made many people feel more insecure and in need of government support to cope with the pressures on their livelihoods and quality of life.
But most of the policy responses needed to meet people’s demand for greater security are beyond the scope and reach of national governments, especially when these governments are trying to cope on their own.
That is why, long ago, European countries saw the sense in pooling their weight and reach through the European Union.
Imperfect as the EU is, it still represents the best response to globalization yet seen among any extended group of countries.
Governments working together are better than governments working separately – or, worse, against one another.
We live in an increasingly multipolar world, in which major emerging economies and their populous societies are transforming the international landscape.
But, at the same time, multilateral frameworks are in decline, undermining the ability to bring sense and coherence to this world.
Consider the international trading system and its centerpiece, the World Trade Organization.
Since the demise of the Doha Round, the WTO’s standing as a multilateral negotiating forum has declined sharply, salvaged in part by the recent agreement in Bali.
It will be essential, after Bali, to reflect seriously on the next phase for the WTO and its role in the international trading system.
The major international financial institutions – the International Monetary Fund, the World Bank, the European Bank for Reconstruction and Development, and the regional development banks – are having to work hard to make themselves fit for the twenty-first century.
The authority of the United Nations has frayed.
Until we reverse the trend of declining multilateralism, governments’ ability to respond to global challenges will not improve.
Business can moan and the media can carp, but the answer to many of the world’s great problems is more – or at least better – government, not less.
A better supply of government meeting the flow of demands.
Ronald Reagan famously insisted that, “government is not the solution to our problem; government is the problem.”
Today we know better: If government is not part of the solution, our problems will only get bigger.
The Gospel of Francis
GLENDALE, CALIFORNIA – Pope John XXIII and Pope John Paul II are the odd couple of modern Roman Catholicism.
The avuncular John XXIII, who wanted to loosen up a hidebound Church, and the combative John Paul II, who struggled to rein in what he viewed as the excesses of the Second Vatican Council that John XXIII had convened, appear to be ideological opposites.
Yet Pope Francis will canonize the pair this month – a surprising move that may offer a glimpse into his goals for the Church.
The most obvious implication is that Francis is weary of polarization, and hopes that the dual canonization will help to propel a shift toward a “big tent” Catholicism that appeals to a broader range of people.
Francis is certainly in a strong position to initiate such a shift; his political capital is extraordinarily high, surpassing even that of US President Barack Obama in his early days in office.
People seem to appreciate his preference for teaching by example and dramatic gesture – exemplified by his canonization of the rival stars of Vatican II – rather than by encyclical.
Will this strategy be enough to bring lapsed Catholics back into the fold, or to bridge the gap between the largely conservative generation of “John Paul II priests” and younger, more liberal Catholics?
Will the recalcitrant elements in the ecclesiastical establishment be amenable to Francis’s changes?
The modernization process that John XXIII initiated was in line with the ebullience of the early 1960’s.
John Paul II viewed his long tenure, from 1978 to 2005, as a course correction to the moral license and sexual experimentation of that decade.
Francis has framed his pontificate as a sort of synthesis of this dialectical tension.
So far, Francis’s actions have not sparked the kind of confrontational response or mass mobilization this is often associated with regime change in authoritarian systems.
But that does not mean that it will be smooth sailing.
Francis still faces widespread apathy among believers, especially in the advanced industrial societies, where disaffected Catholics are increasingly questioning their commitment or simply opting out.
While Francis’s soothing manner seems to be helping to slow this erosion, whether he will be able to reverse the trend remains far from certain.
For starters, it is more difficult to rally support for a positive agenda than it is to build solidarity through opposition to presumed enemies.
Instead of employing the dyspeptic condemnations of “relativism,” “secularism,” “nihilism,” and other “-isms” that John Paul II and his successor Pope Benedict XVI used to rally their base, Francis has formulated a positive agenda focused on social justice.
And, while rising global concern about income inequality has put some wind in Francis’s sails, his agenda has yet to make an impact on Catholic policymakers like Republican US Congressmen Paul Ryan and John Boehner or powerful figures in the Church.
A second potential obstacle for Francis is that he, like his predecessors, is promoting a revolution from above.
The Vatican II reforms, for example, emerged not from a populist groundswell, but from the preferences of progressive theologians and bishops.
This hierarchical approach, in which spectatorship trumps participation among the rank and file, is a major reason for indifference among ordinary Catholics today.
Another key challenge for Francis is the laity’s growing independence from clerical authority.
In recent decades, lay managers and workers in the church’s network of schools, hospitals, and social-service providers have largely replaced priests and nuns.
As a result, since Vatican II, episcopal – and ultimately Vatican – control over church-linked ministries has gradually waned.
Around half of the budget of Catholic Charities USA, for example, comes from the government.
Virtually none of these operations is in open rebellion against Rome; many just quietly go their own way.
So do many ordinary Catholics.
A self-described “obedient son of the church,” he is unlikely to surprise the faithful when it comes to matters like sexuality and women’s ordination.
At the same time, Francis’s Jesuit background has instilled in him a certain independence and flair for the quirky (like nonchalantly sanctifying John XXIII and John Paul II in one fell swoop).
The Society of Jesus – whose spirituality is based on a seemingly discordant yet enduring combination of mysticism and realpolitik – has forged a paradoxical alliance of fidelity to corporate commands and willingness to give individual talents their due.
Traditional hierarchies routinely cultivate eccentrics as harmless adornments of a rigid order.
Catholicism has had mad popes and bad popes and occasionally good popes.
It is used to seeing the world turn upside down for a few days during carnival, and then go back to normal.
But there is something to be said for the fact that Francis is ultimately his own man.
Now, what Max Weber called the slow grinding of hard boards – leveraging a concrete program of structural reform onto a charismatic personality – begins.
Or the enterprise stays where it often seems to be: mostly smoke and mirrors, a grand illusion.
Ethics in Gaza
MELBOURNE – Is Israel’s military action in Gaza morally defensible?
Different answers to that question are possible.
Some depend on answers to prior questions about the founding of the state of Israel, the circumstances that led to many Palestinians becoming refugees, and responsibility for the failure of earlier efforts to reach a peaceful solution.
But let us put aside these questions – which have been explored in great depth – and focus on the moral issues raised by the latest outbreak of hostilities.
The immediate trigger for the current conflict was the murder of three Jewish teenagers in Hebron, on the West Bank.
Israel, blaming Hamas, arrested hundreds of its members in the West Bank, though it has never explained the basis of its accusation.
The Israeli government may have seized on the outrageous murders as a pretext for provoking Hamas into a response that would allow Israel, in turn, to invade and destroy the tunnels Hamas has dug from Gaza into Israel.
Though Israeli leaders claim to have been surprised by the extent and sophistication of the tunnels they discovered, Israel’s military briefed the government on the tunnels more than a year ago, and the government created a special task force to consider how to deal with them.
Hamas responded to the West Bank arrests with a barrage of rockets that reached Tel Aviv and Jerusalem, though without causing any injuries.
Israel then began its air attacks, followed by a ground invasion.
At the time of writing, more than 1,600 Palestinians have been killed, most of them civilians, by Israeli air and ground strikes.
Three Israeli civilians have been killed by rocket or mortar fire from Gaza, and 64 Israeli soldiers have been killed since the ground invasion began.
In firing rockets at Israel, Hamas invited a military response.
A country subject to rocket attacks from across its border has a right to defend itself, even if its own actions can be construed as having provoked the attacks, and the attacks themselves are relatively ineffective.
But a right of self-defense does not mean a right to do anything that can be construed as a defensive act, regardless of the cost to civilians.
Despite calls in some Israeli media for Gaza to be bombed “back to the stone age,” the Israeli government seems to accept that that would be wrong.
Israel has taken some steps to minimize civilian casualties by warning Palestinians to evacuate areas that were about to be targeted.
Hamas, by contrast, has shown no interest in avoiding civilian casualties, either in Israel or in Gaza.
The whole point of firing rockets at Israeli cities is to inflict civilian casualties, and the fact that the rockets have largely failed to do so is due to their inaccuracy, Israel’s “Iron Dome” missile-defense system, and perhaps some good luck.
Hamas’s strategy of launching rockets from residential areas and storing them in schools clearly reflects its leaders’ willingness to put Palestinian civilians in harm’s way in order to confront Israel with the grim choice of killing civilians or allowing the rocket attacks to continue.
So, whatever moral objections to Israel’s actions over the past month there might be, there are even more serious objections to be made against Hamas.
In contrast to previous episodes, Arab countries like Egypt, Jordan, and Saudi Arabia have been very restrained in their criticism of Israel, though perhaps less for moral reasons and more because they regard militant Islam as a graver threat than Israel to their own regimes.
But to say that Israel’s actions are less clearly wrong than those of Hamas is not to say much.
Israel has legitimate military objectives in Gaza: to stop the rockets and destroy the tunnels.
It should be pursuing those objectives while showing the utmost concern for Gaza’s trapped civilians.
In a recent article, Fania Oz-Salzberger, writing from Tel Aviv while rockets were being intercepted overhead, urged her government to send medical supplies to the villages of Gaza.
Since then, the Israeli military has set up a field hospital on the border with Gaza to treat wounded Palestinians.
That is a positive step, but it is outweighed by repeated instances of Israeli airstrikes and shelling that appear to have needlessly killed civilians, from the four boys killed on a beach on July 16 to the 20 Palestinian civilians killed while taking refuge in a United Nations school on July 30.
These incidents are reminiscent of past NATO operations in Afghanistan, in which there was manifestly less care taken to safeguard the lives of local civilians than there would have been if the lives of NATO troops, or their civilian compatriots, had been at risk.
Some will shrug and say, “War is hell.”
But between the untenable extremes of pacifism and the elevation of war to something beyond morality, there is a middle ground that seeks to minimize the unquestionable evil of war.
We can acknowledge that Israel has made some efforts to do that, but we must still say: It is not enough.
The Ethical Cost of High-Price Art
MELBOURNE – In New York last month, Christie’s sold $745 million worth of postwar and contemporary art, the highest total that it has ever reached in a single auction.
Among the higher-priced works sold were paintings by Barnett Newman, Francis Bacon, Mark Rothko, and Andy Warhol, each of which sold for more than $60 million.
According to the New York Times, Asian collectors played a significant part in boosting prices.
No doubt some buyers regard their purchases as an investment, like stocks or real estate or gold bars.
In that case, whether the price they paid was excessive or modest will depend on how much the market will be willing to pay for the work at some future date.
But if profit is not the motive, why would anyone want to pay tens of millions of dollars for works like these?
They are not beautiful, nor do they display great artistic skill.
They are not even unusual within the artists’ oeuvres.
Do an image search for “Barnett Newman” and you will see many paintings with vertical color bars, usually divided by a thin line.
Once Newman had an idea, it seems, he liked to work out all of the variations.
Last month, someone bought one of those variations for $84 million.
A small image of Marilyn Monroe by Andy Warhol – there are many of those, too – sold for $41 million.
Ten years ago, the Metropolitan Museum of Art in New York paid $45 million for a small Madonna and Child by Duccio.
Subsequently, in The Life You Can Save, I wrote that there were better things that the donors who financed the purchase could have done with their money.
I haven’t changed my mind about that, but the Met’s Madonna is beautifully executed and 700 years old.
Duccio is a major figure who worked during a key transitional moment in the history of Western art, and few of his paintings have survived.
None of that applies to Newman or Warhol.
Perhaps, though, the importance of postwar art lies in its ability to challenge our ideas.
That view was firmly expressed by Jeff Koons, one of the artists whose work was on sale at Christie’s.
Koons answered: “With Reaganism, social mobility is collapsing, and instead of a structure composed of low, middle, and high income levels, we’re down to low and high only...
My work stands in opposition to this trend.”
Art as a critique of luxury and excess!
Art as opposition to the widening gap between the rich and the poor!
How noble and courageous that sounds.
But the art market’s greatest strength is its ability to co-opt any radical demands that a work of art makes, and turn it into another consumer good for the super-rich.
When Christie’s put Koons’s work up for auction, the toy train filled with bourbon sold for $33 million.
If artists, art critics, and art buyers really had any interest in reducing the widening gap between the rich and the poor, they would be focusing their efforts on developing countries, where spending a few thousand dollars on the purchase of works by indigenous artists could make a real difference to the wellbeing of entire villages.
Nothing I have said here counts against the importance of creating art.
Drawing, painting, and sculpting, like singing or playing a musical instrument, are significant forms of self-expression, and our lives would be poorer without them.
In all cultures, and in all kinds of situations, people produce art, even when they cannot satisfy their basic physical needs.
But we don’t need art buyers to pay millions of dollars to encourage people to do that.
In fact, it would not be hard to argue that sky-high prices have a corrupting influence on artistic expression.
As for why buyers pay these outlandish sums, my guess is that they think that owning original works by well-known artists will enhance their own status.
If so, that may provide a means to bring about change: a redefinition of status along more ethically grounded lines.
In a more ethical world, to spend tens of millions of dollars on works of art would be status-lowering, not status-enhancing.
Such behavior would lead people to ask: “In a world in which more than six million children die each year because they lack safe drinking water or mosquito nets, or because they have not been immunized against measles, couldn’t you find something better to do with your money?”
A Statue for Stalin?
PRINCETON – Hitler and Stalin were ruthless dictators who committed murder on a vast scale.
But, while it is impossible to imagine a Hitler statue in Berlin, or anywhere else in Germany, statues of Stalin have been restored in towns across Georgia (his birthplace), and another is to be erected in Moscow as part of a commemoration of all Soviet leaders.
The difference in attitude extends beyond the borders of the countries over which these men ruled.
In the United States, there is a bust of Stalin at the National D-Day Memorial in Virginia. In New York, I recently dined at a Russian restaurant that featured Soviet paraphernalia, waitresses in Soviet uniforms, and a painting of Soviet leaders in which Stalin was prominent.
New York also has its KGB Bar.
To the best of my knowledge, there is no Nazi-themed restaurant in New York; nor is there a Gestapo or SS bar.
So, why is Stalin seen as relatively more acceptable than Hitler?
At a press conference last month, Russian President Vladimir Putin attempted a justification.
Asked about Moscow’s plans for a statue of Stalin, he pointed to Oliver Cromwell, the leader of the Parliamentarian side in the seventeenth-century English Civil War, and asked: “What’s the real difference between Cromwell and Stalin?”
He then answered his own question: “None whatsoever,” and went on to describe Cromwell as a “cunning fellow” who “played a very ambiguous role in Britain’s history.”
(A statue of Cromwell stands outside the House of Commons in London.)
“Ambiguous” is a reasonable description of the morality of Cromwell’s actions.
While he promoted parliamentary rule in England, ended the civil war, and allowed a degree of religious toleration, he also supported the trial and execution of Charles I and brutally conquered Ireland in response to a perceived threat from an alliance of Irish Catholics and English Royalists.
But, unlike Cromwell, Stalin was responsible for the deaths of very large numbers of civilians, outside any war or military campaign.
According to Timothy Snyder, author of Bloodlands, 2-3 million people died in the forced labor camps of the Gulag and perhaps a million were shot during the Great Terror of the late 1930’s.
Another five million starved in the famine of 1930-1933, of whom 3.3 million were Ukrainians who died as a result of a deliberate policy related to their nationality or status as relatively prosperous peasants known as kulaks.
Snyder’s estimate of the total number of Stalin’s victims does not take into account those who managed to survive forced labor or internal exile in harsh conditions.
Including them might add as many as 25 million to the number of those who suffered terribly as a result of Stalin’s tyranny.
The total number of deaths that Snyder attributes to Stalin is lower than the commonly cited figure of 20 million, which was estimated before historians had access to the Soviet archives.
It is nonetheless a horrendous total – similar in magnitude to the Nazis’ killings (which took place during a shorter period).
Moreover, the Soviet archives show that one cannot say that the Nazi’s killings were worse because victims were targeted on the basis of their race or ethnicity.
Stalin, too, selected some of his victims on this basis – not only Ukrainians, but also people belonging to ethnic minorities associated with countries bordering the Soviet Union.
Stalin’s persecutions also targeted a disproportionately large number of Jews.
There were no gas chambers, and arguably the motivation for Stalin’s killings was not genocide, but rather the intimidation and suppression of real or imaginary opposition to his rule.
That in no way excuses the extent of the killing and imprisonment that occurred.
If there is any “ambiguity” about Stalin’s moral record, it may be because communism strikes a chord with some of our nobler impulses, seeking equality for all and an end to poverty.
No such universal aspiration can be found in Nazism, which, even on its face, was not concerned about what was good for all, but about what was good for one supposed racial group, and which was clearly motivated by hatred and contempt for other ethnic groups.
But communism under Stalin was the opposite of egalitarian, for it gave absolute power to a few, and denied all rights to the many.
Those who defend Stalin’s reputation credit him with lifting millions out of poverty; but millions could have been lifted out of poverty without murdering and incarcerating millions more.
Others defend Stalin’s greatness on the basis of his role in repelling the Nazi invasion and ultimately defeating Hitler.
Yet Stalin’s purge of military leaders during the Great Terror critically weakened the Red Army, his signing of the Nazi-Soviet Non-Aggression Pact in 1939 paved the way for the start of World War II, and his blindness to the Nazi threat in 1941 left the Soviet Union unprepared to resist Hitler’s attack.
It remains true that Stalin led his country to victory in war, and to a position of global power that it had not held before and from which it has since fallen.
Hitler, by contrast, left his country shattered, occupied, and divided.
People identify with their country and look up to those who led it when it was at its most powerful.
That may explain why Muscovites are more willing to accept a statue of Stalin than Berliners would be to have one of Hitler.
But that can be only part of the reason for the different treatment given to these mass murderers.
It still leaves me puzzled about New York’s Soviet-themed restaurant and KGB Bar.
America’s Drone Dilemma
PRINCETON – Last month, Faisal bin Ali Jaber traveled from his home in Yemen to Washington, DC, to ask why a United States drone had fired missiles at, and killed, his brother-in-law, a cleric who had spoken out against Al Qaeda.
Also killed in the attack was Jaber’s nephew, a policeman who had come to offer protection to his uncle.
Congressional representatives and government officials met Jaber and expressed their condolences, but provided no explanations.
“For years,” Karzai said in a statement issued after the strike, “our people are being killed and their houses are being destroyed under the pretext of the war on terror.”
The war on terror is real enough, and not just a pretext, but so are the civilian casualties that have been occurring for years.
In 2006, I wrote about a US missile attack on a house in Damadola, a Pakistani village near the Afghan border, in which 18 people were killed, including five children.
Then-President George W. Bush did not apologize for the attack, nor did he reprimand those who ordered it.
This was, I pointed out, difficult to reconcile with his assertion (concerning the ethics of destroying human embryos to create stem cells) that America’s president has “an important obligation to foster and encourage respect for life in America and throughout the world.”
Before Barack Obama became President, he argued that, because the US did not have enough troops on the ground in Afghanistan, it was “air-raiding villages and killing civilians, which is causing enormous problems there.”
As Karzai’s statement shows, the problems have not gone away.
Nor have they been limited to Afghanistan.
Civilian casualties caused by the US have been a major source of difficulties in US-Pakistan relations as well.
In September, Ben Emmerson, the United Nations special rapporteur on human rights and counterterrorism, reported that the US had caused at least 400 civilian deaths in Pakistan, with another 200 of those killed being “probable noncombatants.”
(Apart from the problem of knowing who was killed, there is the separate question of how, in a war with no armies, one defines a combatant.
Does cooking for militants warrant being killed?)
Emmerson’s report drew on figures supplied by Pakistan’s foreign ministry, but it was promptly undercut by the country’s defense ministry, which issued its own figures indicating that only 67 of the 2,227 people killed by drone attacks since 2008 were civilians.
That low number surprised many observers.
Last May, in a speech at the National Defense University, Obama defended America’s use of drones.
The US, he said, was attacked on September 11, 2001, so the war it is waging against Al Qaeda, the Taliban, and their associated forces is a just one.
Referring to the key criteria set out in many discussions of traditional “just war” doctrine, he called it “a war waged proportionally, in last resort, and in self-defense.”
That may be true of the war against Al Qaeda; it is less obviously correct with regard to the Taliban.
Obnoxious as their rule over Afghanistan was, the Taliban did not attack America, and Bush’s war against it was not a war of last resort.
Obama acknowledged that innocent people had been killed in US drone attacks, but defended the strikes on the grounds that by eliminating Al Qaeda operatives, they have disrupted terrorist plots and saved lives.
He pointed out that the number of Muslims killed by Al Qaeda’s terrorist attacks “dwarfs any estimate of civilian casualties from drone strikes.”
Doing nothing, Obama added, “is not an option.”
Nor – and here he has reversed himself since 2007 – is “putting boots on the ground” likely to cause fewer civilian casualties than drone strikes.
Doing so would only lead to the US being seen as an occupying army – a perception that would yield “a torrent of unintended consequences.”
Obama did, however, promise a change of policy, indicating that before any strike was undertaken, he would require “near-certainty that no civilians will be killed or injured – the highest standard we can set.”
Since that speech, the frequency of strikes in Yemen and Pakistan has declined, but civilian casualties have continued, albeit at a lower rate.
The “near-certainty” standard appears not to have been met.
As long as Al Qaeda continues to plan terrorist attacks, the US cannot reasonably be required to forego opportunities to kill its leaders and others carrying out these attacks.
But Obama did promise greater transparency, which is essential to any sound debate over the rights and wrongs of drone attacks, and to democratic control over how the US is waging its war against terrorism.
The administration’s refusal to apologize to Jaber, or even to explain what went wrong, indicates that this promise has yet to be fulfilled.
Is There A Right To Secede?
BARCELONA – The European Union has brought 28 countries into a closer political and economic union.
Paradoxically, it has also made it more feasible to contemplate the breakup of some of those countries.
Independence for a small state outside of a political and economic group like the EU would be risky nowadays.
Within the EU, however, the barriers between states – and thus the economic and political risks of independence – are lower.
Consider Scotland, where a popular referendum on independence will be held on September 18.
The referendum is the result of the landslide victory by the Scottish National Party in the 2011 Scottish Parliament election.
British Prime Minister David Cameron has argued against Scotland leaving the United Kingdom, but he has not opposed holding the referendum.
Opinion polls taken since the wording of the referendum (“Should Scotland be an independent country?”) was announced indicate that the “yes” side is unlikely to gain a clear majority.
In Spain, there is a national debate about independence for Catalonia, where national identity is strengthened by the fact that the majority of the region’s residents speak Catalan as well as Spanish.
By contrast, only about 1% of Scots can speak Scottish Gaelic.
Perhaps as a result, support for independence in Catalonia appears to be far broader, with about half of the region’s residents saying that they support secession.
But the Spanish parliament voted overwhelmingly against allowing the Catalan government to hold a referendum on independence, and the central government has said that such a vote would be unconstitutional.
Artur Mas, President of Catalonia’s regional government, has vowed to go ahead with a non-binding referendum anyway.
If a majority of the voters in a distinct region of a country favor independence, does that mean that they have a right to secede?
There are surely more questions that need to be addressed than that single one.
What if a region’s secession leaves behind a rump state that is no longer viable?
Within the EU, this is less of an issue, given that small states – in theory – still benefit from free trade within the Union; but, outside of the EU, the situation of the remaining state can be dire.
In September 1938, Hitler threatened to attack Czechoslovakia in order to bring the ethnic Germans living near the German border under his rule.
The Munich Agreement gave this region, referred to by the Nazis as the Sudetenland, to Germany, leaving Czechoslovakia without defensible borders and paving the way for the Nazi invasion and partition of the country the following March.