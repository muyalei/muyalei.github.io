According to his staff, he developed and improved this gradually over his career.
Oratory and inspirational rhetoric, however, are not the only forms of communication with which leaders frame issues and create meaning for their followers.
Alan Greenspan, the former chairman of the Federal Reserve Board, was hardly an inspirational speaker, but markets and politicians hung on his every word, and he tailored the nuances of his language to reinforce the direction in which he wanted to lead monetary policy.
Unfortunately, as the financial crisis of 2008 demonstrated, it would have been better if Congressional committees had pressed him to communicate more clearly.
Non-verbal signals are also an important component of human communications.
Symbols and examples can be very effective.
Some inspirational leaders are not great orators – witness Mahatma Gandhi.
But the symbolism of Gandhi’s simple dress and lifestyle spoke louder than words.
If one compares those images with pictures of the young insecure Gandhi dressed as a proper British lawyer, one can see how carefully he understood symbolic communication.
He ensured that actions such as the famous 1930 salt march to the sea maintained a slow pace that allowed the drama and tension to build.
The march was designed for communication, not the ostensible reason of resisting the colonial government’s prohibition on the fabrication of salt.
T.E. Lawrence (“Lawrence of Arabia”) also understood how to communicate with symbols.
When he went to the Paris Peace Conference at World War I’s end, he wore Bedouin robes to dramatize the Arab cause.
A year later, at a Cairo conference that negotiated borders in the region, he changed to a British officer’s uniform as he engaged in hard transactional bargaining.
Or, to take a contemporary example, the British entrepreneur Richard Branson overcame dyslexia and poor academic performance by using events and public stunts to promote his Virgin brands.
In addition to communicating with distant audiences, leaders need the ability to communicate one-on-one or in small groups.
In some cases, that close communication is more important than public rhetoric.
Organizational skills – the ability to attract and inspire an effective inner circle of followers – can compensate for rhetorical deficiencies, just as effective public rhetoric can partly compensate for low organizational skills.
Hitler was skillful at communicating with both distant and inner-circle audiences.
Stalin relied primarily on the latter.
Harry Truman was a modest orator, but compensated by attracting and ably managing a stellar set of advisers.
A good narrative is a great source of soft power, and the first rule that fiction writers learn about good narrative is to “show, not tell.”
Franklin Roosevelt used the fictional story of lending a garden hose to a neighbor whose house was on fire to explain his complex lend-lease program to the American people before World War II.
Ronald Reagan was a master of the well selected anecdote.
Setting the right example is another crucial form of communication for leaders.
Anticipating a skeptical public reaction when Singapore raised the salaries of government officials in 2007, Prime Minister Lee Hsien Loong announced that he would forgo the raise himself.
In the aftermath of the recent financial crisis, some business executives voluntarily reduced their salaries as a means of communicating concern for their employees and public opinion.
During the 2008 presidential election campaign, Obama proved to be a talented communicator.
Not only was his rhetorical style effective, but after inflammatory comments by his pastor threatened to derail his campaign, he produced one of the best speeches on race in America since the days of King.
As president, Obama continues to communicate effectively, but an American president has a problem of dual audiences.
Sometimes rhetoric that fares well at home – such as Bush’s second inaugural address – sounds hypocritical to foreign ears.
In contrast, Obama’s inaugural address was well received both at home and abroad.
In a series of foreign-policy speeches, most notably one delivered in Cairo and addressed to the Muslim world, polls show that Obama has been able to restore some of America’s soft power.
So far, so good, but effective leadership is also communicated by actions and policies.
At this stage, it is too early to determine whether Obama’s policies will reinforce or undercut the effects of his words.
As we await the results, it helps to remember the complexity of the relation between effective leadership and communications.
The Making of Sputnik
MOSCOW &#45;&#45; On October 4, 1957, my father, Nikita Khrushchev, awaited a telephone call.
Chief Designer Sergei Korolev was expected to call from the Tyuratam launch site (later renamed Baikonur Cosmodrome) in Kazakhstan to report on the launch of the world’s first man-made satellite.
Earlier that day, my father was in Kyiv, Ukraine, on military business.
He attended a demonstration of tanks crossing the Dnieper River, then discussed with Soviet generals the fate of Defense Minister Marshal Georgy Zhukov.
(Zhukov was suspected of plotting to seize power, and, before forcing a decorated World War II general to resign, my father and his colleagues enlisted the support of other high-ranking generals, who all agreed with Khrushchev’s plan.)
That evening, my father dined with the Ukrainian leaders.
I sat at the end of the table, not paying attention to their conversation.
Everybody was tired, but my father wasn’t in a hurry to sleep.
Around midnight, the door opened and the secretary asked my father to take a phone call.
When Khrushchev came back, he was smiling: Sputnik’s launch was successful.
Soviet engineers began designing Sputnik in January 1956.
The plan was to launch it with the R-7, an intercontinental ballistic missile in development since 1954.
But the rest of the world paid no attention to the vague pronouncements of a possible launch that had been appearing in the Soviet press; everybody outside the Soviet Union knew the United States would launch the world’s first satellite.
Soviet scientists believed that the Americans would keep their plans secret until after they had succeeded in launching a satellite, so all our efforts were put into beating the Americans to the launch.
In August and September, R-7 missiles were successfully launched twice.
Work went on around the clock.
Sputnik’s launch made the front page of Pravda, but just barely.
The story occupied the same amount of space as a report on Zhukov’s visit to Yugoslavia, and ran in a less prestigious position.
There were no banner headlines or enthusiastic comments.
The reason was simple.
My father and all the Soviet people thought that Sputnik’s success was natural, that, step by step, we were getting ahead of the Americans.
After all, we – not the Americans – had opened the world’s first nuclear power plant.
The Soviet MiG set world records in the 1950’s, and the Soviet Tu-104 was the most efficient airliner of its class.
So Sputnik did not surprise us.
Nor did the press report Korolev’s name.
The KGB knew that there was really no need to keep his name secret, but, as KGB chief Ivan Serov told me, the enemy’s resources were limited, so let them waste their efforts trying to uncover “non-secret” secrets.
But the world was desperate to learn his identity.
The Nobel Prize committee decided to give an award to Sputnik’s “chief designer,” but first it needed the person’s name.
The committee requested it from the Soviet government.
My father weighed his response carefully.
The matter was complicated, and his concern wasn’t confidentiality.
The Council of Chief Designers was in charge of all space projects.
Korolev was the head of the council, but the other Chief Designers ­– more than a dozen – considered themselves no less significant.
My father understood that the Chief Designers were ambitious and jealous people.
If the Nobel committee were to give the award only to Korolev, my father thought, the members would fly into a rage.
They would refuse to work with Korolev.
A well-organized team would collapse like a house of cards, and the hopes for future space research and missile design would be dashed, threatening the country’s security.
As my father saw it, you could order scientists and engineers to work together, but you couldn't force them to create something.
In the end, my father told the Nobel committee that all of the Soviet people had distinguished themselves in the work on Sputnik, and that they all deserved the award.
Korolev was offended but kept silent.
The Nobel Prize went to somebody else.
But, despite the pains my father had taken, the other designers expressed growing discontent about Korolev getting all the publicity, even if anonymously.
In their “secret” world, it wasn’t any secret who was behind the title “Chief Designer.”
The first to revolt was engine designer Valentin Glushko, whose RD-170 liquid-propellant engine is used on Russian and some American rockets.
During one council meeting, Glushko said, “My engines could send into space any piece of metal.”
Korolev was offended; his rocket wasn’t just a piece of metal, and, after his success with Sputnik, he no longer considered Glushko his equal.
The dispute was hushed up, but the resentment lingered.
Soon, Glushko offered his services to other Soviet rocket designers, Mikhail Yangel and Vladimir Chelomei – Korolev's rivals.
Even my father couldn’t make peace between them.
Technically, Glushko, by government order, continued to design engines for Korolev, but the work wasn’t good.
So, despite Sputnik’s initial triumph, a decade later the Soviets lost the race to the moon to the Americans.
The Malaysian Miracle
August 31 marked the 50th anniversary of Malaysia’s Merdeka: independence after more than 400 years of colonialism.
Malaysia’s peaceful, non-violent struggle may not have received the attention that Mahatma Gandhi’s did in India, but what Malaysia has accomplished since then is impressive – and has much to teach the world, both about economics, and about how to construct a vibrant multi-racial, multi-ethnic, multi-cultural society.
The numbers themselves say a lot.
At independence, Malaysia was one of the poorest countries in the world.
Though reliable data are hard to come by, its GDP (in purchasing power parity terms) was comparable to that of Haiti, Honduras, and Egypt, and some 5% below that of Ghana.
Today, Malaysia’s income is 7.8 times that of Ghana, more than five times that of Honduras, and more than 2.5 times that of Egypt.
In the global growth league tables, Malaysia is in the top tier, along with China, Taiwan, South Korea, and Thailand.
Moreover, the benefits of the growth have been shared.
Hard-core poverty is set to be eliminated by 2010, with the overall poverty rate falling to 2.8%.
Malaysia has succeeded in markedly reducing the income divides that separated various ethnic groups, not by bringing the top down, but by bringing the bottom up.
Part of the country’s success in reducing poverty reflects strong job creation.
While unemployment is a problem in most of the world, Malaysia has been importing labor.
In the 50 years since independence, 7.24 million jobs have been created, an increase of 261%, which would be equivalent to the creation of 105 million jobs in the United States.
There were many reasons not to have expected Malaysia to be a success.
Just as Malaysia was gaining its independence, the Nobel Prize winning economist Gunnar Myrdal wrote an influential book called Asian Drama , in which he predicted a bleak future for the region.
Malaysia is rich in natural resources.
But, with few exceptions, such countries are afflicted with the so-called “natural resource curse”: countries with an abundance of resources not only do not do as well as expected, but actually do worse than countries without such benefits.
While natural resource wealth should make it easier to create a more equalitarian society, countries with more resources, on average, are marked by greater inequality.
Moreover, Malaysia’s multiracial, multi-cultural society made it more vulnerable to civil strife, which has occurred in many other resource-rich countries, as one group tried to seize the wealth for itself.
In many cases, minorities work hard to garner the fruits of this wealth for themselves, at the expense of the majority – Bolivia, one of the many rich countries with poor people, comes to mind.
At independence, Malaysia also faced a Communist insurgency.
The “hearts and minds” of those in the countryside had to be won, and that meant bringing economic benefits and minimizing “collateral” damage to innocent civilians – an important lesson for the Bush administration in Iraq, if it would only listen to someone outside its closed circle.
And Malaysia had a third strike against it: for all the talk of the “white man’s burden,” the European powers did little to improve living standards in the countries they ruled.
The dramatic decline in India’s share of global GDP under Britain’s rule, as Britain passed trade laws designed to benefit its textile producers at the expense of those in its colony, is the most visible example.
The colonial powers’ divide-and-rule tactics enabled small populations in Europe to rule large numbers outside of Europe, pillaging natural resources while investing little in the physical, human capital, and social capital necessary for an economically successful, democratic self-governing society.
It has taken many of the former colonies decades to overcome this legacy.
How, then, does an economist account for Malaysia’s success?
Economically, Malaysia learned from its neighbors.
Too many of the ex-colonies, rejecting their colonial heritage, turned to Russia and communism.
Malaysia wisely took an alternative course, looking instead to the highly successful countries of East Asia.
It invested in education and technology, pushed a high savings rate, enacted a strong and effective affirmative action program, and adopted sound macroeconomic policies.
Malaysia also recognized that success required an active role for government.
It eschewed ideology, following or rejecting outsiders’ advice on a pragmatic basis.
Most tellingly, during the financial crisis of 1997, it did not adopt IMF policies – and as a result had the shortest and shallowest downturn of any of the afflicted countries.
When it re-emerged, it was not burdened with debt and bankrupt firms like so many of its neighbors.
This success was, of course, not only a matter of economics: had Malaysia followed the policies recommended by the IMF, it would have torn apart the social fabric created over the preceding four decades.
Malaysia’s success thus should be studied both by those looking for economic prosperity and those seeking to understand how our world can live together, not just with toleration, but also with respect, sharing their common humanity and working together to achieve common goals.
Health care now accounts for about one dollar in every six of all US spending – private as well as public – and is on track to double by 2035.
That is a greater share than in anywhere else in the world, but rising health-care costs are also a problem in countries that spend far less.
There are many places where savings can be made. Encouraging people to exercise, to avoid smoking, to use alcohol only in moderation, and to eat less red meat would help to reduce health-care costs.
But, given developed countries’ aging populations, the cost of caring for the elderly is bound to rise. So we will have to find other ways to save money.
Here it makes sense to start at the end.
Treating dying patients who do not want to go on living is a waste, yet only a few countries allow physicians actively to assist a patient who requests aid in dying. In the US, about 27% of Medicare’s budget goes towards care in the last year of life.
While some of that is spent in the hope that the patient will have many years to live, it is not unusual for hospitals to provide treatments costing tens of thousands of dollars to patients who have no hope of living more than a week or two – and often under sedation or barely conscious.
One factor in such decisions is fear on the part of doctors or hospitals that they will be sued by the family for letting their loved one die.
So, for example, patients close to death are resuscitated, against the doctor’s better judgment, because they have not specifically stated that they do not want to be resuscitated in such circumstances.
The system by which doctors and hospitals are paid is another factor in providing expensive treatment that is of little benefit to the patient. When Intermountain Healthcare, a network of hospitals in Utah and Idaho, improved its treatment for premature babies, it reduced the time they spent in intensive care, thereby slashing the costs of treating them.
But, because hospitals are paid a fee for each service they provide, and better care meant that the babies needed fewer services, the change cost the hospital network $329,000 a year.
Even if such perverse incentives are removed, tougher questions about controlling costs need to be faced. One is the cost of new drugs.
Development costs of $800 million are not unusual for a drug, and we can expect to see more of a new type of drug – biopharmaceuticals made from living cells – which cost even more.
Development costs have to be passed on in drug prices, which may be exceptionally high when a drug benefits only a relatively small number of patients.
Gaucher’s disease, for example, is a rare crippling genetic condition that, in its more severe forms, usually killed its victims in childhood. Now those with the disease can lead an almost normal life, thanks to a drug called Cerezyme – but it costs $175,000 a year. 
New medical devices pose equally difficult dilemmas.
The artificial heart machine, also known as a left ventricular assist device, or LVAD, has been used to keep patients alive until they receive a heart transplant.
But there is a shortage of hearts for transplantation, and in the US, LVADs are now being implanted as a long-term treatment for heart failure, just as a dialysis machine replaces a kidney. 
According to Manoj Jain of Emory University, every year 200,000 US patients could be kept alive a little longer with an LVAD, at a cost of $200,000 per patient, or $40 billion.
Is that a sensible use of resources in a country in which there are officially 39 million people below the poverty line, which for a family of four is $22,000?
In countries that provide free health care to their citizens, it is extraordinarily difficult for officials to tell anyone that the government will not pay for the only drug or medical device that can save their life – or their child’s life.
But eventually the point will come when such things must be said.
No one likes putting a dollar value on a human life, but the fact is that we already do, implicitly, by failing to give enough support to organizations working in developing countries.
GiveWell.net, which evaluates organizations working to save the lives of the world’s poor, has identified several that can save a life for under $1,000.
The World Health Organization estimates that its immunization programs in developing countries cost about $300 per life saved – lives that are saved not for a year, but usually for a lifetime.
The United States seems as capable of barbarism as anyone else, as the abuses at the Abu Ghraib prison make clear.
Much of the time the barbarism in Iraq goes unrecorded, as when American tanks sweep into Iraqi neighborhoods and kill dozens of innocents in the name of fighting "insurgents."
But barbarism is found in many quarters, as the grisly beheading of an American hostage made clear.
Every society, under certain conditions, is vulnerable to a descent into barbarism.
Many historians have argued that German society under Hitler was somehow uniquely evil.
False.
Germany was destabilized by defeat in World War I, a harsh peace in 1919, hyperinflation in the 1920's, and the Great Depression of the 1930's, but was otherwise not uniquely barbarous.
On the contrary, in the early part of the twentieth century, Germany was one of the world's richest countries, with enviably high education levels and scientific prowess.
Hannah Arendt was closer to the mark when she wrote about the "banality of evil," not its uniqueness.
There seem to be two common characteristics of the descent into barbarism.
The first is the relentless human tendency to classify the world as "us" versus "them," and then to reduce "them" to sub-human status.
Such classifications probably evolved because they strengthened the cohesion of the "in" group, facilitating cooperation by harnessing hatred for those outside.
Hatred and violence against "others" seem to be manifested most powerfully as the result of fear: they are survival reactions.
The descent into barbarism generally occurs in the midst of economic crisis or when localized violence has already flared.
Fear leads one group to coalesce in order to protect itself, perhaps by attacking a competing group.
This pattern was evident in Yugoslavia's wars of the 1990's, where ethnic communities that had lived together more or less peacefully became enmeshed in civil war in the midst of a deep economic crisis.
Similarly, Israelis and Palestinians have both engaged in barbaric acts in a tragic interplay of mutual fear that empowers extremists in both communities.
There is no glory on either side of the Israel-Palestine debacle.
But Israel, with its vastly stronger army, could and should have taken much wiser political decisions than occupying the West Bank with settlers.
American reactions to the Abu Ghraib torture scenes, followed by the beheading of the American hostage Nicholas Berg, show clearly the route to barbarism in a supposedly civilized country.
In May 2004, The New York Times polled readers in a city in the US heartland, Oswego, Illinois.
One retired businessman said, "Let's kill them all. Let's wipe them off the face of the earth."
A Nazi leader would not have said it differently.
A tow truck driver said that the beheading "just affirms what I thought before.
We're not being tough enough. This is something that we've got to do.
We should attack our enemies directly and not back off till we succeed."
A third respondent said something similar.
They're the ones who are subhuman.
They're the ones who are human debris, not the United States of America and not our soldiers and our prison guards."
I am not saying that the US is more depraved than other countries.
What I am saying is that human society, even in the twenty-first century, is capable of sliding into barbaric thinking and action, no matter the level of "development."
The idea that any nation is morally superior, or that it has been divinely chosen as a leader of nations, is dangerous.
Once we recognize how vulnerable all of the world is to this kind of descent into violence, the importance of international law and international institutions such as the United Nations become all the more obvious.
The UN successfully resisted the powerful pressures of the US to condone a war with Iraq despite repeated US claims, now known to be false, that Iraq was in possession of weapons of mass destruction.
The UN process worked. It was US policy that failed.
The events at Abu Ghraib underscore why international rules such as the Geneva Conventions on the treatment of combatants are so vital.
By putting itself above the law, America allowed itself to succumb to barbaric behavior.
Similarly, these events prove why the new International Criminal Court (ICC) is vital.
The US strongly resisted the jurisdiction of the ICC, but American abuses at Abu Ghraib show why the US, should be subject to international law.
Perhaps that lesson - the need to subject even the most powerful country to international law - will be one benefit of the otherwise disastrous war that the US launched in Iraq.
If this lesson is learned, the world will be far safer.
America itself will be safer, in part because it will be less likely in the future to unleash a spiral of violence fueled by its own irrational fears and misunderstandings of the world.
The Mauritius Miracle
NEW YORK – Suppose someone were to describe a small country that provided free education through university for all of its citizens, transportation for school children, and free health care – including heart surgery – for all.
You might suspect that such a country is either phenomenally rich or on the fast track to fiscal crisis.
After all, rich countries in Europe have increasingly found that they cannot pay for university education, and are asking young people and their families to bear the costs.
Now comes the painful number: Mauritius’s GDP has grown faster than 5% annually for almost 30 years.
Surely, this must be some “trick.” Mauritius must be rich in diamonds, oil, or some other valuable commodity.
But Mauritius has no exploitable natural resources.
Indeed, so dismal were its prospects as it approached independence from Britain, which came in 1968, that the Nobel Prize-winning economist James Meade wrote in 1961: “It is going to be a great achievement if [the country] can find productive employment for its population without a serious reduction in the existing standard of living….[T]he outlook for peaceful development is weak.”
As if to prove Meade wrong, the Mauritians have increased per capita income from less than $400 around the time of independence to more than $6,700 today.
The country has progressed from the sugar-based monoculture of 50 years ago to a diversified economy that includes tourism, finance, textiles, and, if current plans bear fruit, advanced technology.
During my visit, my interest was to understand better what had led to what some have called the Mauritius Miracle, and what others might learn from it.
There are, in fact, many lessons, some of which should be borne in mind by politicians in the US and elsewhere as they fight their budget battles.
First, the question is not whether we can afford to provide health care or education for all, or ensure widespread homeownership.
If Mauritius can afford these things, America and Europe – which are several orders of magnitude richer – can, too.
The question, rather, is how to organize society.
Mauritians have chosen a path that leads to higher levels of social cohesion, welfare, and economic growth – and to a lower level of inequality.
Second, unlike many other small countries, Mauritius has decided that most military spending is a waste.
The US need not go as far: just a fraction of the money that America spends on weapons that don’t work against enemies that don’t exist would go a long way toward creating a more humane society, including provision of health care and education to those who cannot afford them.
Third, Mauritius recognized that without natural resources, its people were its only asset. Maybe that appreciation for its human resources is also what led Mauritius to realize that, particularly given the country’s potential religious, ethnic, and political differences – which some tried to exploit in order to induce it to remain a British colony – education for all was crucial to social unity.
So was a strong commitment to democratic institutions and cooperation between workers, government, and employers – precisely the opposite of the kind of dissension and division being engendered by conservatives in the US today.
This is not to say that Mauritius is without problems.
Like many other successful emerging-market countries, Mauritius is confronting a loss of exchange-rate competitiveness.
And, as more and more countries intervene to weaken their exchange rates in response to America’s attempt at competitive devaluation through quantitative easing, the problem is becoming worse.
Almost surely, Mauritius, too, will have to intervene.
Moreover, like many other countries around the world, Mauritius worries today about imported food and energy inflation.
To respond to inflation by increasing interest rates would simply compound the difficulties of high prices with high unemployment and an even less competitive exchange rate.
Direct interventions, restrictions on short-term capital inflows, capital-gains taxes, and stabilizing prudential banking regulations will all have to be considered.
The Mauritius Miracle dates to independence.
But the country still struggles with some of its colonial legacies: inequality in land and wealth, as well as vulnerability to high-stakes global politics.
The US occupies one of Mauritius’s offshore islands, Diego Garcia, as a naval base without compensation, officially leasing it from the United Kingdom, which not only retained the Chagos Islands in violation of the UN and international law, but expelled its citizens and refuses to allow them to return.
The US should now do right by this peaceful and democratic country: recognize Mauritius’ rightful ownership of Diego Garcia, renegotiate the lease, and redeem past sins by paying a fair amount for land that it has illegally occupied for decades.
The Media Cold War
PRINCETON – An information war has erupted around the world.
The battle lines are drawn between those governments that regard the free flow of information, and the ability to access it, as a matter of fundamental human rights, and those that regard official control of information as a fundamental sovereign prerogative.
The contest is being waged institutionally in organizations like the International Telecommunications Union (ITU) and daily in countries like Syria.
The sociologist Philip N. Howard recently used the term “new cold war” to describe “battles between broadcast media outlets and social-media upstarts, which have very different approaches to news production, ownership, and censorship.”
Because broadcasting requires significant funding, it is more centralized – and thus much more susceptible to state control.
Social media, by contrast, transforms anyone with a mobile phone into a potential roving monitor of government deeds or misdeeds, and are hard to shut down without shutting down the entire Internet.
Surveying struggles between broadcast and social media in Russia, Syria, and Saudi Arabia, Howard concludes that, notwithstanding their different media cultures, all three governments strongly back state-controlled broadcasting.
These intra-media struggles are interesting and important.
The way that information circulates does reflect, as Howard argues, a conception of how a society/polity should be organized.
But an even deeper difference concerns the fundamental issue of who owns information in the first place.
In January 2010, US Secretary of State Hillary Clinton proclaimed that the United States “stand[s] for a single Internet where all of humanity has equal access to information and ideas.”
She linked that stance not only to the US Constitution’s First Amendment, which protects freedom of expression and freedom of the press, but also to the Universal Declaration of Human Rights, which holds that all people have the right “to seek, receive, and impart information and ideas through any media and regardless of frontiers.”
Many governments’ determination to “erect electronic barriers” to block their citizens’ efforts to access the full resources of the Internet, she said, means that “a new information curtain is descending across our world.”
This larger struggle is playing out in many places, including the ITU, which will convene 190 countries in Dubai in December to update an international telecommunications treaty first adopted in 1988.
Although many of the treaty’s details are highly technical, involving the routing of telecommunications, various governments have submitted proposals to amend the treaty that include provisions aimed at facilitating government censorship of the Internet.
Russian President Vladimir Putin has been open about his desire to use the ITU “to establish international control” over the Internet, thereby superseding current arrangements, which leave Internet governance in the hands of private groups like the Internet Corporation for Assigned Names and Numbers and the Internet Engineering Task Force.
The US would never sign a treaty that fundamentally changed Internet governance arrangements, but the point is that many governments will try to use the treaty process to increase their ability to control the information that their citizens can access.
On the ground, governments are often still primarily focused on blocking information about what they are doing.
One of the Syrian government’s first moves after it began shooting protesters, for example, was to expel all foreign journalists.
Several weeks ago, the government of Tajikistan blocked YouTube and reportedly shut down communications networks in a remote region where government forces were battling an opposition group.
The Chinese government barred all foreign journalists from Tibet when it cracked down hard on protesters before the 2008 Olympics.
These more traditional tactics can now be supplemented with new tools for misinformation.
For close followers of the Syrian conflict, tracking key reporters and opposition representatives on Twitter can be a surreal experience.
Two weeks ago, Ausama Monajed, a Syrian strategic communications consultant who sends out a steady stream of information and links to opposition activities in Syria, suddenly started sending out pro-government propaganda.
The Saudi-owned satellite news channel Al Arabiya has also reported the hacking of its Twitter feed by the “Electronic Syrian Army,” a shadowy group most likely comprised of free-lance operatives with the direct or indirect support of the Syrian government.
It is one thing to read about sophisticated cyber-war capabilities; it is quite another to see the online identities of familiar people or Web sites suddenly hijacked.
In the many manifestations of the ongoing and growing information war(s), the pro-freedom-of-information forces need a new weapon.
A government’s banning of journalists or blocking of news and social-media Web sites that were previously allowed should itself be regarded as an early warning sign of a crisis meriting international scrutiny.
The presumption should be that governments with nothing to hide have nothing to lose by allowing their citizens and internationally recognized media to report on their actions.
To give this presumption teeth, it should be included in international trade and investment agreements.
Imagine if the International Monetary Fund, the World Bank, and regional development banks suspended financing as soon as a government pulled down an information curtain.
Suppose foreign investors wrote contracts providing that the expulsion and banning of foreign journalists or widespread blocking of access to international news sources and social media constituted a sign of political risk sufficient to suspend investor obligations under the contract.
Americans say that sunlight is the best disinfectant.
Citizens’ access to information is an essential tool to hold governments accountable.
Government efforts to manipulate or block information should be presumed to be an abuse of power – one intended to mask many other abuses.
The Media Versus the Mentally Ill
MELBOURNE – James Holmes, accused of opening fire in a crowded movie theater in Aurora, Colorado, last summer, had no criminal history, but was seeing a psychiatrist prior to the incident.
Adam Lanza, suspected of murdering his mother and gunning down 20 children and six adult staff members at a Connecticut elementary school before taking his own life, had never been in legal trouble, but had been diagnosed with a “personality disorder,” as well as the developmental disorder Asperger syndrome.
Anders Behring Breivik in Norway, Jared Lee Loughner in Arizona, Seung-Hui Cho in Virginia – the list of mass murderers defined according to their mental illnesses goes on.
The fact is that deciding to murder, at random, a large number of innocent people reflects deeply disturbed thinking, which might reflect a mental illness.
But, contrary to popular belief, this does not mean that people with mental illnesses are likely to be dangerous or violent.
The belief that they are – and the reporting that feeds it – is reinforcing widespread stigmatization of those with mental illness, increasing their suffering and preventing them from participating fully in society.
Public perceptions of the risk of violence associated with mental illness are at odds with the facts.
In the United States, for example, roughly 42% of adults believe that a depressed child is likely to be dangerous.
And 70% of Americans believe that patients hospitalized for a mental illness may be dangerous.
But, according to the American Psychiatric Association, people with mental disorders, who account for roughly one-quarter of the population in a given year, commit only 4-5% of violent crimes.
Indeed, while mentally ill people may be more likely to commit violent acts if they are not treated, or are misusing alcohol or drugs, the risk is small.
Personal experience usually refutes the link between mental illness and violence.
A survey of the American public found that, while 68% of adults knew at least one person who had been hospitalized with a mental illness and 10% knew five or more people, only 9% had ever been threatened or physically harmed by such a person.
People in close contact with the mentally ill, such as mental-health professionals and family members of people affected, are the least likely to believe that they are dangerous.
The discrepancy between experience and perception is largely caused by the media, which frequently link mental illness with acts of violence.
One study of American newspaper reports found that 39% of reports concerning mental illness were focused on violence or danger.
In Germany, extensive reports on violent attacks against prominent politicians by two people suffering from schizophrenia in 1990 bolstered the German public’s belief that mentally ill people are dangerous.
Coverage of violent acts that result in multiple deaths is particularly extensive.
The public inevitably seeks some explanation, and the media will explore any potential link to mental illness to provide one – for example, acquaintances’ reports of “odd” behavior and social withdrawal or accounts of previous interactions with mental-health professionals.
Meanwhile, defense attorneys may try to lessen their client’s culpability by claiming insanity, as Breivik’s lawyers attempted to do after he killed 77 people to protest the multiculturalism that he claims threatens Norway.
Although this approach is rarely successful – Breivik received a 21-year sentence – it is widely reported, linking crime to mental illness in the minds of the public.
The perception that mentally ill people are dangerous is global. But it is more prevalent in developing countries than in the developed world.
The major exception is the US, where the ready availability of firearms contributes to one of the highest homicide rates – and the highest gun homicide rate – among developed countries.
Given that multiple homicides in the US draw international interest, media reports highlighting a perpetrator’s mental illness or describing insanity pleas inform perceptions of mental illness worldwide.
Loughner’s shooting of 19 people in 2011, including Congresswoman Gabrielle Giffords, attracted global attention, as did the court-ordered evaluation of his mental competency.
Indeed, on the other side of the world, the newspaper The Australian mentioned Giffords 160 times in the six months following the shooting, compared to just one mention in the previous 12 months.
Though mass shootings are extremely rare even in the US, the media’s coverage continually reinforces negative perceptions at home and abroad.
In this manner, the US may well be exporting the mental-illness stigma to the rest of the world.
Similarly, Cho’s history of mental illness was widely discussed.
Moreover, his mental-health records were released to the public two years later, reviving the link between his crime – killing 32 people and wounding 17 others before committing suicide – and his mental illness.
In this context, US President Barack Obama’s gun-control efforts – which include intensifying background-check requirements and increasing financing for mental-health programs for young people – hold global significance.
While expanding the scope of mental-health services is a positive step, with the potential to contribute to the physical safety of US citizens, it is crucial that this link not be allowed to feed the perception that people with mental illnesses are dangerous.
Rather, American legislators and media outlets must use their international influence to reduce the stigma experienced by people suffering from mental illnesses worldwide.
The Media War on Terror
“More than half of this battle is taking place on the battlefield of the media, [for] we are in a media battle in a race for the hearts and minds of [Muslims].”
The speaker was not some public relations executive, but Osama bin Laden’s chief lieutenant, Ayman al-Zawahiri.
Terrorists have skillfully adapted to fighting wars in today’s media age, but, for the most part, America and the governments of the other democracies have not.
Consider that the violent extremists have their own “media relations committees” aimed at manipulating elite opinion.
They plan and design headline-grabbing attacks using every means of communications to intimidate and break the collective will of free people.
They know that communications transcend borders, and that a single news story, handled skillfully, can be as damaging to our cause – and as helpful to theirs – as any military attack.
And they are able to act quickly with relatively few people, and with modest resources compared to the vast, expensive bureaucracies of democratic governments.
Today we are fighting the first war in the era of e-mail, blogs, blackberries, instant messaging, digital cameras, the Internet, mobile phones, talk radio, and 24-hour news.
In Tunisia, the largest newspaper has a circulation of roughly 50,000 in a country of 10 million people.
But even in the poorest neighborhoods, you see satellite dishes on nearly every balcony or rooftop.
A few years ago, under Saddam Hussein, an Iraqi could have his tongue cut out if he was found in possession of a satellite dish or used the Internet without government approval.
Today, satellite dishes are ubiquitous in Iraq as well.
Regrettably, many of the news channels being watched through these dishes are hostile to the West.
Media outlets in many parts of the world often serve only to inflame and distort – rather than to explain and inform.
While al-Qaeda and extremist movements have used this forum for many years, further poisoning the Muslim public’s view of the West, we in the West have barely even begun to compete.
We saw this with the false allegations of the desecration of a Koran last year.
First published in a weekly news magazine, the story was then posted on Web sites, sent in e-mails, and repeated on satellite television and radio stations for days before the facts could be discovered.
That false story incited deadly anti-American riots in Afghanistan and Pakistan.
The United States military, appropriately and of necessity, took the time needed to ensure that it had the facts before responding that the charges were untrue.
In the meantime, innocent lives were lost.
But we have begun to adapt.
In Iraq, for example, the US military, working closely with the Iraqi government, has sought non-traditional means to provide accurate information to the Iraqi people.
Yet this has been portrayed as “buying news.”
The resulting explosion of critical press stories then causes everything – all activity, all initiative – to stop.
This leads to a “chilling effect” among those serving in the military public affairs field, who conclude that there is no tolerance for innovation.
Consider for a moment the vast quantity of column inches and hours of television devoted to the allegations of detainee abuse at Abu Ghraib.
Compare that to the volume of coverage and condemnation associated with, say, the discovery of Saddam Hussein’s mass graves, which were filled with hundreds of thousands of innocent Iraqis.
Free governments must make communications planning a central component of every aspect of this struggle.
Indeed, the longer it takes to put a strategic communications framework into place, the more the vacuum will be filled by the enemy.
There are nonetheless signs of modest progress.
Soon after the devastating earthquake in Pakistan, a public affairs team was deployed with our sizable military forces in the disaster area.
They worked to help focus media attention on America’s commitment to help the Pakistani people.
Public opinion surveys conducted by private groups before and after the earthquake suggest that attitudes in Pakistan regarding the US changed dramatically because of this new awareness.
Government public affairs and public diplomacy efforts are slowly beginning to reorient staffing, schedules, and bureaucratic culture to engage the full range of today’s media.
Still, government must develop the institutional capability to anticipate and act within the same news cycle.
That requires instituting 24-hour press operations centers and elevating Internet operations and other channels to the status of traditional twentieth-century press relations.
It will require less reliance on the traditional print media, just as the publics of the US and the world are relying less on newspapers.
This also will mean embracing new ways of engaging people throughout the world.
During the Cold War, institutions such as Radio Free Europe proved to be valuable instruments.
We need to consider the possibility of new organizations and programs that can serve a similarly valuable role in the War on Terror.
We are fighting a war in which the survival of our way of life is at stake.
And the center of gravity of that struggle is not just the battlefield.
It is a test of wills and it will be won or lost in the court of global public opinion.
While the enemy is skillful at manipulating the media and using the tools of communications to their advantage, we have an advantage as well: truth is on our side, and, ultimately, truth wins out.
The Mediterranean Crucible
BERLIN – For most Europeans, the Mediterranean is an annual object of longing – the holiday idyll where they spend the best weeks of the year.
But many Europeans’ sunny view of the region has yielded to lowering clouds of pessimism.
Inside the European Union, the ugly term PIGS (Portugal, Italy/Ireland, Greece, Spain) is now a commonplace, denoting countries that have endangered the euro’s stability and are forcing northern Europeans into costly bailouts.
Where not long ago sunshine and solidarity were the order of the day, depression and confrontation are now the rule.
Worse still, Europe’s debt and confidence crisis is also the EU’s gravest political crisis since its inception: at stake is nothing less then the future of the European project itself.
And now the crisis has reached the southern shore of the Mediterranean, in the form of a revolution in Tunisia and a political showdown in Lebanon that has once again brought the country to the verge of war and disaster.
With the EU’s Mediterranean member states simultaneously faltering, great changes are afoot in Europe’s southern neighborhood.
So it is time to think geopolitically, not just fiscally, about the Mediterranean.
What the EU is facing in the Mediterranean isn’t primarily a currency problem; first and foremost, it is a strategic problem – one that requires solutions urgently.
The overthrow of Tunisia’s Zine El Abidine Ben Ali is the first such spontaneous democratic uprising in the Arabic world, showing that in an age of satellite television and the Internet, suppression of information and free expression by individual governments doesn’t really work anymore.
Add to this the fact that the Arab world’s nationalist regimes, which have calcified into militarized dictatorships, lost their popular legitimacy long ago.
The “Chinese option” – economic rights and prosperity in exchange for public quiescence – is not feasible, owing to these regimes’ ineptness and rampant corruption.
As a result, their inability to reform, combined with rapid population growth and a rising cohort of young people, is placing them under pressure, creating a possibility of explosive change.
Whether a development will occur in the Arab world similar to that in Eastern Europe after the fall of the Iron Curtain is impossible to predict for the time being.
In Eastern Europe, with the withdrawal and eventual disappearance of the Soviet hegemon, the floodgates were opened, and a torrent of change washed over the region.
In the Middle East and the Maghreb, this external factor is missing; democratic change must come from within each society.
Tunisia shows that no government that has lost its legitimacy and is supported only by bayonets is sustainable in the long term.
Whether Tunisia becomes a democratic, economic, and social success story or, instead, sees its revolution end in chaos, civil war, and a new authoritarian regime will have consequences far beyond the country’s borders.
Europe, as Tunisia’s northern neighbor, will be directly affected either way, and should therefore become seriously involved in terms of promoting democracy and aiding economic progress.
Whatever mistakes Europe may have made vis-à-vis the region’s authoritarian regimes in the past, it can now correct them by providing decisive help.
Indeed, Tunisia is a stern test of the EU’s “Mediterranean partnership” – a long-promoted policy that so far consists of little more than empty phrases.
Tunisia’s revolution is a unique historical opportunity, and the EU’s stake in the outcome can hardly be overestimated.
European officials in Brussels and the major EU governments should not go for political and economic half-measures.
Specifically, beyond direct help for Tunisia at this critical moment, the EU must breathe new life into the Mediterranean partnership.
Projects for strategic cooperation in energy – say, production of solar and wind energy in the Sahara for sale to Europe – would be especially valuable.
The EU and its member states – particularly Spain, France, Germany, and Italy – should make this new form of energy production and cooperation the key project of the Mediterranean partnership, and must ensure the necessary political conditions to accomplish it rapidly.
This would create new prospects for the countries in the EU’s southern neighborhood, and thus give the transformation process an economic and technological impetus.
Moreover, such projects would promote cooperation between the states in the EU’s southern neighborhood, potentially boosting investments in education, infrastructure, and industrial development.
This would create the most important thing that these states and their rapidly growing young populations need to produce stability within the framework of democratic development: grounds for hope of economic and social progress.
If the Europeans continue to look inward, allowing accountants to dominate discussions of Europe’s future, they will miss a historic opportunity – one that will directly affect Europe’s security.
In that case, the costs tomorrow would be far higher than the savings today.
The Mexican Muddle
MEXICO CITY – This month, Mexico’s Felipe Calderón celebrates his second anniversary as president.
Calderón took office in December 2006 under adverse circumstances.
Elected with 35% of the vote, he lacked a majority in Congress, and the opposition refused to acknowledge his victory.
He has also had to govern in a persistently difficult environment: a lame-duck president next door in the United States, a severe economic downturn, and the legacy of corruption, negligence, and complicity handed down by his predecessors since 1968, when Mexico’s old one-party political system began to crumble.
Most immediately, Calderón had to address his immediate predecessor’s failure to implement any of the major reforms that Mexico needed.
Vicente Fox took office in 2000 with a broad mandate, but, like Calderón, without a majority in Congress.
He proved unable to forge lasting legislative coalitions, leading Calderón to decide that his first break with the past should consist in building alliances to enact reform.
Soon, however, this became an end in itself, and Calderón proved adept at constructing short-lived coalitions for largely inconsequential reforms.
This watered-down gradualism has become his trademark.
Given this, it is no surprise that the list of question marks attached to Calderón’s administration is longer than that of his achievements.
His poll numbers reflect this ambivalence.
He remains personally popular and well regarded, but the public is increasingly dissatisfied and disappointed with his government’s actual performance.
Among his few successes are state-employee pension fund reforms, which will save the system from bankruptcy.
Another lies in having weakened and neutralized the left-wing opposition, and being able to govern without major confrontations in the streets or Congress.
Other changes – a counter-productive electoral reform, a minor revenue-enhancing tax reform, and hypothetical oil-sector reform – all suffer from Calderón’s penchant for minimalism: enacting legislation seems more important to him than the content.
Indeed, Mexico’s monopoly structures and immense concentration of power remain untouched – as they were under Calderón’s only two democratic predecessors, Ernesto Zedillo and Fox.
Public monopolies like Pemex, private monopolies like Telmex and Cemex, labor monopolies like the teachers’ union, media monopolies like Televisa, and the iron political lock the country’s three parties have on every level of electoral representation, are all stronger than before.
Calderón has refused to touch these power structures, even though everyone, from the World Bank to left-wing rabble-rouser and former presidential candidate Andrés Manuel López Obrador, agree that they are the main impediments to Mexico’s progress.
Because of this enduring status quo, Mexico’s people do not blame Calderón for today’s harsh economic downturn; it is business as usual, they think.
But public opinion is becoming more skeptical about the wisdom of the president’s signature policy: using the armed forces to launch an all-out drive against drug dealers.
Ultimately, this is what he will be judged by.
Mexico’s government and military had established a tacit, semi-violent, corrupt, but effective modus vivendi with the drug cartels since the early 1970’s.
Mexico was then a producer of heroin and marijuana, and a transit country for cocaine from South America.
Calderón decided, in late 2006, that accommodation was no longer tolerable: the violence had gotten out of control; the corruption and complicity had infected the nation’s police and political elite; and Mexico had become a major producer of methamphetamines for the US, as well as a significant consumer of cocaine.
Thus, his declaration of war.
But the new president kicked over the narcotics beehive with neither a fumigator nor protective netting on hand.
So he couldn’t defend himself.
The bees have overwhelmed him, and violence, corruption, complicity, and contamination of the state have all skyrocketed.
The reasons for Calderón’s rejection of the former arrangements now appear less self-evident than before.
It is unclear that drug use in Mexico is extending more widely into the population: alarming rates of addiction growth are misleading, because addiction starts from an extremely low base.
Calderón had tried to sell his “war on drugs” by insisting that he is waging it to “save our children.”
If that connection proves faulty, it will not work, and Mexicans may question the point of a “war” that has more than doubled the number of gangland executions, from 2,000 in 2006 to 5,000 in 2008, when their children, statistically at least, are not in harm’s way.
Moreover, the capacity of Mexico’s security machinery appears diminished.
The police are at best useless and at worst outright employees of the drug lords; the army is less tainted, but unprepared for the role.
It will take years and billions of dollars to change this state of affairs; Calderón does not have the time, and only the US has the money.
But Calderón doesn’t want American aid on American terms.
He rejects the Colombian model, which would put American advisers, instructors, mechanics, agents, and maintenance personnel on the ground in Mexico.
He may be right, but the alternative is no military makeover, and no likelihood of winning a war that perhaps should not have been declared in the first place.
But we realistic optimists are no longer holding our breath: we may be disappointed, once again.
The Microfinance Catalyst
CAMBRIDGE – So-called “impact investors” – providers of capital to businesses that solve social challenges while generating a profit – are the current rage in economic development.
US President Barack Obama’s Office for Social Innovation and Civic Participation recently convened more than 100 practitioners to discuss how impact investing could be unleashed in the United States and the developing world.
The United Nations Foundation and the US State Department have launched a $50 million public-private partnership to promote clean cooking stoves in poor countries.
In the United Kingdom, the Netherlands, and France, development agencies are looking to reposition some of their funding to businesses serving the poor.
According to the World Bank, roughly 1.4 billion people live in extreme poverty (earning less than $1.25/day), and 2.6 billion in moderate poverty (less than $2/day).
More than a billion of the moderately poor – a number that exceeds Africa’s total population – live in South Asia.
Can impact investing do more to reduce global poverty than so many previous efforts, all of which have struggled to have any impact at all?
Impoverished populations desperately require lighting, fuel for cooking, affordable and accessible health care, clean water, elementary education, and financial services.
Government programs to supply these needs are plagued with corruption (by some estimates 50-70% of all welfare spending in India is stolen) and unable to provide quality services.
Moreover, large companies have been unable to serve these populations’ needs, because to do so would require them to reinvent their existing business models around new product, distribution, and pricing paradigms.
This type of disruptive innovation usually comes from entrepreneurs.
But entrepreneurs face daunting barriers, such as inadequate logistics, lack of consumer financing, poorly trained workers, consumer distrust of new technologies, high-cost marketing channels, backlash from existing merchants or moneylenders, and under-developed regulation.
Surmounting these business challenges is an expensive and slow process, and new ventures require many years to become cash-positive.
As a result, commercial providers of debt or equity cannot get the high returns and quick exits that they seek.
When both governments and markets fail, impact investors can stimulate change.
Microfinance – its advent, rise, and recent crises – shows how.
The microfinance industry began in the 1980’s in Bangladesh with the non-profit Grameen Bank and BRAC Bank.
Donors soon helped launch other microfinance institutions in Mexico, India, Peru, Indonesia, and many African countries, where they could offer loans at 25-30% interest rates – well below traditional moneylender rates of 60-100% – and still generate strong profit margins.
Today, the microfinance industry serves about 150-200 million borrowers around the world, and has grown rapidly by securing access to several billion dollars in equity financing.
Banco Compartamos in Mexico and SKS Microfinance in India illustrate impact investors’ catalytic role.
Both started as NGOs (following the Grameen model) and received millions in grant money from development institutions to start lending operations.
They also got access to low-cost lending from government banks and multilateral institutions such as the International Finance Corporation (the World Bank’s commercial-lending arm) and the US Agency for International Development.
Within a few years, as their loan books began to grow rapidly, Compartamos and SKS created for-profit businesses that were owned by their respective NGOs.
Subsequently, they raised equity capital from investors seeking to make a social impact – Compartamos from Accion and the IFC, and SKS from Unitus, Silicon Valley venture capitalist Vinod Khosla, and an Indian government development agency.
An initial public offering in 2007 followed for Compartamos, valuing the company at $2.2 billion.
SKS raised more equity from investors such as Sequoia and Odyssey Capital, before going public on the Indian stock exchange in 2010, raising about $358 million at a valuation exceeding $1.6 billion.
Impact investors’ support eventually led to flows of commercial capital – both VC funding and IPO investors – into Compartamos, SKS, and many other microfinance institutions.
Despite these successes, however, microfinance has struggled recently in India.
Credit histories cannot be shared, because a credit bureau is just getting started.
Moreover, an appropriate consumer-protection code and a nationwide regulatory framework are still lacking.
Not surprisingly, some have sought to exploit the poor as a result, which has put pressure on the authorities to “do something.”
Unfortunately, this often results in inadvertent harm.
For example, the Andhra Pradesh state government passed a restrictive ordinance making it difficult for microfinance institutions to recover their loans from their clients.
As a result, many institutions had to write off much of their loan portfolios and take heavy losses, sending shock waves through the industry and the investor community – and causing the poor to suffer.
The lesson is that markets simply cannot work without accompanying public goods and high-quality government supervision.
Although impact investors can lay the groundwork for commercial investors, they must also work in unison with government authorities to ensure well-functioning market systems.
Only when such systems are firmly established will the poor be able to participate in today’s vast global economy.
The Middle East Awakening
BERLIN – When the democratic revolt in Tunisia successfully ousted the old regime, the world reacted with amazement. Democracy from below in the Arab world?
After the overthrow of Hosni Mubarak’s 30 year-old regime in Egypt, the heartland of the Middle East, amazement has turned into certainty.
The Middle East has awakened and begun to enter the globalized world of the twenty-first century.
Up to now, the region (excluding Israel and Turkey) had more or less missed out on the epochal process of worldwide modernization.
Whether the Arab and wider Islamic world’s democratic awakening will actually prevail or produce only change at the top of authoritarian regimes, whether it will lead to a stable order or sustained chaos and radicalization, still remains unclear.
One thing, however, is already clear: the era when this vast region slept while others modernized has ended.
The grassroots revolt will, of course, continue.
Virtually no country in the region will escape it, though when and where the next eruption will occur remains uncertain. Iran, Syria, and Saudi Arabia are all candidates, with the latter probably posing the most difficulties.
Israel, too, would be well advised to prepare for epochal change in the region and try to reach a peace settlement with the Palestinians and Syria as quickly as possible.
There is, however, little indication that Israel’s government has the vision required for such an undertaking.
The problems are the same almost everywhere (with the exception of Israel and Turkey): political suppression, economic underdevelopment and grinding poverty (except in the smaller oil states), a lack of education, high unemployment, and huge demographic pressures, owing to a very young and rapidly growing population.
These problems have been cited, year after year, in the United Nations Development Program’s reports.
Moreover, the situation was exacerbated by the incompetence of the region’s authoritarian regimes, which have been unable to provide their young people with any prospects beyond repression.
So it was only a matter of time until this powder keg was ignited.
The fuses were the new information technologies of the Internet and satellite television, such as Al Jazeera.
Indeed, one historical irony is that it wasn’t American hard power – as applied, for example, in the Iraq war – that furthered this democratic revolution, but rather its soft power – Twitter and Facebook – which was much maligned under George W. Bush and his neocon advisers.
Silicon Valley, it seems, has more potency than the Pentagon.
These digital tools from the United States became the instruments for a trans-Arabian/Iranian youth revolt for freedom and democracy.
And, although many things in the Middle East are in short supply, there is no dearth of hopeless young people, whose numbers will continue to grow in the coming years.
Indeed, whatever resemblance events on Cairo’s Tahrir Square bear to May 1968 in Paris and the fall of the Berlin Wall in 1989, it would be premature to proclaim that freedom has prevailed.
Whether it does will depend to a large degree on how the West responds now, because what is at stake is not just the ousting of tyrants, but also the profound transformation and modernization of entire societies and economies. It is a staggering task.
Moreover, compared to Eastern Europe in 1989, the Middle East in 2011 lacks any stabilizing external structures, such as NATO and the European Union, that could influence domestic reforms by holding out the prospect of membership.
The efforts involved in this great transformation must come from within these societies, and this in all likelihood is asking too much.
Eastern Europe’s transformation after 1989 took a lot longer and was much more costly than originally envisaged.
There were many people who lost out during this transformation, and the democratic revolution’s organizers were not necessarily those who could push through the democratic and economic development.
And there is the experience of Ukraine’s “Orange Revolution” in 2004, which failed a few years later due to the estrangement, incompetence, and corruption of its leaders.
Taken together, these constraints and analogies suggest that the West, particularly Europe, should focus on long-term assistance for the democratic and economic development of the Middle East’s reborn countries, and also on partnerships with all forces that support their countries’ democratization and modernization. The West can no longer continue with Realpolitik as usual.
These tasks call for largesse, both financial and otherwise (opportunities to travel, for example, were of vital importance in locking in the democratic aspirations of East Europeans after 1989), and they require decades, not years, of persistence.
In other words, success will be expensive – very expensive – which will be anything but popular in the current economic downturn.
But a democracy that does not translate into regular dinners is a democracy that is bound to fail.
Economic aid, the opening of the EU and US markets, strategic energy projects, legal and constitutional advice, and cooperation between universities are among the resources that the West must supply if it wants to contribute to the success of the Middle East’s democratic awakening.
Should this awakening fail, the result will be a radicalization throughout the region.
There can be no return to the status quo ante. The genie is out of the bottle.
The Middle East in Motion
BERLIN – Great speeches are all too often underestimated as being mere words.
In fact, they can have powerful consequences.
This is obviously the case with President Barack Obama’s recent address to the Muslim world in Cairo, because – mere coincidence or excellent timing? – things in the Middle East have been in flux ever since.
Since Obama’s Cairo speech, there have been elections in Lebanon where, surprisingly, the alliance of pro-Western parties scored a clear victory against Hezbollah and its allies.
Also noteworthy in that election is that the losing side immediately accepted defeat and that Syria is now obviously serious about building a new rapport with Lebanon.
Iran’s recent “election” saw blatant manipulation in favor of the incumbent president incite a democratic mass uprising.
One is astonished by the fact that Iran’s government did not opt for transparency immediately, by promptly and comprehensively providing the facts about the voting, facts that it alone possesses.
After all, if President Mahmoud Ahmadinejad has genuinely won by a margin of 2:1, there is nothing to fear.
What is happening, however, is precisely the opposite, and for this there is only one explanation: the election was rigged.
The election fraud in Iran has caused a mass movement in the country’s cities, which – this much is clear even now – will fundamentally change the country.
Indeed, either the regime will resort to brute force to suppress the protests, thus abandoning any pretense of democratic legitimacy in favor of de facto military dictatorship, or it will find it impossible to beat the subversive genie of democracy back into its bottle, and Iran will increasingly open up and reform itself.
In the case of violent suppression, the West will find it a lot harder to hold talks with Iran over its nuclear program, because the regime will be able to rely for its survival solely on isolation and confrontation with the outside world.
Moreover, talks with the regime would give rise to substantial legitimacy problems in the West.
The Islamic Republic will not be able to get away with the Chinese option – to combine political suppression at home with economic reform and greater openness to the outside world – because its structures are too weak and brittle for this.
The ruling ideology, moreover, is unlikely to survive such a step unharmed.
Indeed, aside from matters of domestic policy and the issue of internal freedom, the choice between the major candidates hinges on the question of whether Iran should seek greater international integration.