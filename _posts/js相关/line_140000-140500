Had a free and fair referendum been offered to the Sudeten Germans, a majority might have backed union with Germany.
But would that have given them the right to leave the remainder of Czechoslovakia defenseless against its large and hostile neighbor?
The UK and Spain do not need to fear that independence for Scotland and Catalonia would expose them to such threats.
Nonetheless, the secession of Scotland would deprive the UK of significant North Sea oil revenues, on which the economics of Scottish independence largely relies, and Spain could also suffer from the loss of Catalonia’s disproportionately large contribution to the Spanish economy.
Widespread human rights violations, either caused or tolerated by a national government, can give rise to what is sometimes called a remedial right to secession for a region’s inhabitants.
If other remedies fail in such a situation, secession might be justified as a last resort, even if it imposes heavy costs on the rump state.
That was the case when Bangladesh seceded from Pakistan, and it was also allegedly the case when NATO supported Kosovo’s independence from Serbia.
But this is not true of Scotland or Catalonia; nor, despite Russian propaganda, does it appear to be the case for those regions of Ukraine with ethnic Russian majorities.
If Scotland and Catalonia ever become independent countries, it will only be because the UK and Spain permit it.
All states have an interest in stability, so it is hard to imagine that, in the absence of widespread, grave, and undeniable human rights violations, other states would recognize a region that, after being part of a state for many centuries, declared itself independent without the acquiescence of the country from which it secedes.
The EU is also unlikely to accept Scotland or Catalonia as a member if the UK or Spain rejects their claims to independence.
Indeed, European Commission President José Manuel Barroso has said that the EU may reject Scotland and Catalonia’s applications, or at least delay them considerably, even if the UK and Spain do accept their independence.
And, without EU membership, it is hard to imagine that a majority of people in Scotland or Catalonia would take the plunge into economic uncertainty that independence would bring.
The role of a referendum in a region seeking to secede can therefore only be a form of persuasion aimed at the government of the existing state.
A large turnout showing a clear majority for independence would be a way to say: See how strongly we feel about this issue.
We are so dissatisfied with the status quo that most of us now favor secession.
If you want us to stay, you need to address the grievances that have caused a majority of us to want to leave.
Philosophy on Top
MELBOURNE – Last year, a report from Harvard University set off alarm bells, because it showed that the proportion of students in the United States completing bachelor’s degrees in the humanities fell from 14% to 7%.
Even elite universities like Harvard itself have experienced a similar decrease.
Moreover, the decline seems to have become steeper in recent years.
There is talk of a crisis in the humanities.
I don’t know enough about the humanities as a whole to comment on what is causing enrollments to fall.
Perhaps many humanities disciplines are not seen as likely to lead to fulfilling careers, or to any careers at all.
Maybe that is because some disciplines are failing to communicate to outsiders what they do and why it matters.
Or, difficult as it may be to accept, maybe it is not just a matter of communication: Perhaps some humanities disciplines really have become less relevant to the exciting and fast-changing world in which we live.
I state these possibilities without reaching a judgment about any of them.
What I do know something about, however, is my own discipline, philosophy, which, through its practical side, ethics, makes a vital contribution to the most urgent debates that we can have.
I am a philosopher, so you would be justified in suspecting bias in my view.
Fortunately, I can draw on an independent report by the Gottlieb Duttweiler Institute (GDI), a Swiss think tank, to support my claim.  
GDI recently released a ranked list of the top 100 Global Thought Leaders for 2013.
The ranking includes economists, psychologists, authors, political scientists, physicists, anthropologists, information scientists, biologists, entrepreneurs, theologians, physicians, and people from several other disciplines.
Yet three of the top five global thinkers are philosophers: Slavoj Žižek, Daniel Dennett, and me.
GDI classifies a fourth, Jürgen Habermas, as a sociologist, but the report acknowledges that he, too, is arguably a philosopher.
The only Global Thought Leader in the top five not involved in philosophy is Al Gore.
There are more economists in the top 100 than thinkers from any other single discipline, but the top-ranking economist, Nicholas Stern, ranks tenth overall.
Can it really be true that four of the world’s five most influential thinkers come from the humanities, and 3-4 from philosophy?
To answer that question, we have to ask what GDI measures when it compiles its ranking of Global Thought Leaders. 
GDI aims to identify “the thinkers and ideas that resonate with the global infosphere as a whole.”
The infosphere from which the data are drawn may be global, but it is also English-language only, which may explain why no Chinese thinker is represented in the top 100.
There are three eligibility requirements: one has to be working primarily as a thinker; one must be known beyond one’s own discipline; and one must be influential.
The ranking is an amalgam of many different measurements, including how widely the thinkers are watched and followed on YouTube and Twitter, and how prominently they feature in blogs and in the wikisphere.
The outcome indicates each thinker’s relevance across countries and subject areas, and the ranking selects those thinkers who are most talked about and who are triggering wider debate.
The rankings will no doubt vary from year to year.
But we have to conclude that in 2013 a handful of philosophers were particularly influential in the world of ideas.
That would not have been news to the Athenian leaders who considered what Socrates was doing to be sufficiently disturbing to put him to death for “corrupting the youth.”
Nor will it be news to anyone familiar with the many successful efforts to bring philosophy to a broader market. 
There is, for example, the magazine Philosophy Now, and equivalents in other languages. There are the Philosophy Bites podcasts, many blogs, and free online courses, which are attracting tens of thousands of students.
Perhaps the growing interest in reflecting on the universe and our lives is the result of the fact that, for at least a billion people on our planet, the problems of food, shelter, and personal security have largely been solved.
That leads us to ask what else we want, or should want, from life, and that is a starting point for many lines of philosophical inquiry.
Doing philosophy – thinking and arguing about it, not just passively reading it – develops our critical reasoning abilities, and so equips us for many of the challenges of a rapidly changing world.
Perhaps that is why many employers are now keen to hire graduates who have done well in philosophy courses.
More surprising, and more significant still, is the way in which taking a philosophy class can change a person’s life.
I know from my own experience that taking a course in philosophy can lead students to turn vegan, pursue careers that enable them to give half their income to effective charities, and even donate a kidney to a stranger.
How many other disciplines can say that?
Choosing Death
PRINCETON – “I will take my life today around noon.
It is time.”
With these words, posted online, Gillian Bennett, an 85-year-old New Zealander living in Canada, began her explanation of her decision to end her life.
Bennett had known for three years that she was suffering from dementia.
By August, the dementia had progressed to the point at which, as she put it: “I have nearly lost me.”
“I want out,” Bennett wrote, “before the day when I can no longer assess my situation, or take action to bring my life to an end.”
Her husband, Jonathan Bennett, a retired philosophy professor, and her children supported her decision, but she refused to allow them to assist her suicide in any way, as doing so would have exposed them to the risk of a 14-year prison sentence.
She therefore had to take the final steps while she was still competent to do so.
For most of us, fortunately, life is precious.
We want to go on living because we have things to look forward to, or because, overall, we find it enjoyable, interesting, or stimulating.
Sometimes we want to go on living because there are things that we want to achieve, or people close to us whom we want to help.
Bennett was a great-grandmother; if all had been well with her, she would have wanted to see the next generation grow up.
Bennett’s developing dementia deprived her of all of the reasons for wanting to continue to live.
That makes it hard to deny that her decision was both rational and ethical.
By committing suicide, she was giving up nothing that she wanted, or could reasonably value.
“All I lose is an indefinite number of years of being a vegetable in a hospital setting, eating up the country’s money but having not the faintest idea of who I am.”
Bennett’s decision was also ethical because, as the reference to “the country’s money” suggests, she was not thinking only of herself.
Opponents of legal voluntary euthanasia or physician-assisted suicide sometimes say that if the laws were changed, patients would feel pressured to end their lives in order to avoid being a burden to others.
Baroness Mary Warnock, the moral philosopher who chaired the British government committee responsible for the 1984 “Warnock Report,” which established the framework for her country’s pioneering legislation on in vitro fertilization and embryo research, disagrees.
She has suggested that there is nothing wrong with feeling that you ought to die for the sake of others as well as for yourself.&nbsp;
In an interview published in 2008 in the Church of Scotland’s magazine Life and Work, she supported the right of those suffering intolerably to end their lives.
“If somebody absolutely, desperately wants to die because they’re a burden to their family, or the state,” she argued, “then I think they too should be allowed to die.”
Because Canada’s public health service provides care for people with dementia who are unable to care for themselves, Bennett knew that she would not have to be a burden on her family; nonetheless, she was concerned about the burden that she would impose upon the public purse.
In a hospital, she might survive for another ten years in a vegetative state, at a cost she conservatively estimated to be around $50,000-$75,000 per year.
As Bennett would not benefit from remaining alive, she regarded this as a waste.
She was concerned, too, about the health care workers who would have to care for her: “Nurses, who thought they were embarked on a career that had great meaning, find themselves perpetually changing my diapers and reporting on the physical changes of an empty husk.”
Such a situation would be, in her words, “ludicrous, wasteful, and unfair.”
Some will object to the description of a person with advanced dementia as an “empty husk.”
But, having seen this condition overtake my mother and my aunt – both vibrant, intelligent women, who were reduced to lying, unresponsive, in a bed for months or (in my aunt’s case) years – it seems to me entirely accurate.
Beyond a certain stage of dementia, the person we knew is gone.
If the person did not want to live in that condition, what is the point of maintaining the body?
In any health-care system, resources are limited and should be used for care that is wanted by the patient, or from which the patient will benefit.
For people who do not want to live on when their mind has gone, deciding when to die is difficult.
In 1990, Janet Adkins, who was suffering from Alzheimer’s disease, traveled to Michigan to end her life with the assistance of Dr. Jack Kevorkian, who was widely criticized for helping her to die, because at the time of her death she was still well enough to play tennis.
She chose to die nonetheless, because she could have lost control over her decision if she had delayed it.
Bennett, in her eloquent statement, looked forward to the day when the law would allow a physician to act not only on a prior “living will” that bars life-prolonging treatment, but also on one that requests a lethal dose when the patient becomes incapacitated to a specified extent.
Such a change would remove the anxiety that some patients with progressive dementia have that they will go on too long and miss the opportunity to end their life at all.
The legislation Bennett suggests would enable people in her condition to live as long as they want – but not longer than that.
The Bitterness of Sugar
PRINCETON – Sugar is sweet, but the ethics of its production is anything but appealing.
“Sugar Rush,” a recent report released by Oxfam International as part of its “Behind the Brands” campaign, has shown that our use of sugar implicates us in land grabs that violate the rights of some of the world’s poorest communities.
Better-informed and more ethical consumers could change this.
We are genetically programmed to like sweet things, and when people become more affluent, they consume more sugar.
The resulting increase in sugar prices has led producers to seek more land on which to grow sugarcane.
It is no surprise that the poor lose when their interests conflict with those of the rich and powerful.
The Oxfam report provides several examples of producers who have acquired land without the consent of the people who live on it, turning farmers into landless laborers.
Here is one.
In the northeastern Brazilian state of Pernambuco, a group of fishing families had lived since 1914 on islands in the Sirinhaém River estuary.
In 1998, the Usina Trapiche sugar refinery petitioned the state to take over the land.
The islanders say that the refinery then followed up its petition by destroying their homes and small farms – and threatening further violence to those who did not leave.
As recently as last year, the fishing families say, employees of the refinery burned down homes that had been rebuilt.
Trapiche moved the families to a nearby town, where they gained access to electricity, water, sanitation, and schooling, but if they want to continue to fish, they have to travel a long distance.
Many of them are still seeking to return to the islands.
Both Coca-Cola and PepsiCo use Usina Trapiche sugar in their products.
Does that make them responsible for the wrongs done to the people whose land Trapiche is using to produce that sugar?
In the 1990’s, Nike tried to wash its hands of responsibility for the use of child labor and other unconscionable labor practices in the factories that produced its sneakers.
That did not go down well with its customers, and in the end Nike decided to do the right thing, inspecting factories, tackling problems, and being transparent about its suppliers. 
Likewise, McDonald’s initial response to criticism of its suppliers’ animal-welfare practices was to sue the activists who made the allegations.
The company expected that its critics would give up.
But when two of them, with nothing to lose, defended themselves in court, the result was the longest libel trial in British legal history – and a public-relations disaster for the corporate giant.
After the judge held that some of the activists’ claims were not defamatory, because they were true, McDonald’s began to accept responsibility for its suppliers’ practices.
It has since become a much-needed force for improvement in the treatment of animals used for food in the United States.
More recently, the collapse of the Rana Plaza garment factory in Bangladesh earlier this year, which killed more than 1,000 people, posed a similar question for the garment industry.
Associated British Foods (ABF), which is both a major sugar producer and the owner of the retail clothing chain Primark, took responsibility for its suppliers by signing up, along with 80 other clothing brands, to a legally binding building-safety agreement supported by trade unions and the Bangladeshi government.
What applies to the clothing industry should hold for the food industry, too – and not just for sugar, but for all food production.
Oxfam is asking the ten biggest food brands to show leadership by acknowledging their responsibility for land-rights violations involving their suppliers.
In particular, Oxfam wants these global companies to avoid buying from suppliers that have acquired land from small-scale food producers without these producers’ free, prior, and informed consent.
Where land has already been acquired without such consent, and the acquisition is in dispute, Oxfam wants the corporations to insist on fair dispute-resolution procedures.
“Behind the Brands” includes a score sheet, ranking the Big 10 on a range of issues, including their impact on workers, water, land, women, and climate change.
On land issues, Oxfam rates PepsiCo, and ABF either “poor” or “very poor.”
Nestlé scores better, because its guidelines for suppliers – used for the sourcing of sugar, soy, palm oil, and other commodities – require that they obtain the free, prior, and informed consent of indigenous and local communities before acquiring land.
Nestlé was the first of the Big 10 to support this principle fully.
Then, on November 7, Coca-Cola responded to the Oxfam campaign by declaring that it would have “zero tolerance” for land grabbing by its suppliers and bottlers.
Coca-Cola committed to disclosing the companies that supply it with sugar cane, soy, and palm oil, so that social, environmental, and human rights assessments can be conducted; it will also engage with Usina Trapiche regarding the conflict with the fishing families of the Sirinhaém River estuary.
Oxfam’s advocacy is raising the standards for the global food industry.
If PepsiCo and ABF want us to regard them as ethical producers, they need to follow the lead of Nestlé and now Coca-Cola, and accept responsibility for their suppliers’ conduct toward some of the world’s poorest and most powerless people.
Is Citizenship a Right?
MELBOURNE – Should your government be able to take away your citizenship?
In the United Kingdom, the government has had the legal authority to revoke naturalized Britons’ citizenship since 1918.
But, until the terrorist bombings on the London transport system in 2005, this power was rarely exercised.
Since then, the British government has revoked the citizenship of 42 people, including 20 cases in 2013.
British Home Secretary Theresa May has said that citizenship is “a privilege, not a right.”
Most of the 42 held dual nationality. Mohamed Sakr, however, did not.
His parents came to Britain from Egypt, but he was not an Egyptian citizen.
Therefore, by stripping him of citizenship, the UK government made him stateless.
Sakr appealed the decision from Somalia, where he was living.
His case was strong, because the UK Supreme Court subsequently ruled in a different case that the government does not have the power to make a person stateless.
Nevertheless, Sakr discontinued his appeal, apparently because he was concerned that the use of his cellphone was revealing his location to US intelligence services.
Months later, while still in Somalia, he was killed in an American drone attack.
Now, partly in response to fears that Britons who have joined the fighting in Syria may return to carry out terrorism at home, the government has proposed legislation enabling it to revoke the citizenship of naturalized Britons suspected of involvement in terrorist activities – even if this makes them stateless.
(Since the start of the year, more than 40 Britons have been arrested on suspicion of engaging in military activities in Syria.)
The House of Commons passed the legislation in January, but in April the House of Lords voted to send it to a joint parliamentary committee for additional scrutiny.
In the United States, citizenship can be revoked only on limited grounds, such as fraud committed in the citizenship application or service in another country’s military.
Arguably, joining a terrorist organization hostile to the US is even worse than joining a foreign army, because terrorist organizations are more likely to target civilians.
But one important difference is that if people who join other countries’ military forces lose their US citizenship, they can presumably become citizens of the country for which they are fighting.
Terrorist organizations usually have no such ties to a particular government.
The 1961 United Nations Convention on the Reduction of Statelessness, to which Britain is a signatory, does allow countries to declare their citizens stateless if it is proved that they have done something “prejudicial to the vital interests of the country.”
The legislation currently before the UK Parliament does not require any judicial or public proof even of the weaker claim that someone’s presence in the country is not conducive to the public good.
Should the person whose citizenship is revoked mount an appeal, the government is not required to disclose to the appellant the evidence on which it has based its decision.
Though governments are bound to make mistakes from time to time in such cases, judges or tribunals will be unable to probe the evidence put before them.
Another, more sinister possibility is deliberate abuse of these powers to get rid of citizens whose presence in the country is merely inconvenient.
There is a strong case for an appeal system that allows for full and fair review of decisions to revoke citizenship.
But governments will respond that to make the evidence available to a person believed to be involved with a terrorist organization could reveal intelligence sources and methods, thus jeopardizing national security.
The ability to revoke citizenship without presenting any evidence in public is one reason why a government may prefer this course to arresting and trying terrorism suspects.
And yet simply revoking citizenship does not solve the problem of leaving at large a suspected terrorist, who may then carry out an attack elsewhere – unless, as with Sakr, he is killed.
The larger question raised by the UK’s proposed legislation is the desirable balance between individual rights, including the right to citizenship, and the public good.
Suppose that the government gets it right 19 times out of 20 when it relies on suspicion of involvement in terrorist activities to revoke people’s citizenship.
If that were the case with the decisions made by the UK government in 2013, there would still be a high probability that an innocent naturalized citizen was made stateless.
That is a grave injustice.
Suppose, however, that the 19 people correctly suspected of involvement in terrorism were able to return to Britain, and one carried out a terrorist attack similar to the London transport bombings, which killed 52 innocent people (the four bombers also died).
In the face of such atrocities, it is difficult to insist that individual rights are absolute.
Is it better to have one innocent person unjustly made stateless, or to have 52 innocent people killed and many others injured?
The much greater harm done by the terrorist attack cannot be ignored; but when a democratic government starts to revoke citizenship and make people stateless, it sets a precedent for authoritarian regimes that wish to rid themselves of dissidents by expelling them, as the former Soviet Union did to the poet and later Nobel laureate Joseph Brodsky – among many others.
In the absence of global citizenship, it may be best to retain the principle that citizenship is not to be revoked without a judicial hearing.
Migration on the Move
LONDON – In 2000, the United Nations established the Millennium Development Goals (MDGs) to drive progress on important development objectives, such as reducing poverty, promoting gender equality, and curbing disease.
But the architects of the MDGs neglected one critical issue: migration.
Fortunately, it looks like world leaders will not make the same mistake with the post-2015 development agenda.
The scale of remittances alone should be enough to convince the world that migration deserves a prominent spot in the post-2015 agenda.
Last year, migrants from developing countries sent an estimated $414 billion to their families – triple the total of official development assistance.
More than a billion people rely on such funds to help pay for education, health care, water, and sanitation.
As if that were not enough, remittances have important macroeconomic benefits, enabling countries to pay for essential imports, access private capital markets, and qualify for lower interest rates on sovereign debt.
But many of migration’s benefits are squandered.
Financial intermediaries extract 9% of remittances, on average, for an estimated total of $49 billion in migrant earnings last year.
Rapacious recruiters, who often keep one-third of a migrant’s pay, skim billions more.
Meanwhile, smuggling, trafficking, exploitation, and discrimination take an incalculable human toll.
This is where the post-2015 development agenda comes in.
With the right incentives, governments and companies can be encouraged to pursue policies aimed at ensuring that more funds reach poor families, while working to protect migrants’ rights and prevent discrimination.
At the same time, the agenda can help to transform perceptions of migrants.
As it stands, migration is often viewed as a sign of a home country’s failure to offer adequate opportunities, while locals in destination countries believe that migrants are stealing their jobs, depressing their wages, or exploiting their welfare systems.
But the fact that 9% of British citizens live abroad demonstrates that people move regardless of their home country’s wealth.
Moreover, evidence shows that migrants contribute more than they appropriate, as they foster knowledge transfers, trade, tourism, investment, and even job creation through entrepreneurship, while doing important jobs – from caring for children and the elderly to staffing hotels and restaurants to picking crops – that are undesirable to locals.
Given the undeniable benefits of migration, it might be surprising that it was not included in the MDGs.
The problem is that, in 2000, there was neither enough evidence about migration’s effects on development nor sufficient political support to secure its addition to the agenda.
That is no longer the case.
A group of countries, international agencies, and NGOs has made a strong case to the UN Open Working Group on Sustainable Development Goals (the body responsible for facilitating deliberations on the post-2015 agenda) that migration can help reduce poverty and generate economic growth.
The group’s proposal – which includes lower remittance costs, increased pension portability, and strong action against human trafficking – calls for a specific set of targets and indicators, which would substantially enrich the next development agenda.
It even calls for migrants to be considered when measuring progress toward other goals, such as ensuring decent work and equitable access to health-care services.
There is a groundswell of political support behind this effort.
Last October, when the UN General Assembly met to discuss migration for only the second time, member states unanimously approved a declaration that called for migration’s inclusion in the post-2015 agenda.
The International Organization for Migration Council issued a similar resolution in November, and the campaign has attracted additional support from civil-society groups and international organizations.
The international community has pledged to place people at the center of the post-2015 development agenda.
There is no better expression of this commitment than recognizing the indispensable role that migrants play – and protecting their rights.
To this end, the agenda must create the basis for sustained and meaningful global partnerships on migration and human mobility, similar to efforts under the MDGs, to make trade and technology transfer work for development.
But not everyone stands behind these goals.
A handful of national leaders could veto the inclusion of migration, owing to misplaced fears of its domestic political consequences.
To avoid such an outcome, it is important to note that opinion polls usually reveal public anxiety about unregulated migration, not legal migration or legitimate asylum-seekers.
Even in Europe, where populism is on the rise, citizens are more enlightened than their leaders: 69% of Europeans say that they are not worried about legal migration, and 62% do not believe that migrants take jobs from locals.
Governments like those in Germany and Sweden that manage migration well and invest in integration have the strongest public support.
The number of migrants that a country welcomes is a matter that only it can decide.
But how migrants are treated, whether they are allowed to keep what they earn, and what they contribute to social and economic development are issues that matter to everyone.
International law requires that the human rights of all migrants, whatever their status, are respected, which is also a fundamental precondition for individual and collective development.
Migration – when it is safe, legal, and voluntary – is the oldest poverty-reduction and human-development strategy.
It seems that this long-ignored reality is finally sinking in, pushing discussion of the post-2015 development agenda in the right direction.
Dying for Europe
LONDON – This summer, a gruesome tragedy unfolded aboard a ship in the Mediterranean Sea.
Twenty-nine men, women, and children fleeing crisis-torn countries succumbed to engine fumes in the vessel’s hold.
As 60 others scrambled to escape, the human traffickers carrying them to Europe stabbed them and threw them into the sea off the coast of Lampedusa.
Eventually, a Danish petrol tanker rescued 569 survivors.
More recently, some 500 migrants died off the coast of Malta, when a group of human traffickers responded to the passengers’ refusal to move onto smaller vessels by deliberately ramming the boat that had carried them from Egypt.
Less than a week later, dozens of asylum-seekers perished when their boat capsized near the Libyan coast.
Such large numbers of deaths in and around Europe should do more than briefly seize headlines.
But Europeans seem inured to the plight of asylum-seekers and migrants, almost 3,000 of whom have died in the Mediterranean this year.
This situation is untenable, both morally and politically.
Of course, Europe cannot help all of those fleeing violence and destitution.
But, as the world’s wealthiest continent, it can certainly do more, especially if it adopts a unified approach.
At a time when the number of displaced people is at an historic high, the European Union – which accounts for 29% of global wealth – hosts just 9% of refugees, leaving far poorer countries to carry most of the burden.
For example, tiny Lebanon shelters more than one million of the three million Syrians who are displaced, whereas the EU – 100 times larger – has taken in only about 100,000.
The EU is hardly powerless to address the tragic situation in and around the Mediterranean.
The new European Commission, Council, and Parliament should be able to uphold the EU’s humanitarian obligations by reducing the number of deaths at sea, thereby setting the stage for a more reasonable public debate about migration, while improving strained relations with Africa.
This will demand, first and foremost, that EU leaders overcome the forces that have so far impeded action.
One obstacle is anti-migrant populism, which has intensified owing to the serious economic challenges that Europeans have faced.
With far-right political parties nipping at their heels, most mainstream politicians have avoided taking a stance on migration that might make them seem “soft.”
Equally paralyzing is the way that the tragic episodes in the Mediterranean are portrayed: as sudden crises, rather than as part of a long-term trend.
As such, they often provoke fiery rhetoric and defensiveness, instead of thoughtful debate.
For example, when the Arab Spring rebellions erupted in 2011, many Europeans immediately feared that millions of North Africans would descend on their shores.
In the three years since then, just 30,000 have arrived.
Worse, this portrayal implies that little can be done to mitigate, or even prevent, such occurrences.
But the longer-term trends driving such tragedies – including demographic shifts, inadequate legal routes to Europe, poor governance and economic prospects in origin countries, and skewed public perceptions of refugees and migrants – can, to various degrees, be addressed.
For starters, European Commission President Jean-Claude Juncker, European Council President Donald Tusk, and the European Parliament should work to ensure that the public debate about refugees and migrants is anchored in fact.
As it stands, misconceptions are rampant, with residents of many EU countries believing, for example, that they host three times as many foreigners as they actually do.
By debunking such myths, EU leaders can create space for action, while undercutting the credibility of populists.
Second, in designing asylum and migration policies, the European Commission should involve member states’ foreign, employment, and development ministers, instead of just their interior ministers.
The European Council also should deepen its engagement on these issues.
Third, the EU should craft asylum, migration, and border systems that equitably distribute financial, political, and other costs.
The so-called&nbsp;Common European Asylum System that prevails today is perceived as unfair by many member states, and violates the rights of many asylum seekers.
Fourth, the EU should increase the number of refugees that it accepts, and give more people the option of applying for asylum without having to reach Europe’s borders.
This would result in fewer asylum-seekers making life-threatening voyages that leave their families deeply indebted – human traffickers charge as much as €15,000 ($19,700) to cross the Mediterranean – only to be turned away.
Finally, in order to help prevent the crises that lead to displacement, the EU should work to strengthen its relationships with African countries, especially those bordering the Mediterranean.
Talk of an African Marshall Plan – with loans issued to local businesses, which would repay them to their national governments to use for infrastructure development – proliferated after the Arab Spring, but led nowhere.
But such investment in Africa’s development, together with a regular, structured dialogue, could help to ease many Africans’ plight.
This would reduce their incentive to migrate and, in cases where people still aspire to reach Europe, facilitate a more orderly process.
The EU’s new leaders have an important opportunity to fashion a fresh approach to asylum and migration – one that recognizes that safe, orderly migration can bring major benefits to countries of origin and destination alike.
Given Europeans’ tendency toward generosity and reason – qualities that their political leaders often underestimate – such an initiative would likely even bring political gains.
In short, there is no compelling reason not to extend the principled policy approach on which Europe prides itself to those who would risk their lives to reach its shores.
A Step Closer to Brexit?
LONDON – The referendum on Scottish independence, due on September 18, comes at a time of growing opposition in the United Kingdom to remaining in the European Union.
Of course, it is premature to draw any firm conclusions from these figures.
The referendum on exiting the EU that Prime Minister David Cameron has proposed may not take place, regardless of the success (whatever that may mean) of his promised “renegotiation” of the terms of British membership.
But, as a result of various ostensibly minor issues, the likelihood of a British exit seems to be increasing – which fundamentally alters the importance of the vote in Scotland.
For example, the proposal of the relatively unknown Jonathan Hill, the leader of the UK House of Lords, as the British member of the new European Commission headed by Jean-Claude Juncker was just the latest in a long series of British EU errors.
Cameron’s spokesmen said in July that, at his first meeting with the new Commission president, Cameron would seek a prestigious portfolio, such as the internal market, for Hill.
Juncker’s office coldly replied that important portfolios in the new Commission would go to major political figures, and that Juncker “does not owe [Cameron] anything.”
Given Cameron’s opposition to Juncker’s candidacy for the Commission presidency, the abuse to which Juncker has been subjected by the British press, and Hill’s lack of centrality within British politics, Cameron may be justifiably nervous when Juncker announces his appointments to the new Commission.
Juncker, after all, has many senior politicians to accommodate, and their approval by the European Parliament is no minor matter.
And, though it was perhaps unsurprising that Cameron should be unenthusiastic about Juncker’s candidacy, the vehemence of his opposition was extraordinary.
Cameron no doubt wished to reassure those in his Conservative Party who doubt his euroskeptic zeal.
Even so, Cameron’s supposed remark that the UK was more likely to leave the EU if Juncker’s candidacy succeeded was strange and disquieting, not least because any renegotiation of the terms of British membership will be carried out primarily with other member states, not with the Commission.
Cameron’s bid to thwart Juncker’s candidacy mirrored his unsuccessful attempt to prevent the adoption of the EU’s fiscal compact in 2012.
He overestimated German Chancellor Angela Merkel’s willingness and ability to support the British position.
Many influential Germans are reluctant to help Cameron in what they regard as his self-created European problem.
They will not allow Merkel much latitude here, even if she seeks it.
If Cameron’s advisers overestimated the support that they might have received from Merkel and others, they clearly underestimated the power and effectiveness of the European Parliament.
For many months, the European Parliament had made it clear that it had specific ideas about how it would exercise its new powers, granted by the Treaty of Lisbon, over the Commission presidency.
But the rapid post-election agreement among the Parliament’s major political groups to support Juncker caught Cameron off guard.
Far from enhancing British influence, threats of withdrawal have undermined the Cameron government’s credibility and influence within the EU, as colleagues have become disinclined to engage in significant compromises with a UK that may not be a member in two years.
Traditionally, EU heads of state and governments try to help one another with their domestic political problems.
But there is a growing sense in the European Council that Cameron is abusing this goodwill.
Thus, Cameron’s claim that Juncker’s election would make it more difficult to ensure the UK’s continued EU membership risks becoming a self-fulfilling prophecy.
To present Juncker’s candidacy as a matter of high political principle, with Cameron heroically but unsuccessfully standing alone against the dark forces of federalism and centralization, could have only reinforced English feelings of alienation from Europe.
That is less likely to be the case in Scotland.
If Cameron returns as prime minister after the general election in 2015, he will face an uphill battle in renegotiating the terms of British EU membership, owing to resistance not only from his European partners, but also from his own Conservative Party, which is close to advocating British withdrawal.
Indeed, it is difficult to see how a re-elected Cameron could maintain the Conservative Party’s unity without endorsing a “no” vote in the referendum on the outcome of his own renegotiation.
Britain’s EU membership will be one of the major issues at stake in next year’s election.
It would be a tragedy if British voters cast their ballots without fully understanding the European implications of their choice.
One thing, however, seems certain: If Scotland votes for independence in September, a referendum within the rump UK on continued EU membership would be even less likely to produce a victory for those who wish to remain.
Migration’s Hall of Mirrors
LONDON – On both sides of the Atlantic, anti-immigrant politics are undermining democracies and damaging lives.
Far-right nationalist parties are gaining traction in Europe, while millions of undocumented migrants suffer in the shadows.
In the United States, President Barack Obama, concerned about his party’s ability to retain control of the Senate, has decided to put off immigration reform until after the election in November.
Yet that may be the wrong approach.
A new public-opinion survey by the German Marshall Fund (GMF) reveals that anti-immigrant sentiment stems largely from misinformation, not entrenched animus.
The most important finding of the GMF’s Transatlantic Trends survey is that concern about immigrants falls sharply when people are given even the most basic facts.
For example, when asked if there are too many immigrants in their country, 38% of the Americans surveyed agreed.
But when respondents were told how many foreigners actually reside in the US before being asked that question, their views changed significantly: just 21% replied that there were too many.
The same was true in country after country.
In the United Kingdom, 54% of respondents said that there were too many immigrants; that number fell to 31% among those who were given the facts about foreigners.
In Greece, 58% became 27%; Italy went from 44% to 22%; and so on.
The only countries without such a gap were those with either very little immigration, like Poland, or those with a more open, informed, and progressive political debate about immigration, like Sweden and Germany.
Other surveys have exposed extraordinary inaccuracies in perceptions of migrants.
In many developed countries, for example, the public believes that there are three times as many immigrants residing in their country as there really are.
The average Briton believes that 34% of UK residents are foreigners; the true number is just 11%.
Such distortions disappear in countries where migration challenges are confronted openly, discussed reasonably, and addressed with conviction.
The average Swede, for example, believes that 18% of the country’s population is composed of migrants; the actual number is close to 13%.
As a result, populism in these societies is not on the rise, and mainstream politicians do not vilify minorities and migrants.
This constitutes strong evidence that reality-based debate and policymaking can fundamentally transform the negative political dynamics generated by migration.
It also suggests that, by failing to engage voters on the reality of migration, mainstream politicians in Europe are manufacturing support for extremist parties.
This self-inflicted political wound is extremely dangerous.
The Transatlantic Trends survey also shows that the American public is not worried about legal migration, while around two-thirds believe that the children of immigrants are being well integrated into their communities.
These findings should embolden policymakers to be more proactive in designing pathways for legal migration and policies to integrate migrants.
Even when it comes to illegal immigrants, though US citizens express concern, they are more reasonable than their political leaders about how to solve the problem.
A plurality of Americans surveyed by the GMF, for example, said that illegal immigrants should be allowed to obtain legal status.
A deliberative approach to engaging the public on other aspects of migration also could help quell anti-migrant sentiment.
For example, recent research&nbsp;in several countries shows that immigrants as a whole contribute more economically to their communities than they take from them.
In Germany, a study by the Bertelsmann Foundation, to be released next month, shows that the net fiscal contribution per migrant amounted to €3,300 ($4,260) in 2012.
Such data upend the conventional wisdom that migrants are a drain on public services.
Of course, migration creates real challenges for communities and can lead to job losses and lower wages for native workers.
But here, too, it is the absence of attention to these issues, not necessarily the presence of migrants, that is the problem.
Implementing vigorous retraining policies, for example, would be a better way to counter these adverse effects than calling for mass deportations.
This is one reason why labor unions, which once opposed immigration across the board, are now far more supportive of measures that would legalize undocumented workers and create more pathways for migration.
Informed public debate is the sine qua non of a democratic polity.
In its absence, bias and populism prevail.
The immigration debate will never be an easy one, but it can become less tendentious and more deliberative if its participants consider the facts.
Buy Back or Pay Forward?
LONDON – When British Prime Minister David Cameron asked me to lead a review into the problem of antimicrobial resistance, the last thing I expected was that accepting the position would lead me to question one of the most popular tools for corporate financial management: share buybacks.
The problem of antimicrobial resistance is a serious one.
Left unaddressed, it could be responsible by 2050 for the deaths of some ten million people a year, more than currently die of cancer, along with an astonishing $100 trillion in economic damage.
Fortunately, however, there is much we can do to mitigate the threat – provided that adequate resources are made available.
One important avenue to pursue is the development of new drugs.
In a forthcoming paper, the Review on Antimicrobial Resistance estimates that bringing new antimicrobials to market and improving their administration will cost about $25 billion – a significant sum, but one that pales in comparison to the costs to society if the problem is not checked.
It is also roughly what two of the world’s largest pharmaceutical companies will spend this year buying back their own shares.
While the review has yet to come up with recommendations for financing the development of new drugs, it seems clear to me that it is well within the capacity of the pharmaceutical industry to contribute.
A common argument made by drug companies is that they need to be guaranteed a reward if they are to invest in developing medicines that are unlikely to deliver the kind of returns that other investments may provide.
The only sure way to guarantee drug development, the argument goes, is to allow prices to rise until demand matches supply.
And yet there is a good reason why the pharmaceutical industry can and should play a major role in financing something like a common “Innovation Fund” to provide financing for early-stage research into solving the problem of antimicrobial resistance.
And that reason is one that I became familiar with during my years at Goldman Sachs: enlightened self-interest.
Six years after the eruption of the global financial crisis, the banking industry is still widely blamed for the catastrophe.
And, as a result, banks are being hit with regulatory constraints that limit some aspects of their business.
I suspect that if the industry had shown greater leadership on issues – for example, excessive executive pay – they would have found themselves in a much more favorable environment today.
The same is true of the pharmaceutical industry.
Share buybacks can sometimes be legitimate, but on other occasions they do not seem justified – especially when considered from the standpoint of enlightened self-interest.
In December, the pharmaceutical giant Merck spent $8.4 billion to acquire Cubist Pharmaceuticals, a Massachusetts-based drug-maker that specializes in combating Methicillin-resistant Staphylococcus aureus (MRSA), a bacteria that has become resistant to many types of antibiotics.
In early March – less than three months after the acquisition – Merck announced it would close down Cubist’s early-stage research unit, laying off some 120 staff and perhaps crippling its efforts to introduce new drugs into the pipeline.
Three weeks later, Merck announced that it would spend an additional $10 billion to buy back some of its shares.
It is difficult for an outside observer not to draw a connection between the two decisions.
Of course, dubious buybacks are not confined to the pharmaceutical industry.
Apple is another good example.
The company’s latest quarterly sales results show how the company has become something more than a technology firm; it is now a major middle-class Chinese consumer brand.
Within a year, China will likely be a bigger market for its products than the United States.
And yet, even more striking than this confirmation of the still-rising importance of the BRIC (Brazil, Russia, India, and China) economies is the sheer size of Apple’s ongoing buyback program.
In April, the company announced it had authorized an additional $50 billion to be used for repurchasing shares, bringing the total to $140 billion.
Coming at a time when the technology industry is under increasing scrutiny in the developed world as governments struggle with budget shortfalls and rising debt, this seems to me to be a questionable decision.
Companies’ ability to minimize their global tax burden, while boosting their earnings per share through buybacks – in some cases financed with debt – does not strike me as a stable trend.
When companies are genuinely unable to identify areas of research and investment that would help their business (and employees and clients), they are better off returning the savings to shareholders in the form of higher dividends than authorizing buybacks.
Or, better yet, in a world confronted with a host of problems – from climate change to antimicrobial resistance – industry leaders should begin asking themselves how they can contribute to averting the crises of the future.
Megafunding Drug Research
SEATTLE – As price-gouging practices by a handful of drug companies attract headlines, one troubling aspect of the story remains underplayed.
Exorbitant increases in the prices of existing drugs, including generics, are motivated not just by crass profiteering but by a deep skepticism about the economic feasibility of developing new drugs.
That skepticism is justified.
Traditional models for funding drug development are faltering.
In the US and many other developed countries, the average cost of bringing a new drug to market has skyrocketed, even as patents on some of the industry’s most profitable drugs have expired.
Venture capital has pulled back from early-stage life-sciences companies, and big pharmaceutical companies have seen fewer drugs reach the market per dollar spent on research and development.
Indeed, on average, only one of every 10,000 compounds identified as potentially useful in early-stage research will ultimately win approval from regulators.
The approval process can take as long as 15 years and errs on the side of caution.
Even among drugs that make it to human clinical trials, only one in five will clear that final hurdle.
The price tag for these “slow fails” can be enormous.
Pfizer, for example, spent a reported $800 million on its cholesterol-lowering drug, torcetrapib, before pulling it from phase III clinical trials in 2006.
That’s an unappealing prospect for most investors.
Because the risk of backing any one compound, or even a particular company, is so high, vast pools of investment capital lie out of reach for drug developers.
Spurred by these pressures, finance experts have proposed several funding alternatives that reduce the risk of biopharma investments while improving the efficiency and productivity of the R&D pipeline.
Although industry incumbents may be slow to shift gears, developing countries creating next-generation biopharma hubs have a unique opportunity to adopt and benefit from alternative models.
Many of those models build upon a common strategy for de-risking investments: assembling a diversified portfolio.
Two decades ago, a company called Royalty Pharma launched a diversified model, building a fund of ownership interests in multiple drug royalty streams.
Royalty Pharma focused on approved drugs with blockbuster potential, creating stable revenue streams and impressive equity returns – even during periods of extreme stock-market volatility.
But Royalty Pharma’s model will not bridge the funding gap between the basic research supported by government grants and the late-stage development of drugs that are in clinical trials.
Because the candidate drugs in this R&D “valley of death” are riskier than anything in which Royalty Pharma invests, an even larger portfolio of compounds would be needed to yield levels of risk and rates of return that are acceptable to typical investors.
How large would that portfolio have to be?
One of us (Lo) has carried out simulations of diversified funds for early- and mid-stage cancer drugs, which show that a so-called megafund of $5-30 billion, comprising 100-200 compounds, could sufficiently de-risk the investment while generating returns of between 9-11%.
That’s not exciting territory for venture capitalists and private-equity investors, but it is in keeping with the expectations of institutional investors, such as pension funds, endowments, and sovereign wealth funds.
Moreover, the risk reduction from diversification would allow the megafund to issue large amounts of debt as well as equity, further broadening the pool of potential investors.
To put these numbers in context, consider that the US National Institutes of Health funds just over $30 billion annually in basic medical research, and members of the Pharmaceutical Research and Manufacturers of America spent about $51 billion last year on R&D. A megafund approach would help to make both investments more productive by filling the funding gap between them.
Moreover, this model may work on a smaller scale.
Further simulations suggest that funds specializing in some drug classes, such as therapies for orphan diseases, could achieve double-digit rates of return with just $250-500 million dollars and fewer compounds in the portfolio.
Of course, this approach faces challenges.
It won’t be easy to manage a large pool of candidate compounds and dozens of simultaneous drug trials.
Simulations show that megafunds will not work for all classes of drugs in all therapeutic areas.
Development of Alzheimers’ therapies, for example, is unlikely to benefit from the megafund model.
But where they do work, megafunds could make drug development vastly more efficient, and therefore less costly.
No single company possesses the scale or finances to deploy all the advances in science and technology since the genomics revolution, but a megafund-backed effort could.
Researchers employed by the fund could share knowledge, facilities, and state-of-the-art equipment, data, and computing resources, spread over a wide array of projects.
Failures would be faster – and much cheaper – because stakeholders would be less dependent on any one project.
Emerging-market countries should take note.
Most are chasing the pharmaceutical and biotechnology industries.
China has established hundreds of life-sciences research parks and committed billions of dollars in national funds for drug development; comparable programs are under way in India, Singapore, and South Korea.
For these countries, forming a megafund to test a few hundred compounds could be a much better bet than seeding biotech startups or offering incentives to big pharmaceutical firms.
A biopharma megafund would offer a competitive edge in the industry, with lower development costs, a higher success rate, and faster time to market.
Regional economies would benefit from the same networks of high-paying research jobs, entrepreneurs, investors, and service providers that traditional life-sciences innovation hubs create.
London’s mayor recently embraced this approach, proposing a $15 billion megafund to help the United Kingdom maintain a leadership role in drug development.
In addition to direct investment, governments can also create incentives for the formation of these kinds of funds – for example, by guaranteeing bonds issued for biopharma research.
Ushering a drug from lab bench to bedside requires investing vast sums of money over long horizons.
That funding must pay off for both society and investors.
Emerging countries can lead the world to better health and greater wealth by pioneering new ways to finance drug development.
The Disenchantment of Europe
LONDON – The recent European Parliament elections were dominated by disillusion and despair.
Only 43% of Europeans bothered to vote – and many of them deserted establishment parties, often for anti-EU extremists.
Indeed, the official results understate the extent of popular dissatisfaction; many who stuck with traditional parties did so reluctantly, faute de mieux.
There are many reasons for this political earthquake, but the biggest are the enduring misery of depressed living standards, double-digit unemployment rates, and diminished hopes for the future.
Europe’s rolling crisis has shredded trust in the competence and motives of policymakers, who failed to prevent it, have so far failed to resolve it, and bailed out banks and their creditors while inflicting pain on voters (but not on themselves).
The crisis has lasted so long that most governing parties (and technocrats) have been found wanting.
In the eurozone, successive governments of all stripes have been bullied into implementing flawed and unjust policies demanded by Germany’s government and imposed by the European Commission.
Though German Chancellor Angela Merkel calls the surge in support for extremists “regrettable,” her administration – and EU institutions more generally – is substantially responsible for it.
Start with Greece.
Merkel, together with the European Commission and the European Central Bank, threatened to deprive Greeks of the use of their own currency, the euro, unless their government accepted punitive conditions.
Greeks have been forced to accept brutal austerity measures in order to continue to service an unbearable debt burden, thereby limiting losses for French and German banks and for eurozone taxpayers whose loans to Greece bailed out those banks.
As a result, Greece has suffered a slump worse than Germany’s in the 1930’s.
Is it really any wonder that popular support for the governing parties that complied with this diktat plunged from 69% in the 2009 European Parliament election to 31% in 2014, that a far-left coalition demanding debt justice topped the poll, or that the neo-Nazi Golden Dawn party finished third?
In Ireland, Portugal, and Spain, the bad lending of German and French banks in the bubble years was primarily to local banks rather than to the government.
But here, too, the Berlin-Brussels-Frankfurt axis blackmailed local taxpayers into paying for foreign banks’ mistakes – presenting the Irish with a €64 billion ($87 billion) bill, roughly €14,000 per person, for banks’ bad debt – while imposing massive austerity.
Support for compliant establishment parties duly collapsed – from 81% in 2009 to 49% in 2014 in Spain.
Fortunately, memories of fascist dictatorship may have inoculated Spain and Portugal against the far-right virus, with left-wing anti-austerity parties and regionalists benefiting instead.
In Ireland, independents topped the poll.
The misconception that northern European taxpayers are bailing out southern ones also prompted a backlash in Finland, where the far-right Finns won 13% of the vote, and in Germany, where the new anti-euro Alternative für Deutschland won 7%.
At Merkel’s behest and with the complicity of the ECB, which waited until July 2012 to quell a bond-market panic sparked by eurozone policymakers’ mistakes, the Commission also imposed eurozone-wide austerity, causing a cumulative loss of nearly 10% of GDP in 2011-13, according to the Commission’s own economic model.
By plunging Italy into a deep recession (from which it has yet to recover), austerity sank interim Prime Minister Mario Monti’s broad-based coalition and boosted Beppe Grillo’s anti-establishment, anti-euro Five Star Movement, which finished second in the European Parliament election.
Merkel also demanded a stifling and undemocratic EU fiscal straightjacket, which the Commission duly enforces.
So when voters throw out a government, EU fiscal enforcer Olli Rehn immediately insists that the new administration stick to its predecessor’s failed policies, alienating voters from the EU and pushing them toward the extremes.
Consider France.
After François Hollande became President in 2012 on a pledge to end austerity, his Socialist Party won a large majority in parliamentary elections.
But Berlin browbeat him into further austerity.
Now, with both the center right and the center left discredited – together, they received only 35% of the popular vote – Marine Le Pen’s racist Front National topped the poll by promising radical change.
Along with a chronic economic crisis, Europe now has an acute political crisis.
Yet the EU establishment seems bent on pursuing business as usual.
In the parliament, a vocal but fragmented minority of critics, cranks, and bigots is likely to push the center-right and center-left groups, which still have a combined majority, to club together even more closely.
The low turnout and weakening of mainstream parties gives the European Council – national leaders of the EU’s member states – a pretext to continue cutting deals in smoke-free rooms.
First up will be the choice of the European Commission’s next president.
The outgoing president, José Manuel Barroso, claims that “the political forces that led and supported…the Union’s joint crisis response…have overall won once again.”
Merkel wants to stick to current policies that have failed to deliver growth and jobs.
Perhaps the man to shake things up is Matteo Renzi, Italy’s dynamic 39-year-old prime minister.
In office since February, he won a resounding 41% of the vote, twice that of his nearest rival.
Already committed to reforming his country’s crony capitalism, he now has a mandate to challenge Merkel’s crisis response.
The timing is perfect: Italy takes over the EU’s rotating presidency in July.
Renzi has already called for a €150 billion EU investment boost and greater fiscal flexibility.
Instead of a eurozone caged in by Germany’s narrow interests as a creditor, Europe needs a monetary union that works for all of its citizens.
Zombie banks should be restructured, excessive debts (both private and public) written down, and increased investment combined with reforms to boost productivity (and thus wages).
The fiscal straightjacket should be scrapped, with governments that borrow too much allowed to default.
Ultimately, the fairer, freer, and richer eurozone that would emerge is in Germany’s interest, too.
Europeans also need a greater say over the EU’s direction – and the right to change course.
They need a European Spring of economic and political renewal.
Germany’s Economic Mirage
LONDON – For 60 years, successive German governments sought a more European Germany. But now, Chancellor Angela Merkel’s administration wants to reshape Europe’s economies in Germany’s image.
This is politically unwise and economically dangerous. Far from being Europe’s most successful economy – as German Finance Minister Wolfgang Schäuble and others boast – Germany’s economy is dysfunctional.
To be sure, Germany has its strengths: world-renowned companies, low unemployment, and an excellent credit rating. But it also has stagnant wages, busted banks, inadequate investment, weak productivity gains, dismal demographics, and anemic output growth.
Its “beggar-thy-neighbor” economic model – suppressing wages to subsidize exports – should not serve as an example for the rest of the eurozone to follow.
Germany’s economy contracted in the second quarter of 2014, and has grown by a mere 3.6% since the 2008 global financial crisis – slightly more than France and the United Kingdom, but less than half the rate in Sweden, Switzerland, and the United States.
Since 2000, GDP growth has averaged just 1.1% annually, ranking 13th in the 18-member eurozone.
Written off as the “sick man of Europe” when the euro was launched in 1999, Germany responded not by boosting dynamism, but by cutting costs.
Investment has fallen from 22.3% of GDP in 2000 to 17% in 2013.
Infrastructure, such as highways, bridges, and even the Kiel Canal, is crumbling after years of neglect.
The education system is creaking: the number of new apprentices is at a post-reunification low, the country has fewer young graduates (29%) than Greece (34%), and its best universities barely scrape into the global top 50.
Hobbled by underinvestment, Germany’s arthritic economy struggles to adapt.
Despite former Chancellor Gerhard Schröder’s labor-market reforms, it is harder to lay off a permanent employee in Germany than anywhere else in the OECD. Germany languishes in 111th place globally for ease of starting a business, according to the World Bank’s Doing Business rankings.
Its largest firms are old and entrenched; it has produced no equivalent of Google or Facebook; and the service sector is particularly hidebound.
The government has introduced fewer pro-growth reforms over the past seven years than any other advanced economy, according to the OECD.
Average annual productivity growth over the past decade, at a mere 0.9%, has been slower even than Portugal’s.
The brunt of the stagnation has been borne by German workers.
Though their productivity has risen by 17.8% over the past 15 years, they now earn less in real terms than in 1999, when a tripartite agreement among the government, companies, and unions effectively capped wages.
Business owners might cheer, but suppressing wages harms the economy’s longer-term prospects by discouraging workers from upgrading skills, and companies from investing in higher-value production.
Wage compression saps domestic demand, while subsidizing exports, on which Germany’s growth relies.
The euro, which is undoubtedly much weaker than the Deutschmark would have been, has also helped, by reducing the prices of German goods and preventing France and Italy from pursuing currency depreciation.
Until recently, the euro also provided booming external demand in southern Europe, while China’s breakneck industrial development raised demand for Germany’s traditional exports.
But, with southern Europe now depressed, and China’s economy decelerating and shifting away from investment spending, the German export machine has slowed.
Its share of global exports fell from 9.1% in 2007 to 8% in 2013 – as low as in the “sick man” era, when Germany was struggling with reunification.
Because cars and other exports “made in Germany” now contain many parts produced in central and eastern Europe, Germany’s share of global exports is at a record low in value-added terms.
German policymakers pride themselves on the country’s vast current-account surplus – €197 billion ($262 billion) as of June 2014 – viewing it as a sign of Germany’s superior competitiveness.
Why, then, are businesses unwilling to invest more in the country?
External surpluses are in fact symptomatic of an ailing economy.
Stagnant wages boost corporate surpluses, while subdued spending, a stifled service sector, and stunted start-ups suppress domestic investment, with the resulting surplus savings often squandered overseas.
The Berlin-based DIW institute calculates that from 2006 to 2012, the value of Germany’s foreign portfolio holdings fell by €600 billion, or 22% of GDP.
Worse, rather than being an “anchor of stability” for the eurozone, as Schäuble claims, Germany spreads instability.
Its banks’ poor approach to lending their surplus savings inflated asset-price bubbles in the run-up to the financial crisis, and have imposed debt deflation since then.
Nor is Germany a “growth engine” for the eurozone.
In fact, its weak domestic demand has dampened growth elsewhere.
As a result, German banks and taxpayers are less likely to recover their bad loans to southern Europe.
Given how bad wage compression has been for Germany’s economy, foisting wage cuts on the rest of the eurozone would be disastrous.
Slashing incomes depresses domestic spending and makes debts even less manageable.
With global demand weak, the eurozone as a whole cannot rely on exports to grow out of its debts.
For struggling southern European economies whose traditional exports have been undercut by Chinese and Turkish competition, the solution is to invest in moving up the value chain by producing new and better products.
Germany’s economy needs an overhaul.
Policymakers should focus on boosting productivity, not “competitiveness,” with workers being paid their due.
The government should take advantage of near-zero interest rates to invest, and encourage businesses – especially start-ups – to do likewise.
Finally, Germany should welcome more dynamic young immigrants to stem its demographic decline.
This would be a better economic model for Germany.
It would also set the right example for the rest of Europe.
Europe’s Bogus Banking Union
LONDON – After a 16-hour marathon negotiating session ending on March 20, politicians, technocrats, and journalists were all keen to declare the deal on the final piece of Europe’s banking union a success.
But appearances are deceptive.
While the “banking union” may soon exist on paper, in practice the eurozone banking system is likely to remain fragmented along national lines and divided between a northern “core,” where governments continue to stand behind local banks, and a southern “periphery,” where governments have run out of money.
Think back to June 2012.
Spain’s busted banks threatened to drag down the Spanish state, as Ireland’s had done to the Irish state 18 months earlier, while panic tore through the eurozone.
European Union leaders resolved to break the link between weak banks and cash-strapped governments.
A European banking union would move responsibility for dealing with bank failures to the eurozone level – akin to America, where distressed banks in, say, Florida are dealt with by federal authorities with the power to bail in bondholders, inject federal funds, and close down financial institutions.
But, a month later, the European Central Bank finally intervened to quell the panic.
That saved the euro, but it also relieved the pressure on Germany to cede control of its oft-distressed banks.
Since then, the German government has used its clout to eviscerate the proposed banking union; all that remains is a shell to keep up appearances.