The flood of people is exposing new rifts among EU members, raising questions about the principle of open borders and free movement that has long been at the core of the EU.
Germany and a few other countries have stepped up to the challenge in ways that are admirable but unsustainable.
Some 8,000 refugees a day – a modern day Volkerwanderung – are entering Germany, partly because of harsh conditions back home, and partly because of Germany’s willingness to take them in.
The challenge of caring for, employing, and integrating such numbers will soon run up against the limits of physical capacity, financial resources, and public tolerance.
It is obvious that public policy cannot succeed if it is focused on the consequences, rather than the causes, of the refugee crisis.
The change that would have the greatest positive impact would be the emergence of a new government in Damascus that was acceptable to the bulk of the Syrian people and a satisfactory partner for the United States and Europe.
Unfortunately, this seems likely to come about only with the blessing of Russia and Iran, both of which appear more inclined to increase their support for President Bashar al-Assad than to work for his removal.
Other steps, however, would improve the situation.
Increased international financial support for countries in Europe or the Middle East hosting large numbers of refugees is one.
Ideally, such funding would help persuade more countries to follow Germany’s example.
Another useful development would be the creation of enclaves inside Syria where people could gather with some expectation of security.
Such enclaves would require local support from Kurdish forces or select Arab tribes, with military backing by the US and others.
A new comprehensive arrangement with Turkey is also needed to reduce the flow of jihadist recruits to Syria and the number of refugees heading north.
Turkey would receive financial and military assistance in exchange for asserting greater control over its borders, while the question of Turkey’s long-term relationship with Europe would be set aside until the crisis passed.
The US has a special obligation to help.
Both by what it has done and what it has failed to do in Iraq, Syria, and elsewhere in the Middle East, American foreign policy bears more than a little responsibility for outcomes that have led to the refugee exodus.
The US also has a strategic interest in helping Germany and Europe contend with this crisis.
Europe still constitutes a quarter of the world’s economy and remains one of America’s principal geopolitical partners.
A Europe overwhelmed by a demographic challenge, in addition to its economic and security challenges, would be neither able nor willing to be an effective ally.
In all of this, time is of the essence.
Europe – and Germany in particular – cannot sustain the status quo.
Waiting for a solution to the Syrian situation is no answer; while lesser steps will not resolve Europe’s predicament, they could make it manageable.
Africa’s Urban Challenge
NAIROBI – My mother, like her mother, her grandmother, and so on, was born into poverty in the rural village of Rarieda, Kenya.
I, too, was born in the village, and lived there until it was struck by a brutal famine when I was two years old.
With no food, money, or opportunities, my mother did what thousands of African villagers do every day: she moved us to the city in search of a better life.
But, given the lack of jobs and housing in Nairobi, we ended up in Kibera, one of Africa’s largest slums.
Located just a couple of miles from downtown Nairobi, Kibera is a heavily polluted, densely populated settlement composed of informal roads and shacks with corrugated tin roofs.
Kenya’s government does not recognize Kibera, there is no sewage system or formal power grid.
Its residents, estimated to number anywhere from a few hundred thousand to more than a million, do not officially exist.
Kibera is just one example of the consequences of the rapid urbanization that is gaining momentum worldwide.
More than 44% of developing-country residents already live in cities.
The Population Reference Bureau estimates that by 2050, only 30% of the global population will remain in rural areas.
But few have stopped to consider this shift’s implications for families like mine.
When people think of Africa, they often focus on the hardships of village life – a perception reflected in iconic images of African women on their daily excursions to fetch water.
But an increasing number of people – already nearly 300 million – are facing the harsh reality of the urban slum, where resources are scarce and economic opportunities are elusive.
More than 78% of the urban population in the world’s least-developed countries, and one-third of the global urban population, lives in slums.
Nairobi is a dynamic and growing city, with shopping centers, restaurants, and Western-style companies catering to Kenya’s emerging middle class.
Yet no one knows how many people live there.
According to the last (highly politicized) census, completed in 2009, Nairobi has a population of over three million; but it is probably closer to five million, with a large percentage living in slums.
It is these people, Nairobi’s poorest residents, who build the buildings, staff the restaurants, drive the taxis, and power the city.
(From the age of 12 until I was 22, I was part of this group, working at construction sites and in factories.)&#160;Indeed, without the poor, Nairobi could not function for a single day.
Nevertheless, they remain all but invisible, with no political voice.
The world’s enduring perception of Africa as a village exacerbates slum dwellers’ plight, keeping them off the global development agenda.
Every day, more people arrive in Nairobi, lured by the promise of employment, resources, and a better life, only to realize that they are not equipped to survive there and that their children will grow up in a slum.
At least half of those living in urban slums are under the age of 20.
Without access to education, this generation – which will soon be the majority – has little hope of ever escaping its straitened conditions.
But for how long will a majority serve a minority?&#160;For how long will it accept a lack of water, sanitation, education, and dignity?
Urban slums worldwide will soon reach a tipping point, with young people rejecting the lives that they have been offered.
Their power lies in their numbers – more than half of the world’s youth shares their fate – and in their anger.
They will rise up, refusing to accept their status as second-class citizens of ever-expanding urban settlements, and they will destabilize countries like Kenya, undermining efforts to build more stable, prosperous societies.
Cities are not just Africa’s future; they are its present.
Unless collective action is taken now to transform cities like Nairobi into the drivers of economic development and sources of opportunity that they are supposed to be, they will become a tinderbox of perpetual inequality.
For the sake of the millions of people like my mother – and, more important, for the sake of their children and grandchildren – we must fulfill the promise that attracts the poor to cities in the first place.
Mitt and the Moochers
WASHINGTON, DC – The Republican Party has some potentially winning themes for America’s presidential and congressional elections in November.
Americans have long been skeptical of government, with a tradition of resistance to perceived government overreach that extends back to their country’s founding years.
This tradition has bequeathed to today’s Americans a related rejection of public subsidies and a cultural aversion to “dependence” on state support.
But Republican presidential candidate Mitt Romney and other leading members of his party have played these cards completely wrong in this election cycle.
Romney is apparently taken with the idea that many Americans, the so-called 47%, do not pay federal income tax.
He believes that they view themselves as “victims” and have become “dependent” on the government.
But this misses two obvious points.
First, most of the 47% pay a great deal of tax on their earnings, property, and goods purchased.
They also work hard to make a living in a country where median household income has declined to a level last seen in the mid-1990’s.
Second, the really big subsidies in modern America flow to a part of its financial elite – the privileged few who are in charge of the biggest firms on Wall Street.
Seen in broad historical perspective, this is not such an unusual situation.
In their recent bestselling economic history, Why Nations Fail, Daron Acemoglu and James Robinson cite many past and current cases in which powerful individuals attain control over the state and use this power to enrich themselves.
In many pre-industrial societies, for example, control over the state was the best way to assure wealth.
And, in many developing countries endowed with valuable natural resources, fighting to gain control of the government has proved a very attractive strategy.
(I have worked with Acemoglu and Robinson on related issues, though I was not involved in writing the book.)
The traditional mechanism of state capture in much of the world is violence.
But that is not true in the United States.
Nor is it the case that US government officials are typically bribed in an open fashion (though there have been some prominent exceptions).
Instead, special interests compete for influence through campaign contributions and other forms of political donations.
They also run large, sophisticated media campaigns aimed at persuading policymakers and the public that what is good for their special interest is good for the country.
No one has succeeded in the modern American political game like the biggest banks on Wall Street, which lobbied for deregulation during the three decades prior to the crisis of 2008, and then pushed back effectively against almost all dimensions of financial reform.
Their success has paid off handsomely.
The top executives at 14 leading financial firms received cash compensation (as salary, bonus, and/or stock options exercised) totaling roughly $2.5 billion in 2000-2008 – with five individuals alone receiving $2 billion.
But these masters of the universe did not earn that money without massive government assistance.
By being perceived as “too big to fail,” their banks benefit from a government backstop or downside guarantee.
They can take on more risk – running a more highly leveraged business with less shareholder capital.
They get bigger returns when things go well and receive state support when fortune turns against them: heads they win, tails we lose.
And the losses are colossal.
According to a recent report on the aftermath of the 2008 crisis, prepared by Better Markets, an advocacy group that pushes for stronger financial reforms, the cost to the US economy of the financial crisis – caused by financial institutions’ reckless risk-taking – amounts to at least $12.8 trillion.
A big part of this cost has come in the form of jobs lost and lives derailed for the bottom 47% of the American income distribution.
Former Utah Governor and Republican presidential candidate Jon Huntsman addressed this issue clearly and repeatedly as he sought – unsuccessfully – to win his party’s nomination to challenge President Barack Obama.
Force the banks to break up, he argued, in order to cut off their subsidies.
Make these financial institutions small enough and simple enough to fail – then let the market decide which of them should sink or swim.
That is an argument around which all conservatives should be able to rally.
After all, the emergence of global megabanks was not a market outcome; these banks are government-sponsored and subsidized enterprises, propped up by taxpayers.
(This is as true in Europe today as it is in the US.)
Romney is right to raise the issue of subsidies, but he badly misstates what has happened in the US during the last four years.
The big, nontransparent, and dangerous subsidies are off-budget, contingent liabilities generated by government support for too-big-to-fail financial institutions.&nbsp; These subsidies do not appear in any annual appropriation, and they are not well measured by the government – which is part of what makes them so appealing to the big banks and so damaging to everyone else.
If only Romney had turned popular disdain for subsidies against the global megabanks, he would now be coasting into the White House.
Instead, by going after the hard-pressed 47% of America – the very people who have been hurt the most by reckless bank behavior – his prospect of victory in November has been severely damaged.
Mitt Romney and America’s Four Deficits
BERKELEY – The United States is beset by four deficits: a fiscal deficit, a jobs deficit, a deficit in public investment, and an opportunity deficit.
The budget proposals put forward by presidential candidate Mitt Romney and his running mate, Paul Ryan, could reduce the fiscal deficit, but would exacerbate the other three.
To be sure, Romney and Ryan have failed to provide specifics about how they would reduce the fiscal deficit, relying on “trust me” assertions.
But the overarching direction of their proposals is clear: more tax cuts, disproportionately benefiting those at the top, coupled with significantly lower non-defense discretionary spending, disproportionately hurting everybody else – and weakening the economy’s growth prospects.
Despite 30 months of private-sector job growth, the US still confronts a large jobs deficit. The unemployment rate remains more than two percentage points above the “normal” rate (when the economy is operating near capacity).
Moreover, the labor-force participation rate remains near historic lows.
More than 11 million additional jobs are needed to return the US to its pre-recession employment level.
At the current pace of recovery, that is more than eight years away.
In the meantime, persistent high unemployment reduces the economy’s growth potential by robbing today’s workers of skills and experience.
When weak aggregate demand causes the economy to operate far below its potential, cuts in government spending enlarge the jobs deficit.
Indeed, in his recent speech in Jackson Hole, Wyoming, US Federal Reserve Chairman Ben Bernanke warned that such cuts significantly inhibit job creation.
Without revealing which programs he would reduce, Romney promises to slash federal spending by more than $500 billion in 2016, capping it at 20% of GDP thereafter.
He also promises an immediate 5% cut in non-defense discretionary spending in 2013, on top of the huge cuts already scheduled to take effect.
And he has ruled out additional temporary fiscal measures aimed at job creation, like President Barack Obama’s proposals for additional grants to states and additional infrastructure spending.
Romney acknowledges that large spending cuts, along with the scheduled expiration of tax cuts at the end of this year, could throw the economy back into recession in 2013.
But he vows to steer the economy from the fiscal cliff by extending the tax cuts enacted under George W. Bush, doubling down with a further 20% across-the-board cut in income-tax rates and cutting the corporate rate from 35% to 25%.
With the possible exception of the extension of the Bush-era tax cuts, these changes would take considerable time to implement.
Even when enacted, their near-term effects on job creation would be minimal.
An across-the-board reduction in tax rates performs poorly in terms of budgetary effectiveness (the number of jobs created per dollar of foregone revenue).
Payroll-tax relief and spending on programs like food stamps and unemployment compensation are much more effective.
Romney overstates his tax proposals’ long-term growth effects as well.
Reducing individual tax rates and taxes on savings and investment at best fosters modest increases in employment, work effort, and income.
Despite the Bush-era tax cuts, the 2001-2007 expansion was the worst of the post-war period in terms of investment, employment, wage, and GDP growth.
Job creation and growth were much stronger following President Bill Clinton’s tax increases in the 1990’s.
Moreover, if all of Romney’s additional tax cuts were financed in a revenue-neutral way, as he promises, only the composition of taxes would change; the overall tax share of GDP would not.
There is no evidence that this would significantly boost growth, as Romney claims.
Based on what Romney has told us, we can conclude that his plan would exacerbate the public-investment deficit as well.
Romney’s vow to cap federal spending at 20% of GDP by 2016, while maintaining defense spending at 4% of GDP and leaving both Social Security and Medicare unchanged for those 55 or older, implies exempting more than 50% of government spending from cuts for the next decade.
So, to hit the 20% cap, spending on everything else would have to be slashed by an average of roughly 40% by 2016 and 57% by 2022.
Everything else includes government investments in three major areas on which growth and high-wage jobs depend: education, infrastructure, and research. These areas account for less than 8% of federal spending, and their share has been declining steadily.
Under Romney, it would plummet to new lows.
Everything else also includes spending on programs that help low-income families, like food stamps, student grants, and Medicaid.
The Center on Budget and Policy Priorities finds that almost two-thirds of the Ryan budget’s spending cuts would come from such programs.
Romney offers few specifics, but simple arithmetic shows that his plan would require even deeper cuts in these programs than Ryan’s plan would.
Meanwhile, Romney’s plan would actually increase taxes on middle-income families.
His plan would pay for lower income-tax rates by eliminating tax deductions like those for charitable giving and mortgages, while maintaining tax preferences for saving and investment.
But there are not enough tax breaks for the rich to cover another 20% reduction in their income-tax rate.
That is why the nonpartisan Tax Policy Center found that Romney’s plan would cut overall taxes for households with incomes above $200,000, but would require an average annual tax increase of at least $2,000 for households with incomes between $100,000 and $200,000.
Romney’s budget plan would also make the federal tax-and-transfer system considerably less progressive, thereby worsening income inequality, which is already at its highest level since the Great Depression.
Rising income inequality fuels a growing opportunity deficit for children born into poor and middle-income families, reflected in disparities in educational attainment by family background, and a decline in intergenerational mobility.
Under Romney, the opportunity deficit would widen, robbing the country of future talent and productivity.
Romney has provided few details about his deficit-reduction plan.
But, based on what he has revealed, we know that it would increase the jobs deficit, the investment deficit, and the opportunity deficit, with negative consequences for future growth and prosperity.
Mitt Romney’s Reality Check
NEW YORK – There is a kind of war underway in the United States nowadays between fact and fantasy.
President Barack Obama’s re-election marked a victory, limited but unmistakable, for the cause of fact.
Events in the days leading up to America’s presidential election provided a stark illustration of the struggle.
Among senior aides to Republican challenger Mitt Romney, a belief developed that he was on the cusp of victory.
Their conviction had no basis in poll results.
Nevertheless, the feeling grew so strong that aides began to address Romney as “Mr. President.”
But wanting that to be true was not enough for them to make it true.
It would be as close to becoming President as Romney would get, and he apparently wanted to enjoy it while he could, however prematurely.
Then, on election night, when the television networks projected Romney’s defeat in Ohio and therefore Barack Obama’s re-election, the Romney campaign, in a further denial of fact, refused to accept the result.
A very awkward hour passed before he accepted reality and made a gracious concession speech.
The same disregard for reality has been the hallmark not only of the Republican campaign but of the entire Republican Party in recent times.
When the Bureau of Labor Statistics issued a report in October showing that the national unemployment rate remained “essentially unchanged at 7.9%,” Republican operatives sought to discredit the highly respected BLS.
When polls showed that Romney was falling behind President Barack Obama, they sought to discredit the polls.
When the non-partisan Congressional Research Service reported that a Republican tax plan would do nothing to foster economic growth, Republican Senators muscled the CRS into withdrawing its report.
These refusals to accept matters of plain fact reflect a still wider pattern.
After all, if left unchecked, global warming has the potential to degrade and destroy the climactic conditions that underlay and made possible the rise of human civilization over the last ten millennia.
Romney, as governor of Massachusetts, had expressed belief in the reality of global warming.
As a presidential candidate, however, he joined the deniers – a switch made clear when he accepted the party’s nomination in Tampa, Florida, in August.
“President Obama promised to begin to slow the rise of the oceans,” Romney told the Republican convention, and then paused, with the expectant smile of a comedian waiting for the audience to catch on to the joke.
It did.
Laughter broke and built.
Romney let it grow, then delivered the punch line:&nbsp; “And to heal the planet.”
The crowd cracked up.
It was perhaps the most memorable and lamentable moment in a lamentable campaign – a moment that, in the history now to be written of humanity’s effort to preserve a livable planet, is destined for immortal notoriety.
There was an astonishing sequel.
Eight weeks later, Hurricane Sandy struck the New Jersey shore and New York City.
Its 14-foot surge of seawater was backed by the sea-level rise already caused by a century of global warming, and the storm’s sweep and intensity was fueled by a warming planet’s warmer ocean waters.
That tide of reality – what Alexander Solzhenitsyn once called “the pitiless crowbar of events”— burst the closed bubble of Romney’s campaign, its walls breached as decisively as those of lower Manhattan and Far Rockaway.
In the contest between fact and fantasy, fact suddenly had a powerful ally.
The political map was subtly but consequentially redrawn.
Obama swung into action, now not just a suspect candidate but a trusted president whose services were sorely needed by the battered population of the East Coast.
Eight of ten voters, as polls showed, viewed his performance favorably, and many declared that the impression influenced their vote.
In a surprising, politically potent twist, New Jersey’s governor, Chris Christie, who had been the keynote speaker at the Republican convention at which Romney had mocked the dangers of global warming, turned out to be one of those impressed with Obama’s performance, and said so publicly.
The American political world – not only Republicans, but also Democrats (albeit to a lesser extent) – had fenced out huge, ominous realities.
But those realities, as if listening and responding, entered the fray.
They voted early, and they may very well have swayed the outcome.
Earth spoke, and Americans, for once, listened.
Mitt Romney’s Fair Share
NEW YORK – Mitt Romney’s income taxes have become a major issue in the American presidential campaign.
Is this just petty politics, or does it really matter?
In fact, it does matter – and not just for Americans.
A major theme of the underlying political debate in the United States is the role of the state and the need for collective action.
The private sector, while central in a modern economy, cannot ensure its success alone.
For example, the financial crisis that began in 2008 demonstrated the need for adequate regulation.
Moreover, beyond effective regulation (including ensuring a level playing field for competition), modern economies are founded on technological innovation, which in turn presupposes basic research funded by government.
This is an example of a public good – things from which we all benefit, but that would be undersupplied (or not supplied at all) were we to rely on the private sector.
Conservative politicians in the US underestimate the importance of publicly provided education, technology, and infrastructure.
Economies in which government provides these public goods perform far better than those in which it does not.
But public goods must be paid for, and it is imperative that everyone pays their fair share.
While there may be disagreement about what that entails, those at the top of the income distribution who pay 15% of their reported income (money accruing in tax shelters in the Cayman Islands and other tax havens may not be reported to US authorities) clearly are not paying their fair share.
There is an old adage that a fish rots from the head.
If presidents and those around them do not pay their fair share of taxes, how can we expect that anyone else will?
And if no one does, how can we expect to finance the public goods that we need?
Democracies rely on a spirit of trust and cooperation in paying taxes.
If every individual devoted as much energy and resources as the rich do to avoiding their fair share of taxes, the tax system either would collapse, or would have to be replaced by a far more intrusive and coercive scheme.
Both alternatives are unacceptable.
More broadly, a market economy could not work if every contract had to be enforced through legal action.
But trust and cooperation can survive only if there is a belief that the system is fair.
Recent research has shown that a belief that the economic system is unfair undermines both cooperation and effort.
Yet, increasingly, Americans are coming to believe that their economic system is unfair; and the tax system is emblematic of that sense of injustice.
The billionaire investor Warren Buffett argues that he should pay only the taxes that he must, but that there is something fundamentally wrong with a system that taxes his income at a lower rate than his secretary is required to pay.
He is right.
Romney might be forgiven were he to take a similar position.
Indeed, it might be a Nixon-in-China moment: a wealthy politician at the pinnacle of power advocating higher taxes for the rich could change the course of history.
But Romney has not chosen to do so.
He evidently does not recognize that a system that taxes speculation at a lower rate than hard work distorts the economy.
Indeed, much of the money that accrues to those at the top is what economists call rents, which arise not from increasing the size of the economic pie, but from grabbing a larger slice of the existing pie.
Those at the top include a disproportionate number of monopolists who increase their income by restricting production and engaging in anti-competitive practices; CEOs who exploit deficiencies in corporate-governance laws to grab a larger share of corporate revenues for themselves (leaving less for workers); and bankers who have engaged in predatory lending and abusive credit-card practices (often targeting poor and middle-class households).
It is perhaps no accident that rent-seeking and inequality have increased as top tax rates have fallen, regulations have been eviscerated, and enforcement of existing rules has been weakened: the opportunity and returns from rent-seeking have increased.
Today, a deficiency of aggregate demand afflicts almost all advanced countries, leading to high unemployment, lower wages, greater inequality, and – coming full, vicious circle – constrained consumption.
There is now a growing recognition of the link between inequality and economic instability and weakness.
There is another vicious circle: Economic inequality translates into political inequality, which in turn reinforces the former, including through a tax system that allows people like Romney – who insists that he has been subject to an income-tax rate of “at least 13%” for the last ten years – not to pay their fair share.
The resulting economic inequality – a result of politics as much as market forces – contributes to today’s overall economic weakness.
Romney may not be a tax evader; only a thorough investigation by the US Internal Revenue Service could reach that conclusion.
But, given that the top US marginal income-tax rate is 35%, he certainly is a tax avoider on a grand scale.
And, of course, the problem is not just Romney; writ large, his level of tax avoidance makes it difficult to finance the public goods without which a modern economy cannot flourish.
But, even more important, tax avoidance on Romney’s scale undermines belief in the system’s fundamental fairness, and thus weakens the bonds that hold a society together.
Did Hamas Win?
GAZA CITY – This summer’s 51-day war on Gaza left more than 2,100 Palestinians dead, over 11,000 injured, and vast areas of devastation that will take years to rebuild.
After the third Israeli war on Gaza in less than six years, many Palestinians are questioning the purpose of continuing to fight – and hoping for a solution that does not increase their suffering.
Can Hamas, with its newly acquired position at the forefront of Palestinian politics, provide such a solution?
Before the latest war erupted, Hamas was politically isolated.
It had lost traditional allies in Syria, Iran, and Hezbollah.
Most damaging, the ouster of former Egyptian President Mohamed Morsi’s Muslim Brotherhood government had deprived Hamas of its lifeline of supplies and armaments.
Egypt’s military regime, led by General Abdel Fattah el-Sisi, has been unrelentingly hostile toward Hamas, blaming it for the fighting in Sinai between the army and insurgent groups.
Egypt even mounted an operation to destroy the tunnels between Gaza and Sinai, isolating Gaza completely.
Hamas faced an intensifying crisis.
Unable to pay the salaries of more than 40,000 public employees in Gaza, it was being slowly strangled by the Israeli and Egyptian authorities.
And the unity government that it established with the Palestinian Authority in June brought no relief.
With nothing to lose, Hamas decided that another round of fighting with Israel was the only way to shake things up.
Despite its modest military capabilities, Hamas managed to hold out for 51 days – and, in the process, place itself at the center of Palestinian and regional politics.
Israel, by contrast, failed to achieve any of its goals – beginning with restoring its deterrent capacity.
Indeed, despite Israel’s best efforts, Hamas continued to launch long-range missiles at major populated areas from Haifa in the north to Ashkelon and Dimona in the south, and it repeatedly crossed Israeli lines using underground tunnels.
Such achievements shattered the indomitable image of the Israeli army, exposing a weakness that other radical Islamist groups may attempt to exploit.
Against this background, it is perhaps unsurprising that Hamas managed to compel most Israelis living in areas adjacent to Gaza to flee, with many Israelis accusing their government of failing to protect its citizens adequately.
In short, the war in Gaza shook the status quo.
It did not, however, bring about any progress toward resolving the outstanding issues underlying the dispute between Israel and Hamas, or change the conditions that spurred the latest conflict in the first place.
Israel did agree to terms much like those that ended its last assault on Gaza in 2012; but that agreement was never implemented.
Israel is now expected, for example, to ease its blockade of Gaza and allow the transport of humanitarian and construction supplies.
But more complicated issues, such as the release of Palestinian prisoners and the establishment of a Gaza airport and seaport, will be left for next month’s discussions.
And there is no guarantee that Israel will accede to Hamas’s demands without disarming Gaza.
The stakes for Hamas could not be higher.
By breaking Gaza’s political isolation, the ceasefire has fueled hope of relief from economic and financial deprivation.
Given Hamas’s leading role, it will be held accountable not only for the success of Gaza’s reconstruction, but also for any further delays in Palestinian reconciliation.
Hamas also faces pressure from the international community, which, despite supporting the Palestinians’ demand for an end to the Israeli siege and blockade, is adamant that Israel’s security concerns also be addressed.
As United Nations Secretary-General Ban Ki-moon warned, “Any peace effort that does not tackle the root causes of the crisis will do little other than set the stage for the next cycle of violence.”
In other words, achieving a lasting peace deal will require compromises from both sides – the kind of compromises that Hamas has long resisted.
For its part, the international community must embrace Hamas’s involvement in the pursuit of a peaceful settlement of the Palestine-Israel conflict.
Former Israeli President Shimon Peres once said, “The real challenge is to transform any crisis, however large, into new opportunities for action.”
It is time for all of the relevant actors in the conflict between Israel and Palestine to address this challenge, and to take concrete, productive, and creative action to bring peace to Gaza at last.
Hamas has proved its staying power.
After decades of standoffs and stalemate, perhaps it can focus less on its own survival and more on helping to achieve a just and lasting peace.
The Promise of Digital Finance
BERKELEY – An economic development revolution lies literally in the palm of a single hand.
As mobile phones and digital technologies rapidly spread around the world, their implications for economic development, and particularly finance, have yet to be fully realized.
The sooner that changes, the better for people worldwide.
In emerging economies today, two billion people – 45% of all adults – do not have a formal account at a bank, financial institution, or with a mobile-money provider.
The “unbanked” rate is even higher for women, the poor, and people living in rural areas.
Moreover, at least 200 million small- and medium-size enterprises lack sufficient credit, or have no access to credit at all.
Entrepreneurship, investment, and economic growth suffer when savings are stored outside the financial system, and credit is scarce and expensive.
Fortunately, according to a recent report by the McKinsey Global Institute (MGI), digital technologies – starting with mobile phones – can rapidly fix this problem and foster faster, more inclusive growth.
Mobile phones and the Internet can reduce the need for cash and bypass traditional brick-and-mortar channels.
This dramatically reduces financial-service providers’ costs, and makes their services more convenient and accessible for users – especially low-income users in remote locations.
MGI estimates that if digital finance is widely adopted, it could add $3.7 trillion to emerging countries’ GDP by 2025.
That amounts to a 6% increase above business as usual.
In low-income countries with very low financial inclusion rates, such as Nigeria, Ethiopia, and India, GDP could increase by as much as 12%.
Digital finance can boost GDP in several ways.
Nearly two-thirds of the expected growth would come from increased productivity, because businesses, financial-service providers, and government organizations would be able to operate much more efficiently if they did not have to rely on cash and paper recordkeeping.
Another one-third would come from increased investment throughout the economy, as personal and business savings were moved into the formal financial system, and then mobilized to provide more credit.
The remaining gains would come from people working more hours – the time they would have spent traveling to bank branches and waiting in queues.
As for financial inclusion, digital finance has two positive effects.
First, it expands access.
In emerging markets in 2014, only about 55% of adults had a bank or financial-services account, but nearly 80% had a mobile phone.
That 25-percentage-point gap could be closed by making mobile banking and digital wallets a reality.
But a gender gap will also have to be closed: worldwide, about 200 million fewer women than men have mobile phones or Internet access.
Second, digital finance reduces costs: MGI estimates that it would cost financial-service providers 80-90% less – about $10 per year, compared to the $100 per year it costs today – to offer customers digital accounts than accounts through traditional bank branches.
Using purely digital channels thus makes it feasible to meet the needs of low-income customers.
Financial inclusion becomes profitable for providers even when account balances and transactions are small.
With digital finance, as many as 1.6 billion unbanked people – more than half of whom are women – could gain access to financial services, shifting about $4.2 trillion in cash and savings currently held in informal vehicles into the formal financial system.
This would allow for an additional $2.1 trillion to be extended as credit to individuals and small businesses.
Businesses could also save on labor costs, to the tune of 25 billion hours annually, by swapping cash transactions for digital payments.
And governments could take in an additional $110 billion every year – to invest in growth-enhancing public goods like education – because digital channels make tax collection cheaper and more reliable.
New mobile-money services are already demonstrating digital finance’s potential.
In Kenya, M-Pesa – which transforms one’s phone into a mobile wallet – has leveraged powerful network effects to bring about a vast expansion in the share of adults using digital financial services.
That share grew from zero to 40% in just three years, and had risen to 68% by the end of last year.
Traditional financial-services accounts tend to grow at the pace of national income, but M-Pesa’s adoption rate has been dramatically faster, demonstrating that digital finance can achieve significant market penetration rapidly even in the world’s poorest countries.
But such success stories do not happen in a vacuum.
For starters, everyone needs a mobile phone with an affordable data plan.
While businesses can help, it is incumbent upon governments and non-governmental organizations to extend mobile networks to low-return areas and remote populations.
Governments must also ensure that networks between banks and telecommunications companies are interoperable; otherwise, widespread use of mobile phones for financial services and payments would be impossible.
Governments must establish universally accepted forms of identity as well, so that service providers can control fraud.
In emerging economies, one in five people are unregistered, compared to only one in ten in advanced economies.
Nearly 20% of unbanked women in emerging countries do not have the documentation necessary to open a bank account.
Even when people have recognized IDs, they must be amenable to digital authentication.
Digital IDs that use microchips, fingerprints, or iris scans could prove useful – and are already gaining popularity – in emerging economies.
Finally, governments must implement regulations that strike a balance between protecting investors and consumers, and giving banks, retailers, and financial-technology and telecommunications companies room to compete and innovate.
Because regulations often shut out non-bank competitors, governments should consider a tiered approach, whereby businesses without a full banking license can provide basic financial products to customers with smaller accounts.
A good model for this is the United Kingdom’s “regulatory sandbox” for financial-technology companies, which imposes lower regulatory requirements on emerging players until they reach a certain size.
Financial inclusion is vital for inclusive economic growth and gender equality, and it has assumed a prominent role in global development efforts, with the World Bank aiming for universal financial inclusion by 2020.
With billions of people in emerging economies already using mobile phones, digital finance makes this goal achievable.
The Changing Development Order
DUBAI – The Millennium Development Goals established a successful framework for the world to address fundamental social issues such as poverty, health, hunger, and education.
As discussions commence on the shape and scope of the global development agenda that will succeed the MDGs, which expire in 2015, it would be helpful to consider the role of the private sector and rethink the international community’s overall approach to development.
Economic development is the best way – indeed, the only way – to achieve sustainable poverty reduction.
It creates a virtuous circle.
Growth creates jobs, and jobs reduce poverty.
The private sector has a key role to play.
Private-sector capital flows now dwarf traditional public-sector aid flows.
For example, of the $200 billion in total US resources dedicated to development in 2010, 87% came from private flows.
By contrast, in the 1960’s, official overseas development assistance accounted for 70% of capital flows into developing countries.
A similar picture prevails globally.
Domestic resource mobilization, remittances from expatriate workers, private debt and equity flows, and philanthropic contributions exceed official international aid by a wide margin.
Private flows are no longer the tail, but the dog that wags the development agenda.
Nonetheless, much of the development-policy community remains stuck in the distant past.
For example, policymakers insist on the importance of “public-private partnerships” and argue that the private sector needs “to learn to work with the public sector.”
But today’s reality would be better described as “private-philanthropic-public partnerships” (expressed in that order to reflect each component’s relative importance), or “P-4.”
We need to persuade public institutions to focus on how to work better with their private counterparts, not vice versa, because the public and private sectors have a shared interest in accelerating economic development and ensuring that everyone benefits from globalization.
This is not meant to diminish the important role played by the public sector, which alone can create the conditions – the rule of law, sound macroeconomic policies, and good regulatory regimes – needed for the private sector to flourish.
For example, they catalyze the development of supportive property and customs regimes, including the establishment of credit bureaus and laws to protect creditors’ rights – all necessary prerequisites to channel financing flows.
Possibly the biggest prize in aligning private- and public-sector development efforts lies in the relatively unexplored area of blended finance.
We have barely scratched the surface in integrating the efforts of development finance institutions (DFIs) with private and philanthropic initiatives, which could make the whole greater than the sum of its parts.
A World Economic Forum study estimates that, when aligned, an annual increase of only $36 billion in public-sector investment in climate change could be leveraged to 16 times that amount by mobilizing $570 billion of private capital.
In fact, to improve alignment and reflect the new P-4 order, government agencies and DFIs should be encouraged to establish explicit targets for leveraging private capital.
Particularly in an age of lean governments and public austerity, success in meeting such targets should become a key performance indicator.
What should the private sector do better?
While there are many examples of responsible companies that want to “do well by doing good,” sustainability and development goals are not always integrated into businesses’ core agenda.
Total shareholder return (TSR) and corporate social responsibility (CSR) often seem completely divorced from each other.
The thinking, mostly implicit, is that maximizing TSR by polluting the environment is acceptable as long as some compensating contributions are made to CSR initiatives – the corporate equivalent of bathing in the Ganges to wash away one’s sins.
We need a new standard that requires companies to report not just their financial metrics, but also their performance on social, developmental, and environmental issues.
For maximum impact, and in order to restore public trust in corporations, the standards must be global, clear, and consistent.
The other reality of the post-MDG world is the multi-directionality of development flows.
Traditionally, development flows were unidirectional, going from the wealthy North to the impoverished South.
But the world order has changed.
Emerging markets and developing countries now account for 50% of global GDP and 75% of global growth, and demographics will further accentuate the shift in the center of economic gravity.
“Southern” countries such as Brazil, China, and India, to name only a few, are enhancing their contributions to overseas development.
Domestic resources and diaspora remittances are increasingly funding development.
As countries become less reliant on traditional sources of financing, they are less likely to follow foreign diktats blindly.
Indeed, people in developing countries are increasingly demanding a stronger voice in determining what is good for them.
The North’s old “we know what is good for you” approach, however well-informed or well-intentioned, will no longer work.
A more inclusive approach that reflects local conditions and preferences is essential.
The P-4 approach reflects the new global realities and seeks to leverage the best qualities of the private, philanthropic, and public sectors.
Models of Madness
Mental health services around the world are largely based on the assumption that being upset or disoriented is a sort of condition like medical illnesses.
Here in Australasia, we imported this perspective from overseas, actively suppressing more holistic Maori and Aboriginal understandings about human distress.
We did so despite numerous studies that show that recovery rates from “mental illness” in “underdeveloped” countries are far superior to those in “advanced” societies.
Nowadays, more and more problems are being redefined as “disorders” or “illnesses,” supposedly caused by genetic predispositions and biochemical imbalances.
Life events are relegated to mere triggers of an underlying biological time bomb.
Thus, feeling very sad has become “depressive disorder.”
Worrying too much is “anxiety disorder.”
Being painfully shy has become “avoidant personality disorder.”
Beating up people is “intermittent explosive disorder.”
Excessive gambling, drinking, drug use, or eating are also illnesses.
The same applies to having too little food, sleep, or sex.
Our Diagnostic and Statistical Manual of Mental Disorders has 886 pages of such illnesses.
Unusual or undesirable behaviors are called “symptoms” and the labels are “diagnoses.”
Our children are labeled, too.
Being bad at math has become “mathematics disorder.”
Ignoring other people’s feelings (once called being naughty) means that the child is suffering from “conduct disorder.”
If this includes getting angry at grownups, he or she (usually he) has “oppositional defiant disorder.”
A “diagnosis” frequently in the news is “attention-deficit/hyperactivity disorder.”
The “symptoms” include fidgeting, losing things, talking excessively, and difficulty playing quietly or taking turns.
Of course some children sometimes have problems. But does anyone help them?
Perhaps it sometimes conceals the causes, while locating the problem entirely within the child, who is often stigmatized as a result.
Indeed, making lists of behaviors, applying medical-sounding labels to people who manifest them, and then using the presence of those behaviors to prove that a person has the illness in question is scientifically meaningless.
It tells us nothing about causes or solutions.
How did this simplistic, certainly unscientific, and frequently damaging approach gain such dominance?
First, it is tempting to avoid facing the painful events in our lives that might be the cause of our difficulties.
If we just accept the diagnosis on offer, nobody is to blame.
Nobody needs do anything differently – except take the tablets.
We were just unlucky enough to get the “illness.”
Second, a model of individual pathology is invaluable to politicians.
They don’t need to spend money on prevention programs to address the psycho-social problems –overwhelming stress, poverty, discrimination, child neglect and abuse, and loneliness, to name but a few – that research has repeatedly demonstrated play a large role in undermining mental health.
Third, exciting developments in technologies for studying our brains and genes have created the hope that we are about to discover the biological causes of, and solutions to, human misery and confusion.
Finally, a new player has entered the nature-nurture debate.
The pharmaceutical industry, fueled by our desire for quick fixes, has effectively deployed its considerable power to promulgate the notion of “disorders” and “illnesses” in all domains of our lives.
The fundamental purpose of drug companies is to produce profits for shareholders.
Naturally, they encourage us to diagnose eating, sleeping, or feeling too much (or too little) as illnesses requiring a chemical cure.
Having listened for 20 years to people unfortunate enough to be labeled “schizophrenic,” considered to be the most extreme form of “mental illness,” and having subsequently researched the causes of hallucinations and delusions for ten years, I believe that the public understands madness better than we experts.
Surveys of public opinion all over the world find that most people believe that emotional problems, including those deemed severe, such as hearing voices, are primarily caused by bad things happening to us rather than by faulty brains or genes.
The public also favors psycho-social approaches, such as talking to someone and getting advice, or help finding friends or a job, rather than drugs, electro-shocks, or admission to psychiatric hospital.
Some experts, however, dismiss these views as “mental health illiteracy.”
If these new treatments aren’t used more frequently, this isn’t because they don’t work.
The main obstacle is that they won’t increase the profits of drug companies, on which, in the absence of adequate government funding, our professional organizations, conferences, journals, research, and teaching institutions have become so dependent.
The Misrule of Law
MADRID – Once upon a time, despots simply acted like despots.
Nowadays, they dress up their dictatorships in the trappings of the rule of law.
Consider Ukrainian President Viktor Yanukovych.
Less than a year after his narrow victory over opposition leader Yulia Tymoshenko in the 2010 presidential elections, Tymoshenko was arrested on trumped-up contempt charges.
She is now serving a seven-year prison sentence for supposedly abusing her position as Prime Minister by signing a gas deal with Russia – and awaits the completion of two more trials.
Unlike most politically motivated trials, Tymoshenko’s case benefits from the oversight of the European Court of Human Rights (ECHR), which recently ruled that her pre-trial detention violated the European Convention on Human Rights.
But Yanukovych continues to feign respect for the rule of law, insisting that he cannot consider granting her a presidential pardon until the legal proceedings have been concluded.
This type of “misrule of law” is not unique to Ukraine.
Russian President Vladimir Putin has consistently used the courts to neutralize his opponents.
Currently, the anti-corruption activist Alexei Navalny, a prominent critic of Putin’s regime, is being prosecuted for allegedly conspiring to embezzle from a state-owned timber firm, while Sergei Magnitsky, a lawyer who died in prison after accusing several Russian officials of large-scale embezzlement, is being tried posthumously on conspiracy charges.
Similarly, opposition leaders in Zimbabwe have been prosecuted for treason; sodomy charges were leveled against the Malaysian opposition leader Anwar Ibrahim; and former Belarusian presidential candidate Andrei Sannikov was imprisoned for allegedly organizing mass protests.
Iran pursued mass prosecutions of government critics following President Mahmoud Ahmadinejad’s controversial reelection in 2009, and it now appears that some of Ahmadinejad’s allies may themselves become collateral damage in his conflict with Supreme Leader Ayatollah Ali Hosseini Khamenei, facing politically motivated criminal trials.
Of course, history is replete with examples of autocrats perverting justice: Stalin had his Moscow Trials, and Hitler had his People’s Court.
But the authority of those dictators did not rest primarily on the rule of law; that of their modern counterparts does.
The sociologist Max Weber described three bases of authority: traditional, charismatic, and rational (legal).
The first has fallen away, its accompanying social structures superseded by industrialization, urbanization, and feminization of the workforce, while the oft-seen link with despotism has delegitimized the second.
As a result, legality has emerged as the main source of political legitimacy in the modern world.
Consequently, those who are more interested in arbitrary authority than legality are using the language of law to legitimize their actions – and weakening the rule of law in the process.
Barely a week after prosecutors accused Tymoshenko – based, yet again, on mere hearsay – of organizing the 1996 assassination of a member of parliament, Yanukovych proclaimed: “Our goal is to ensure real economic and political independence, strengthen democracy and the rule of law, and establish Ukraine as [a] young, powerful, and modern state.”
This disturbingly common disconnect between official words and deeds devalues the rule of law in the eyes of the public.
Two separate discussions about the rule of law are currently underway: an erudite, theoretical debate among elites at think tanks and universities, and a more general – and more consequential – popular “conversation” predicated on frustration with legal abuses.
From southern Europe to Malaysia and China, citizens are taking to the streets to demonstrate their desire for equity and justice.
They may not be able to define the rule of law precisely, but they know when it is being perverted.
In countries with an engaged civil society and room for debate, the discourse on the rule of law can be wide-ranging and substantive.
Too often, however, governments ignore or distort public aspirations, stripping the discussion of all but officially approved platitudes.
In order to reverse this trend, a third discussion, one that engages the international community, is needed.
While such a discussion is beginning to emerge in response to the Tymoshenko case, the ECHR is a lonely voice, and the international community has so far hesitated to challenge violations of the rule of law publicly.
At the same time, some institutions that could provide effective international oversight have been coopted, as demonstrated by Libya’s leadership of the United Nations Commission on Human Rights while under the control of deposed dictator Muammar el-Qaddafi.
And others have been far too willing to accept at face value submissions from countries where the rule of law is routinely abused.
For example, while Interpol recently rejected Russia’s request to pursue the investment banker William F. Browder (a fierce critic of the Russian government since the death of his lawyer, Magnitsky), it should apply Article 3 of its constitution, which bars it from participating in political matters, much more scrupulously.
The response to legal abuses has often been timid or declaratory, carried out by individual countries or communities like the European Union in response to specific violations.
For example, the United States recently banned 18 Russian officials, most of whom were allegedly linked to Magnitsky’s death, from traveling to the US and froze all assets that they hold there.
But, while unilateral pressure can make a difference, the rule of law’s defenders must be willing to apply it convincingly.
The EU’s talks with Ukraine on a proposed Association Agreement, for example, provide some leverage, which the EU should not hesitate to use.
At the same time, the potential impact of targeted action by individual governments is not enough.
Protection of the rule of law must become an international priority – and that will require strong leadership.
The EU – which was not only founded on the rule of law, but is also among its most vocal defenders in its diplomatic relationships – should fill this role.
Beyond the ECHR and its rulings in cases like that of Tymoshenko, a strong European voice would go a long way toward galvanizing broader international action.
Modernization à la Carte?
BERLIN &#45;&#45; Two centuries ago the American and French Revolutions brought forth the natural law concept of inalienable human rights.
However, it took nearly two centuries of wars, political and social disasters, and decolonization before this idea became globally accepted, at least in theory.
In the beginning, the idea of human rights was limited to domestic politics.
In international relations, power, not right, continued to be the only thing that mattered: the traditional concept of state sovereignty focused exclusively on power, i.e., on control over people and territory, and protected the state’s authority, regardless of whether its enforcement was civilized or brutal, democratic or authoritarian.
The Nuremberg Trials of the German war criminals after World War II marked the first important change in the world’s understanding of the concept of sovereignty.
For the first time, an entire state leadership was put on trial for its crimes, as its representatives and henchmen were brought to justice.
The Nuremberg Trials and, in parallel, the creation of the United Nations and its Universal Declaration of Human Rights, signaled the growing importance of law in international relations.
Sovereignty was no longer based solely on power, but increasingly on law and respect for the rights of citizens.
This process was largely frozen during the five decades of the Cold War.
But human rights and the rule of law began to re-emerge as a theme of Western policy, especially in the wake of the Helsinki Conference on European Security and Cooperation and its use by the administration of US President Jimmy Carter, as well as by numerous non-governmental advocates protesting the treatment of Soviet dissidents.
The next big step was the emergence of the concept of humanitarian intervention after the genocide in Rwanda and the Balkan wars in the 1990’s.
As a result, international law came to recognize the “right of protection” against governmental arbitrariness and states’ crimes against their own people, even though enforcement remains quite uncertain.
Finally, the same developments in politics and international law led to the creation of the International Court of Justice.
With its establishment, resulting from long and terrible experience, the basic idea of modernity – that the power of states and their rulers should be subject to the rule of higher law, thus placing individual rights above state sovereignty – has taken a great step forward.
This development was anything but accidental.
In the face of the totalitarian challenges of fascism and communism in the twentieth century, Europe and the United States have become aware that the rule of law, separation of powers, and democracy decisively determine foreign policy and matter greatly from the point of view of international security.
Democracies have proved to be much more peaceful than authoritarian regimes and dictatorships.
But the progress achieved so far is again under threat.
China’s rise and Russia’s resurgence suggest that there is no necessary link between economic development, on the one hand, and political and cultural modernization, on the other.
In particular, China’s breathtaking economic success seems to point to the existence of viable authoritarian alternatives to the Western idea that freedom, democracy, the rule of law, and the market economy are bound together.
Indeed, China appears to suggest that selective modernization is possible (modernization à la carte, so to speak), allowing states to choose to implement only those elements of modernity – technology, economics, infrastructure, political institutions, and values – that they like.
But modernization à la carte is an illusion.
Its proponents forget the experience of the first half of the twentieth century, when authoritarian modernization was tried in both Germany and Russia –with disastrous results.
In the medium term, modernity is indivisible: you can have all of it or none.
The deep technological and social changes unleashed by the forces of modernity create tensions that, in the end, cannot be resolved without appropriate normative and institutional responses.
China and Russia today are no exceptions.
The symptoms of the disease of selective modernization are clearly discernible in both countries in the form of ubiquitous corruption.
China, for example, faces increasing export difficulties because of deficient control of the safety of its products, which is largely the result of corruption.
Without a commitment to a free press and an independent judiciary, these difficulties will only intensify.
Before too long, Russia’s “managed” (read: authoritarian) modernization will also have to allow for the rule of law and a functioning separation of powers, or the country will remain dependent on oil and gas prices and mired in a brutal struggle for power, influence, and money.
Moreover, neither oil and gas deposits nor imperialist policies will stop Russia’s decline.
Without functioning democratic institutions, Russia’s second attempt at selective modernization will fail just as certainly as its previous, Soviet incarnation did.
In the globalized world of the twenty-first century, in which crises in one part of the world spread like wildfire to others, selective modernization, based on suppression of the conflicts and tensions that modernization generates, is likely to be even more dangerous.
Indeed, while the greatest threats to peace once came from power politics and economic rivalry, they now increasingly derive from the regional and global repercussions of the political and social disintegration of stable countries, a decline of their normative and institutional systems, and new totalitarian ideologies.
This is why the opposition between so-called “realists” and “idealists” in foreign policy, and between proponents of “hard” and “soft” power, is proving to be a thing of the past.
To be sure, states are still following traditional interest-oriented policies.
But such policies will be less and less able to guarantee peace and stability in the future.
In the twenty-first century, human rights and security will be inextricably intertwined.
Such is the outcome of globalization, i.e., the mutual dependence of 6.5 billion people in a single global economy and system of states.
Modernizing Multilateralism
WASHINGTON, DC – 2008 will be remembered as a year of extraordinary turmoil.
The financial crisis came on the heels of food and fuel crises.
Now the world is in the midst of an economic crisis, which will lead to many job losses.
Virtually no country has escaped.
We are moving into a new danger zone, with heightened risks to exports and investment, to credit, banking systems, budgets, and balances of payments.
In 2009, we may see the first decline in global trade since 1982.
As always, the poor are the most defenseless.
For developing countries, tighter credit conditions and much weaker growth mean that governments are less able to meet education and health goals, and to invest in the infrastructure needed to sustain growth.
Economic nationalism that seeks gains from the disadvantage of others will trigger ever more dangers.
Global challenges require global solutions.
In October, I called for modernizing multilateralism and markets to better reflect the changing world economy and to enable countries to act in concert to address interconnected problems.
Looking beyond the old G-7 system, we need a twenty-first-century approach to multilateralism through the dynamism of a flexible network, not new hierarchies of a fixed or static system. ampnbsp;
The new multilateralism must maximize the strengths of interdependent and overlapping actors and institutions, public and private.
It should reach beyond the traditional focus on finance and trade, to include other pressing economic and political issues: development, energy, climate change, and stabilizing fragile and post-conflict states.
It needs to draw together existing international institutions, with their expertise and resources, to reform them when necessary, and encourage effective cooperation and common action.
Multilateralism, at its best, is a means for solving problems among countries, with the group at the table willing and able to take constructive action together.
It needs to draw its strength – and legitimacy – from both broader participation and by achieving results.
November’s G-20 summit brought to the table for the first time the rising powers as active stakeholders to address the global financial crisis.
They agreed to a good agenda, but the true test will be the follow-up.
It is a positive step that leaders of major developed economies are now meeting with leaders from the rising economic powers.
But the poorest developing countries must not be left out in the cold.
We will not solve this crisis, or put in place sustainable long-term solutions, by accepting a two-tier world.
The goal must be to build an inclusive and sustainable globalization.
Trillions of dollars are now being spent on a financial rescue in the developed world.
By comparison, about $100 billion a year is currently being spent on overseas aid.
We need a “human” rescue as well as a financial rescue.
In this environment, the global commitment to provide development assistance to the poorest countries must be paramount.ampnbsp; 
At the World Bank Group, we are scaling up our financial support for those in need.
Our private sector arm, the International Finance Corporation (IFC), is launching or expanding three facilities to help the private sector, expected to total around $30 billion over the next three years.
The Future of Force
MUNICH – At the World Economic Forum’s recent annual meeting in Davos, I participated in a panel of defense leaders to discuss the future of the military.
The issue we addressed is a critical one: What kind of war should militaries today be preparing to fight?
Governments have a very poor track record when it comes to answering this question.
After the Vietnam War, for example, the United States’ armed forces suppressed what it had learned about counter-insurgency, only to rediscover it the hard way in Iraq and Afghanistan.
America’s military interventions in these countries exemplify another key challenge of modern warfare.
As outgoing US Secretary of Defense Chuck Hagel pointed out in a recent interview, in war, “things can get out of control, and drift and wander” in ways that can cause a military to fall into a more “accelerated” use of force than was initially anticipated.
Against this background, the notion that force alone can transform conflict-riven societies in the Middle East and elsewhere is a dangerous fallacy.