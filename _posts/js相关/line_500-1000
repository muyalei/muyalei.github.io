Abe was the first G7 leader to hold a summit with Putin after Russia annexed Crimea, and now Russia has won Japan’s economic cooperation, too.
Japan is the only G7 country that has a territorial dispute with Russia, and it is clearly more eager to reach a deal than the Kremlin is.
But this has only strengthened Russia’s hand.
While Japan has softened its position, and signaled that it may accept only a partial return of the islands, Russia has grown only more intransigent.
After the recent summit, Abe revealed that Putin now seems to be reneging on a 1956 agreement between Japan and the Soviet Union, which stipulates that the smaller two of the four islands will be returned to Japan after a peace treaty is signed.
As it happens, this year marks the 60th anniversary of that joint declaration, which was widely viewed as a breakthrough at the time.
The Kremlin is now suggesting that its commitment to fulfilling the declaration was conditional on Japan not joining any security alliance against Russia.
And Putin has expressed concerns that the 1960 Japan-US Security Treaty would extend to the disputed islands if they were returned, thus allowing the US to establish a military presence there.
Japan is in no position to address Russia’s concerns.
It cannot opt out of the US-led sanctions regime; and it cannot exempt the disputed Kurils from its security treaty with the US, especially now that it has been urging the US to provide an explicit commitment to defend the Japanese-controlled Senkaku Islands, over which China claims sovereignty.
Putin, for his part, appears smugly content with his negotiating position.
Not only did he arrive almost three hours late to the onsen summit, in keeping with his habit of leaving foreign leaders waiting; he also declined a Japanese government gift – a male companion for his native Japanese Akita dog, which Japan gave him in 2012.
There is little hope now that Abe will see tangible returns on the political capital he has invested in cultivating Putin.
And Japan’s dilemma will only deepen.
US President-elect Donald Trump’s desire to improve relations with Russia may give Abe leeway to continue wooing Putin; but if Russia gets the US in its corner, it won’t need Japan anymore.
A Berlin Consensus?
HONG KONG – A recent trip to Berlin brought back memories of an earlier visit in the summer of 1967, when I was a poor student who marveled at the Wall that would divide and devastate an entire society for another two decades.
Berlin today is vibrant and rejuvenated, rebuilt by the German peoples' hard work and sacrifice to unify the country, and an apt setting for the conference of the Institute for New Economic Thinking (INET), which I was there to attend.
The conference’s theme was “Paradigm Lost,” with more than 300 economists, political scientists, systems analysts, and ecologists gathering to rethink economic and political theory for the challenges and uncertainty posed by growing inequality, rising unemployment, global financial disarray, and climate change.
Almost everyone agreed that the old paradigm of neoclassical economics was broken, but there was no agreement on what can replace it.
Nobel laureate Amartya Sen attributed the European crisis to four failures – political, economic, social, and intellectual.
The global financial crisis, which began in 2007 as a crisis of US subprime lending and has broadened into a European sovereign-debt (and banking) crisis, has raised questions that we cannot answer, owing to over-specialization and fragmentation of knowledge.
And yet there is no denying that the world has become too intricate for any simple, overarching theory to explain complex economic, technological, demographic, and environmental shifts.
In particular, the rise of emerging markets has challenged traditional Western deductive and inductive logic.
Deductive inference enables us to predict effects if we know the principles (the rule) and the cause.
By inductive reasoning, if we know the cause and effects, we can infer the principles.
Eastern thinking, by contrast, has been abductive, moving from pragmatism to guessing the next steps.
Abductive inference is pragmatic, looking only at outcomes, guessing at the rule, and identifying the cause.
Like history, social-scientific theory is written by the victors and shaped by the context and challenges of its time.
Free-market thinking evolved from Anglo-Saxon theorists (many from Scotland), who migrated and colonized territories, allowing fortunate individuals to assume that there were no limits to consumption.
European continental thinking, responding to urbanization and the need for social order, emphasized institutional analysis of political economy.
Thus, the emergence of neoclassical economics in the nineteenth century was very much influenced by Newtonian and Cartesian physics, moving from qualitative analysis to quantifying human behavior by assuming rational behavior and excluding uncertainty.
This “predetermined equilibrium” thinking – reflected in the view that markets always self-correct – led to policy paralysis until the Great Depression, when John Maynard Keynes’s argument for government intervention to address unemployment and output gaps gained traction.
By the 1970’s, the neoclassical general-equilibrium school captured Keynesian economics through real-sector models that assumed that “finance is a veil,” thereby becoming blind to financial markets’ destabilizing effects.
Economists like Hyman Minsky, who tried to correct this, were largely ignored as Milton Friedman and others led the profession’s push for free markets and minimal government intervention.
But then technology, demographics, and globalization brought dramatic new challenges that the neoclassical approach could not foresee.
Even as the world’s advanced countries over-consumed through leveraging from derivative finance, four billion of the world’s seven billion people began moving to middle-income status, making huge demands on global resources and raising the issue of ecological sustainability.
New thinking is required to manage these massive and systemic changes, as well as the integration of giants like China and India into the modern world.
A change of mindset is needed not just in the West, but also in the East.
In 1987, the historian Ray Huang explained it for China:
“As the world enters the modern era, most countries under internal and external pressure need to reconstruct themselves by substituting the mode of governance rooted in agrarian experience with a new set of rules based on commerce.…This is easier said than done.
The renewal process could affect the top and bottom layers, and inevitably it is necessary to recondition the institutional links between them.
Comprehensive destruction is often the order; and it may take decades to bring the work to completion.”
Using this macro-historical framework, we can see Japanese deflation, European debt, and even the Arab Spring as phases of systemic changes within complex structures that are interacting with one another in a new, multipolar global system.
We are witnessing simultaneous global convergence (the narrowing of income, wealth, and knowledge gaps between countries) and local divergence (widening income, wealth, and knowledge gaps within countries).
Adaptive systems struggle with order and creativity as they evolve.
As the philosopher Bertrand Russell presciently put it: “Security and justice require centralized governmental control, which must extend to the creation of a world government if it is to be effective.
Progress, on the contrary, requires the utmost scope for personal initiative that is compatible with social order.”
A new wave of what the economist Joseph Schumpeter famously called “creative destruction” is under way: even as central banks struggle to maintain stability by flooding markets with liquidity, credit to business and households is shrinking.
We live in an age of simultaneous fear of inflation and deflation; of unprecedented prosperity amid growing inequality; and of technological advancement and resource depletion.
Meanwhile, existing political systems promise good jobs, sound governance, a sustainable environment, and social harmony without sacrifice – a paradise of self-interested free riders that can be sustained only by sacrificing the natural environment and the welfare of future generations.
We cannot postpone the pain of adjustment forever by printing money.
Sustainability can be achieved only when the haves become willing to sacrifice for the have-nots.
The Washington Consensus of free-market reforms for developing countries ended more than two decades ago.
The INET conference in Berlin showed the need for a new one – a consensus that supports sacrifice in the interest of unity.
Europe could use it.
Accepting Japan at Its Word
TOKYO – In recent years, the number of tourists visiting Japan has been increasing rapidly, reaching a record 13.4 million last year, a 29% increase from 2013.
Japan seems to be making great strides toward its goal of recapturing the position as an Asian cultural center that it held a century ago, when the Indian Nobel laureate poet Rabindranath Tagore lived in Tokyo.
Chinese revolutionary leaders Sun Yat-sen and Chiang Kai-shek, along with many other prominent Asians, moved there as well.
Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”
Ordinary Japanese say sumimasen countless times each day, to apologize to friends or strangers for even the most trivial accident or mistake.
But, as Japan’s leaders have experienced firsthand since World War II, expressing regret to other countries is not so simple.
Yet that is precisely what Prime Minister Shinzo Abe must do in his upcoming statement marking the 70th anniversary of the end of the war.
The statement will be based on consultations with many of Japan’s, and the world’s, leading WWII historians, as well as – and more important – with himself, his conscience, and his heart, because he understands the significance of his words on this highly fraught topic.
Of course, Abe is far from the first Japanese leader to confront this challenge.
His statement will follow a long line of declarations by prime ministers and chief cabinet secretaries expressing sincere remorse over the events of WWII.
Twenty years ago, Prime Minister Tomiichi Murayama, the head of the Socialist Party, acknowledged that “Japan, through its colonial rule and aggression, caused tremendous damage and suffering to the people of many countries,” particularly in Asia.
He went on to express “feelings of deep remorse” and offer a “heartfelt apology” to the victims.
Ten years later, Prime Minister Junichiro Koizumi reiterated Murayama’s words, adding that since the war, Japan had been “manifesting its remorse for the war through actions,” especially development assistance and humanitarian activities.
Koizumi also pledged that “Japan, as a peace-loving nation, will work to achieve peace and prosperity for all humankind with all its resources.”
Despite these straightforward declarations of regret, some governments and citizens continue to demand more, giving the impression that nothing a Japanese leader says or does will convince them of the country’s remorse.
This intractability is, in some cases, understandable; the pain of survivors and their descendants remains acute.
But in many other cases, the unwillingness to move beyond history is driven by political interests.
Indeed, political motivations are behind claims that Abe does not agree with past official apologies, despite his repeated assurances that he does, as well as suggestions that he is seeking to revise history, even though he has never denied Japan’s colonial aggression.
Moreover, some have produced portrayals of Japan, as a whole, as an unrepentant country – or, worse, as one that is hell-bent on remilitarization.
Such depictions are breathtaking in their audacity, given Japan’s seven-decade record as a peaceful and constructive member of the international community.
This is not lost on those in Japan who ask for how long their country will have to apologize, with some even suggesting that after 70 years, a “tweet” on the subject should amount to adequate acknowledgement by Abe.
The prime minister, however, remains committed to issuing a strong and sincere statement on the subject.
Early this year, Abe announced his intention to use the 70th anniversary statement to communicate Japan’s remorse for the war, describe the progress the country has made in upholding peace, and describe the contributions that Japan can make to Asia and the rest of the world in the coming decades.
In fact, it is the third component of the announcement that inspires fear in some observers: By helping to build a strong security architecture in the Asia-Pacific region, Japan could undermine the ability of some actors to advance their own interests.
That is why they launched a whisper campaign against Abe’s statement months before he even began to write it.
But, of course, Asian security and prosperity is in everyone’s interest.
Given this, not even the language of Abe’s statement is particularly important; what matters is the determination he expresses, and the actions he takes to follow through – with appropriate humility – on his pledges.
And it seems that Abe is, indeed, determined to make real contributions to peace, based on effective cooperation with Japan’s friends and allies.
But if Asia is to move beyond its past, the victims of Japan’s wartime aggression must recognize that the Japan of 2015 is not the Japan of 1931, 1941, or even 1945, and that, as many Asian leaders have realized over the years, forgiveness benefits everyone.
In 1998, South Korean President Kim Dae-jung responded positively to a statement by former Japanese Prime Minister Keizo Obuchi.
The governments of Indonesia, the Philippines, Vietnam, and other countries have done the same, and now welcome Japan’s commitment to act with its allies to protect regional security.
These countries’ openness to reconciliation have enabled Japan to recast itself as a key arbiter of regional peace and prosperity, not to mention an increasingly dynamic cultural hub.
It is time for the rest of the region to follow suit, accepting at face value Japan’s sincere apologies and working with the country to build a better future.
At a time when Asia is facing serious security challenges, this stance could not be more urgent.
A Big Chance for Small Farmers
NEW YORK – The G-8’s $20 billion initiative on smallholder agriculture, launched at the group’s recent summit in L’Aquila, Italy, is a potentially historic breakthrough in the fight against hunger and extreme poverty.
With serious management of the new funds, food production in Africa will soar.
Indeed, the new initiative, combined with others in health, education, and infrastructure, could be the greatest step so far toward achieving the Millennium Development Goals, the internationally agreed effort to reduce extreme poverty, disease, and hunger by half by 2015 .
During 2002-2006, I led the United Nations Millennium Project, which aimed to achieve the Millennium Development Goals, for then-UN Secretary General Kofi Annan.
One cornerstone of the project was “smallholder farmers,” meaning peasant farm families in Africa, Latin America, and Asia – working farms of around one hectare (2.5 acres) or less.
These are some of the poorest households in the world, and, ironically, some of the hungriest as well, despite being food producers.
They are hungry because they lack the ability to buy high-yield seeds, fertilizer, irrigation equipment, and other tools needed to increase productivity.
As a result, their output is meager and insufficient for their subsistence.
Their poverty causes low farm productivity, and low farm productivity reinforces their poverty.
It’s a vicious circle, technically known as a poverty trap.
The UN Millennium Project’s Hunger Task Force, led by two world-leading scientists, M. S. Swaminathan and Pedro Sanchez, examined how to break this vicious circle.
The Hunger Task Force determined that Africa could substantially increase its food production if help was given to smallholder farmers, in the form of agricultural inputs.  The Millennium Project recommended a big increase in global funding for this purpose.  Drawing on that work and related scientific findings, Annan launched a call in 2004 for an African Green Revolution, based on an expanded partnership between Africa and donor countries.
Many of us, notably current UN Secretary General Ban Ki-moon, have worked hard to make this possible, with Ban repeatedly emphasizing the special emergency arising from the global food, financial, and energy crises of the past two years.
The G-8 announcement reflects these years of effort, and of course the boost from the leadership of US President Barack Obama, Spanish Prime Minister Jose Luis Zapatero, Australian Prime Minister Kevin Rudd, World Bank President Robert Zoellick, European Commissioner Louis Michel, European Parliamentarian Thijs Berman, and others.
Now the key is to make this effort work.
The lessons of history are clear.
Getting seed and fertilizer to smallholder farmers at highly subsidized prices (or even free in some cases) will make a lasting difference.
Not only will food yields rise in the short term, but farm households will use their higher incomes and better health to accumulate all sorts of assets: cash balances, soil nutrients, farm animals, and their children’s health and education.
That boost in assets will, in turn, enable local credit markets, such as micro-finance, to begin operating.
Farmers will be able to buy inputs, either out of their own cash, or by borrowing against their improved creditworthiness.
A consensus has now been reached on the need to assist smallholders, but obstacles remain.
Perhaps the main risk is that the “aid bureaucracies” now trip over each other to try to get their hands on the $20 billion, so that much of it gets taken up by meetings, expert consultations, overhead, reports, and further meetings.
“Partnerships” of donors can become an expensive end in themselves, merely delaying real action.
If donor governments really want results, they should take the money out of the hands of thirty or more separate aid bureaucracies and pool it in one or two places, the most logical being the World Bank in Washington and the International Fund for Agricultural Development (IFAD) in Rome.
One or both of these agencies would then have an account with several billion dollars.
Governments in hunger-stricken regions, especially Africa, would then submit national action plans that would provide details on how they would use the donor funds to get high-yield seeds, fertilizer, irrigation, farm tools, storage silos, and local advice to impoverished farmers.
An independent expert panel would review the national plans to verify their scientific and managerial coherence.
Assuming that a plan passes muster, the money to support it would quickly be disbursed.
Afterward, each national program would be monitored, audited, and evaluated.
This approach is straightforward, efficient, accountable, and scientifically sound.
Two major recent success stories in aid have used this approach: the Global Alliance on Vaccines and Immunizations, which successfully gets immunizations to young children, and the Global Fund to Fight AIDS, TB, and Malaria, which supports national action plans to battle these killer diseases.
Both have saved millions of lives during the past decade, and have paved the way to a new more efficient and scientifically sound method of development assistance.
Not surprisingly, many UN agencies and aid agencies in rich countries fight this approach.
All too often, the fight is about turf, rather than about the most effective way to speed help to the poor.
Obama, Rudd, Zapatero, and other forward-thinking leaders can therefore make a huge difference by following up on their pledges at the G-8 and insisting that the aid really works.
The bureaucracies must be bypassed to get help to where it is needed: in the soil tilled by the world’s poorest farm families.
A Black and White Question
NEW YORK – In the afternoon of July 16 two men appeared to be breaking into a fine house in an expensive area of Cambridge, Massachusetts.
Alerted by a telephone call, a policeman arrived smartly on the scene.
He saw one black male standing inside the house and asked him to come out.
The man refused.
He was then told to identify himself.
The man, still refusing to step out, said he was a Harvard professor, showed his ID, and warned the cop not to mess with him.
He said something about black men in America being singled out, and asked the cop, who was white, for his name and identification.
The cop, joined by several colleagues, arrested the professor for disorderly conduct.
We now know that the professor had broken into his own home, with the help of his chauffeur, because the door was jammed. 
What was unusual here was not the cop’s heavy-handedness.
Most people in the US know that if you talk back to the police, they will get nasty very fast.
The fact that the man was black might or might not have made the cop go for his handcuffs even sooner than he might normally have done.
That, too, would not have been unusual.
What made this case special was that Henry Louis “Skip” Gates is one of the most celebrated professors in the country, famous for his books, his articles, and numerous television appearances.
He is a grandee, a mover and shaker in the academic and media world, a friend of President Barack Obama.
That is why he warned the cop, Sgt. James Crowley, a veteran of the Cambridge police force, not to mess with him.
Class and race overlap in the US.
In this instance, it is impossible to pry them apart.
Gates, deeply conscious, indeed a specialist of the terrible history of race relations in his country, instinctively assumed that he was a victim of prejudice.
From his words it appears that he was equally conscious of not getting the proper respect due to a distinguished Harvard professor and media celebrity.
As he put it to his daughter in an interview published online: “[Crowley] should have gotten out of there and said, ‘I’m sorry, sir, good luck.
Loved your [television] series—check with you later!’”
Alas, Sgt.Crowley had never heard of Professor Gates.
A local man whose brothers all serve in the police force, a sports fan, and an amateur basketball coach, Crowley does not move in the same social circles as Gates.
As it happens, the charges were duly dropped, and there the case might have rested if President Obama, tired and frustrated after weeks of fighting for his healthcare bill, had not weighed in on behalf of his “friend” Gates, and called the police “stupid.”
Both he and Gates then spoke of “learning” from the incident.
Gates might even be planning a television documentary on racial profiling.
One thing to be learned, if we didn’t know this already, is how close racial sensitivities are to the surface of US life, despite the election of a black president.
The complexities of black anger, white guilt, and of black, and white fear, are so vexed that most Americans prefer not to talk about race at all.
The field is too full of mines.
One of Obama’s great achievements is that he made it into a serious topic through the brilliance and subtlety of his rhetoric.
One might argue that it
There is, however, a danger that it will have an adverse affect on the necessary national discussion about race.
By having made such a big issue out of what was in fact a relatively minor event Gates could be accused of trivializing much worse instances of abuse.
Indeed, we don’t even know for certain whether this was such an instance.
Crowley never mentioned the color of Gates’ skin. There was no question of violence.
There were just very raw nerves and hypersensitivity to hints of disrespect, on the part of the professor, and of the cop.
Outrage about a professor who is not to be messed with is not the best way to discuss the plight of countless, poor, anonymous people, whom most of us find it too easy to ignore.
A Bollywood Bride for Sarkozy?
PARIS &#45;&#45; Ever since French President Nicolas Sarkozy took himself off his country’s most-eligible-bachelor list by publicly acknowledging his affair with supermodel-turned-pop-musician Carla Bruni during a romantic trip to Euro Disney, he’s run into trouble. 
His ratings have dipped below 50% for the first time.
Older French citizens don’t find the public spectacle of their leader in love very amusing.
Abroad, Egyptian lawmakers were so exercised over the prospect of the French head of state sharing a bed with his girlfriend that several vented their disapproval on the floor of the parliament. 
Likewise, India is all in a quandary over how to handle protocol during Sarkozy’s impending visit to the subcontinent as the guest of honor at the country’s Republic Day celebrations on January 26.
Should the First Girlfriend have her own motorcade, as a first lady would?
Meanwhile, the same hard-right Hindu groups that protest Valentine’s Day as a decadent Western holiday have warned that if Sarkozy arrives with his girlfriend in tow, they’ll be out in the streets to welcome him.
This controversy has threatened to cast a pall over a much-heralded summit between two of the world’s great democracies.
With lucrative deals at stake for the big-ticket products that drive the French economy – military hardware, nuclear power plants, and Airbus planes – France has a strong interest in a successful summit in India.
“You’ll probably find out after it’s happened,” he taunted.
Rumor has it the couple has set February 8 or 9 for the wedding.   Others say that Sarkozy has already outsmarted the media by secretly marrying in the Elysee Palace, even as he was dodging wedding questions.
If that is true, then Sarkozy missed the romantic opportunity of a lifetime.
If the couple sizzled for cameras with Luxor and Petra as the backdrop, just imagine how hot things could get at the most romantic spot on Earth, the Taj Mahal.
And, given the current rage for all things Bollywood in France, a lavish Indian wedding would be fitting.
Bruni’s own life path closely resembles any number of Bollywood stars who have made the transition from model to actress.
A comely brunette who sings is perfect for a Bollywood makeover.
The Indian government will be nothing if not relieved to see the first girlfriend made a wife.
As one of India’s leading daily newspapers, the
Despite the sometimes downright pornographic on-screen writhing of Bollywood starlets, India is still a deeply conservative society.
Divorce is anathema. (Sarkozy is now twice divorced.)
And, while mistresses abound among the privileged classes, they do not strut publicly by their power-mates’ sides.
Kissing and fondling in public, even by spouses, is taboo.
In this respect, India more resembles the France with which Sarkozy wants to make a clean break than the current one.
Most Indians, as it seems many French, would prefer not to shed the “hypocrisy” Sarkozy laments on the part of his predecessors (read: former French President François Mitterrand, who had a child with his mistress about whom the public knew nothing until the man’s funeral).
Sarkozy, of all people, should know that a large part of the gravitas of office derives from pomp and circumstance.
Statecraft is a realm where appearances are meant to be deceiving.
When Sarkozy, who otherwise has such finely tuned media instincts, protests that he’s no different from any other man, he comes dangerously close to confusing the office and the person of the president.
Most French people could only dream of an exotic wedding in India.
Sarkozy could make that dream come true.
If he really is as head-over-heels in love with Bruni as he claims, and plans to marry her imminently, why not take advantage of his upcoming trip to India and make this a wedding to remember?
He could meet his bride seated majestically on the caparison of an elaborately decorated elephant, and she would look ravishing swathed and bejeweled in Indian finery.
The “bling-bling” president, as Sarkozy has been dubbed, can wear all the gold he wants and heap yet more diamonds on his bride. 
The cameras would roll, Indians would smile, and France would be treated to a Bollywood spectacle beyond its wildest dreams.
And if it’s too late for the wedding, there’s always the reception.
A Born-Again CAP
WAGENINGEN, NETHERLANDS – Born in 1957, the Common Agricultural Policy (CAP) is now more than 50 years old, and the European Commission is proposing what it calls a health check for its middle-aged child.
But superficial repairs will not meet the European Union’s future needs. The CAP must be born again.
Work on its renewal is due to start now, with the completed project ready in 2013. But a much more profound re-think is needed.
The CAP’s original aim was to provide a secure source of food for the six original member states of the Union, which were importers of food and sought a degree of self-sufficiency.
Good, healthy, and cheap food had to be accessible for all citizens.
Improved agricultural productivity would benefit rural areas and give farmers a comparative share in the Union’s growing wealth.
Instruments to achieve those objectives were developed, and food security was achieved.
The CAP quickly came to be seen as the jewel in the crown of the European project.
As the EU has evolved and expanded, food systems have become more complex, involving production, processing, supply-chain organization, and wholesale and retail distribution, with all of these involving new issues like health and the environment.
The use of land is also receiving more serious scrutiny.
A 1991 study by the Netherlands Scientific Council for Governmental Policy, entitled
Those figures were for an EU of 15 countries, so with today’s 27 members the possibilities are even greater.
A Dutch analysis of land use has shown that by employing the best technical and ecological means on the best available land, substantial gains could be made in food production.
So it is not surprising that the number of farmers needed has fallen substantially.
A simplified CAP would encourage cleaner, more productive, and efficient agriculture.
A side benefit for the EU’s standing in the world could be that the World Trade Organization’s stalled Doha negotiations could be restarted once farmers in developing countries are assured of getting a fair deal from Europe.
Moreover, the CAP’s role as a motor of political and social integration in Europe could be restored once renewed policies are in place.
But renewal of this sort cannot be left to global market forces, as the results might not necessarily benefit European agriculture and society.
If the market “misbehaves,” farmers could be reduced to poverty, leading to the neglect of large areas of Europe.
That is a real enough danger to which policymakers must give serious thought as they reform the CAP on the basis of the following five pillars.
1.&#160;&#160;&#160;&#160;&#160; The EU needs a knowledge and innovation policy that strengthens European agriculture’s competitiveness.
Such a policy has been successful in the Netherlands, substantially contributing to the development and power of the country’s agribusiness.
Ten of 21 branches of Dutch agribusiness, including horticultural seeds, ornamentals, seed potatoes, and veal, are among the top contributors to the national economy and the country’s trade balance.
In the EU as a whole, a policy directed toward research programs stimulating scientific excellence and greater coherence in the European knowledge system would greatly strengthen agriculture’s competitiveness and contribute to food security and sustainable development.
2.&#160;&#160;&#160;&#160;&#160; Europe also needs a restructuring policy for land use.
Many structural improvement programs have been financed at the European level, but agricultural production and land use are not among them.
The development of an Agricultural Main Structure would compliment the European Ecological Main Structure.
Reforestation and the repair of natural ecosystems should also be part of a land use policy.
3.&#160;&#160;&#160;&#160;&#160; A policy for European food systems would treat production, processing, distribution, logistics, and retailing in combination.
Consumption patterns and preferences are an integral part of such systems.
Preliminary studies by the European Science Foundation’s “Forward Look on European Food Systems” could prove useful in devising an EU-wide policy.
4.&#160;&#160;&#160;&#160;&#160; Metropolitan agriculture in a rapidly urbanizing world can provide high-quality produce on small amounts of land.
It offers an answer to rising demand for healthy food with minimal environmental side effects.
5.&#160;&#160;&#160;&#160;&#160; A new CAP should include a policy to safeguard Europe’s landscapes.
But a cultural heritage should not be maintained everywhere, nor should it ignore cost.
And it should not be a defensive policy of the sort that tends to concentrate on poor-quality land.
These five pillars involve drastic choices, but they will probably require less money from Europe’s taxpayers, not more.
They could make a real contribution to cleaner, more productive, and efficient farming and land use, while addressing social needs.
A Breakthrough Against Hunger
NEW YORK – Today’s world hunger crisis is unprecedentedly severe and requires urgent measures.
Nearly one billion people are trapped in chronic hunger – perhaps 100 million more than two years ago.
Spain is taking global leadership in combating hunger by inviting world leaders to Madrid in late January to move beyond words to action.
With Spain’s leadership and United Nations Secretary General Ban Ki-moon’s partnership, several donor governments are proposing to pool their financial resources so that the world’s poorest farmers can grow more food and escape the poverty trap.
The benefits of some donor help can be remarkable.
Peasant farmers in Africa, Haiti, and other impoverished regions currently plant their crops without the benefit of high-yield seed varieties and fertilizers.
The result is a grain yield (for example, maize) that is roughly one-third less than what could be achieved with better farm inputs.
African farmers produce roughly one ton of grain per hectare, compared with more than four tons per hectare in China, where farmers use fertilizers heavily.
African farmers know that they need fertilizer; they just can’t afford it.
With donor help, they can.
Not only do these farmers then feed their families, but they also can begin to earn market income and to save for the future.
By building up savings over a few years, the farmers eventually become creditworthy, or have enough cash to purchase vitally necessary inputs on their own.
There is now widespread agreement on the need for increased donor financing for small farmers (those with two hectares or less of land, or impoverished pastoralists), which is especially urgent in Africa.
The UN Secretary General led a steering group last year that determined that African agriculture needs around $8 billion per year in donor financing – roughly four times the current total – with a heavy emphasis on improved seeds, fertilizer, irrigation systems, and extension training.
In addition to direct help for small farms, donors should provide more help for the research and development needed to identify new high-yielding seed varieties, especially to breed plants that can withstand temporary flooding, excess nitrogen, salty soils, crop pests, and other challenges to sustainable food production.
Helping the poor with today’s technologies, while investing in future improved technologies, is the optimum division of labor.
This investment pays off wonderfully, with research centers such as the International Rice Research Institute and the International Maize and Wheat Improvement Centre providing the high-yield seeds and innovative farming strategies that together triggered the Asian Green Revolution.
These centers are not household names, but they deserve to be.
Their scientific breakthroughs have helped to feed the world, and we’ll need more of them.
Dozens of low-income, food-deficit countries, perhaps as many as 40-50, have elaborated urgent programs for increased food production by small farms, but are currently held back by the lack of donor funding.
These countries have appealed to the World Bank for financing, and the Bank made a valiant effort in 2008 to help through its new Global Food Crisis Response Program (GFCRP).
But the Bank does not yet have sufficient funds to meet these countries’ urgent needs, and has had to ration assistance to a small fraction of the flows that could be effectively and reliably used.
Hundreds of millions of people, in the meantime, remain trapped in hunger.
Many individual donor countries have declared that they are now prepared to increase their financial support for smallholder agriculture, but are searching for the appropriate mechanisms to do so.
The current aid structures are inadequate.
The more than 20 bilateral and multilateral donor agencies for agriculture are highly fragmented and of insufficient scale individually and collectively.
Despite the dedicated efforts of many professionals, the response to the hunger crisis remains utterly inadequate.
The 2008 planting seasons came and went with much too little additional help for impoverished small farmers.
African countries search endlessly, and mostly fruitlessly, for the small amounts of funding needed for their purchases of fertilizer and improved seeds.
My colleagues and I, serving on an advisory committee for the Spanish initiative, have recommended that donors pool their funds into a single international account, which we call the Financial Coordination Mechanism (FCM).
These pooled funds would enable farmers in poor countries to obtain the fertilizer, improved seed varieties, and small-scale irrigation equipment that they urgently need.
Poor countries would receive prompt and predictable financing for agricultural inputs from a single account, rather than from dozens of distinct and fragmented donors.
By pooling financial resources into a single-donor FCM, aid programs’ administrative costs could be kept low, the availability of aid flows could be assured, and poor countries would not have to negotiate 25 times in order to receive help. 
The time for business as usual is over.
The donors promised to double aid to Africa by 2010, but are still far off track.
Indeed, during the past 20 years, they actually cut aid for agriculture programs, and only now are reversing course.
Meanwhile, a billion people go hungry each day.
We need a breakthrough that is demonstrable, public, clear, and convincing, that can mobilize the public’s hearts and minds, and that can demonstrate success.
History can be made in Madrid at the end of January, when the world’s richest and poorest countries converge to seek solutions to the global hunger crisis.
The lives of the billion poorest people depend on it.
A Breakthrough Opportunity for Global Health
NEW YORK – Every year, millions of people die from preventable and treatable diseases, especially in poor countries.
In many cases, lifesaving medicines can be cheaply mass-produced, but are sold at prices that block access to those who need them.
And many die simply because there are no cures or vaccines, because so little of the world’s valuable research talent and limited resources is devoted to addressing the diseases of the poor.
This state of affairs represents a failure of economics and law that urgently needs to be corrected.
The good news is that there are now opportunities for change, most promisingly through an international effort headed by the World Health Organization that would begin to fix the broken intellectual-property regime that is holding back the development and availability of cheap drugs.
Two main problems limit the availability of medicines today.
One is that they are very costly; or, more accurately, the price charged for them is very high, though the cost of producing them is but a fraction of that amount.
Second, drug development is geared toward maximizing profit, not social benefit, which skews efforts directed at the creation of medicines that are essential to human welfare.
Because the poor have so little money to spend, drug companies, under current arrangements, have little incentive to do research on the diseases that afflict them.
It doesn’t have to be this way.
Drug companies argue that high prices are necessary to fund research and development.
But, in the United States, it is actually the government that finances most health-related research and development – directly, through public support (National Institutes of Health, National Science Foundation), and indirectly, through public purchases of medicine, both in the Medicare and Medicaid programs.
Even the part that is not government-financed is not a conventional market; most individuals’ purchases of prescription medicines are covered by insurance.
Government finances health-care research because improved medicines are a public good.
The resulting knowledge benefits everyone by stopping epidemics and limiting the economic and human toll of widespread illness.
Efficiency requires sharing research as widely as possible as soon as it is available.
Thomas Jefferson compared knowledge to candles: when one is used to light another, it does not diminish the light of the first.
On the contrary, everything becomes brighter.
Yet, in America and most of the world, drug prices are still exorbitant and the spread of knowledge is tightly limited.
That is because we have created a patent system that gives innovators a temporary monopoly over what they create, which encourages them to hoard their knowledge, lest they help a competitor.
While this system does provide incentives for certain kinds of research by making innovation profitable, it allows drug companies to drive up prices, and the incentives do not necessarily correspond to social returns.
In the health-care sector, it may be more profitable to devote research to a “me-too” drug than to the development of a treatment that really makes a difference.
The patent system may even have adverse effects on innovation, because, while the most important input into any research is prior ideas, the patent system encourages secrecy.
A solution to both high prices and misdirected research is to replace the current model with a government-supported prize fund.
With a prize system, innovators are rewarded for new knowledge, but they do not retain a monopoly on its use.
That way, the power of competitive markets can ensure that, once a drug is developed, it is made available at the lowest possible price – not at an inflated monopoly price.
Fortunately, some US lawmakers are taking a strong interest in this approach.
The Prize Fund for HIV/AIDS Act, a congressional bill introduced by Senator Bernie Sanders, is just such an initiative.
His bill also contains an important provision aimed at encouraging open-source research, which would move the current research model away from secrecy toward sharing.
But, globally, our innovation system needs much bigger changes.
The WHO’s efforts to encourage broad reforms at the international level are crucial.
This spring, the WHO released a report that recommends solutions similar to those proposed in the US Senate bill, but on a global level.
Importantly, the report, “Research and Development to Meet Health Needs in Developing Countries,” recommends a comprehensive approach, including mandatory funding contributions from governments for research on developing countries’ health needs; international coordination of health-care priorities and implementation; and a global observatory that would monitor where needs are greatest.
In late May, the international community will have a chance to begin implementing these ideas at the WHO World Health Assembly – a moment of hope for public health around the world.
Reforming our innovation system is not just a matter of economics.
It is, in many cases, a matter of life and death.
It is therefore essential to de-link R&amp;D incentives from drug prices, and to promote greater sharing of scientific knowledge.
For America, the Sanders bill marks important progress.
For the world, the WHO’s recommendations represent a once-in-a-generation opportunity to remedy a long-standing and egregious inequity in health care, and, more broadly, to set a model for governance of global public goods befitting an era of globalization.
We cannot afford to let this opportunity pass us by.
Absent-Minded Killers
As a species, human beings have a major self-control problem.
We humans are now so aggressively fishing, hunting, logging, and growing crops in all parts of the world that we are literally chasing other species off the planet.
Our intense desire to take all that we can from nature leaves precious little for other forms of life.
In 1992, when the world’s governments first promised to address man-made global warming, they also vowed to head off the human-induced extinction of other species.
The Convention on Biological Diversity, agreed at the Rio Earth Summit, established that “biological diversity is a common concern of humanity.”
The signatories agreed to conserve biological diversity, by saving species and their habitats, and to use biological resources (e.g., forests) in a sustainable manner.
In 2002, the treaty’s signatories went further, committing to “a significant reduction in the current rate of biodiversity loss” by 2010.
Unfortunately, like so many other international agreements, the Convention on Biological Diversity remains essentially unknown, un-championed, and unfulfilled.
That neglect is a human tragedy.
For a very low cash outlay – and perhaps none at all on balance – we could conserve nature and thus protect the basis of our own lives and livelihoods.
We kill other species not because we must, but because we are too negligent to do otherwise.
Consider a couple of notorious examples.
Some rich countries, such as Spain, Portugal, Australia, and New Zealand, have fishing fleets that engage in so-called “bottom trawling.”
Bottom trawlers drag heavy nets over the ocean bottom, destroying magnificent, unexplored, and endangered marine species in the process.
Complex and unique ecologies, most notably underground volcanoes known as seamounts, are ripped to shreds, because bottom trawling is the “low cost” way to catch a few deep sea fish species.
One of these species, orange roughy, has been caught commercially for only around a quarter-century, but already is being fished to the point of collapse.
Likewise, in many parts of the world, tropical rainforest is being cleared for pasture land and food crops.
The result is massive loss of habitat and destruction of species, yielding a tiny economic benefit at a huge social cost.
After cutting down a swath of rainforest, soils are often quickly leached of their nutrients so that they cannot sustain crops or nutritious grasses for livestock.
As a result, the new pasture land or farmland is soon abandoned, with no prospect for regeneration of the original forest and its unique ecosystems.
Because these activities’ costs are so high and their benefits so low, stopping them would be easy.
Bottom trawling should simply be outlawed; it would be simple and inexpensive to compensate the fishing industry during a transition to other activities.
Forest clearing, on the other hand, is probably best stopped by economic incentives, perhaps combined with regulatory limits.
Simply restricting the practice of land clearing probably would not work, since farm families and communities would face a strong temptation to evade legal limits.
On the other hand, financial incentives would probably succeed, because cutting down forest to create pastureland is not profitable enough to induce farmers to forego payments for protecting the land.
Many rainforest countries have united in recent years to suggest the establishment of a rainforest conservation fund by the rich countries, to pay impoverished small farmers a small amount of money to preserve the forest.
A well-designed fund would slow or stop deforestation, preserve biodiversity, and reduce emissions of carbon dioxide the burning of cleared forests.
At the same time, small farmers would receive a steady flow of income, which they could use for micro-investments to improve their household’s wealth, education, and health.
Aside from banning bottom trawling and establishing a global fund for avoided deforestation, we should designate a global network of protected marine areas, in which fishing, boating, polluting, dredging, drilling, and other damaging activities would be prohibited.
Such areas not only permit the regeneration of species, but also provide ecological benefits that spill over to neighboring unprotected areas.
We also need a regular scientific process to present the world with the evidence on species abundance and extinction, just as we now have such a process for climate change.
Politicians don’t listen very well to individual scientists, but they are forced to listen when hundreds of scientists speak with a united voice.
Finally, the world should negotiate a new framework no later than 2010 to slow human-induced climate change.
There can be little doubt that climate change poses one of the greatest risks to species’ viability.
As the planet warms, and rain and storm patterns change dramatically, many species will find themselves in climate zones that no longer support their survival.
Some can migrate, but others (such as polar bears) are likely to be driven to extinction unless we take decisive action to head off climate change.
These measures are achievable by 2010.
They are affordable, and in each case would ultimately deliver large net benefits.
Most importantly, they would allow us to follow through on a global promise.
It is too painful to believe that humanity would destroy millions of other species – and jeopardize our own future – in a fit of absent-mindedness.
Making Do With More
BERKELEY – In the United States, just three out of ten workers are needed to produce and deliver the goods we consume.
Everything we extract, grow, design, build, make, engineer, and transport – down to brewing a cup of coffee in a restaurant kitchen and carrying it to a customer's table – is done by roughly 30% of the country's workforce.
The rest of us spend our time planning what to make, deciding where to install the things we have made, performing personal services, talking to each other, and keeping track of what is being done, so that we can figure out what needs to be done next.
And yet, despite our obvious ability to produce much more than we need, we do not seem to be blessed with an embarrassment of riches.
One of the great paradoxes of our time is that workers and middle-class households continue to struggle in a time of unparalleled plenty.
John Maynard Keynes was not off by much when he famously predicted in 1930 that the human race's “economic problem, the struggle for subsistence," was likely to be “solved, or be at least within sight of solution, within a hundred years."
It will take another generation, perhaps, before robots have completely taken over manufacturing, kitchen work, and construction; and the developing world looks to be 50 years behind. But Keynes would have been spot on had he targeted his essay at his readers' great-great-great-great grandchildren.
And yet there are few signs that working- and middle-class Americans are living any better than they did 35 years ago.
Even stranger, productivity growth does not seem to be soaring, as one would expect; in fact, it seems to be decelerating, according to research by John Fernald and Bing Wang, economists in the Economic Research Department of the Federal Reserve Bank of San Francisco.
Growth prospects are even worse, as innovation hits gale-force headwinds.
One way to reconcile the changes in the job market with our lived experience and statistics like these is to note that much of what we are producing is very different from what we have made in the past.
For most of human experience, the bulk of what we produced could not be easily shared or used without permission.
The goods we made were what economists call “rival" and “excludible" commodities.
Being “rival" means that two people cannot use the same product at the same time.
Being “excludible" means that the owner of a product can easily prevent others from using it.
These two traits put a great deal of bargaining power in the hands of those who control production and distribution, making them ideal for a market economy based on private property.
Money naturally flows to where utility and value are being provided – and those flows are easy to track in national accounts.
But much of what we are producing in the information age is neither rival nor excludible – and this changes the entire picture.
The creation of information-age goods is difficult to incentivize; their distribution is hard to monetize; and we lack the tools to track them easily in national accounts.
The result is an ever-growing discrepancy between what people would be willing to pay for a given service and growth as measured in national statistics.
In other words, we are producing and consuming much more than our economic indicators suggest – and the creators of many of those products are not being adequately compensated.
This produces a set of unique problems.
To ensure that the workers of today and tomorrow are able to capture the benefits of the information age will require us to redesign our economic system to stimulate the creation of these new types of commodities.
In addition to developing ways to account for this new type of wealth, we will have to develop channels through which demand for a product contributes to the income of its creator.
Only by finding ways to put true value on the goods we produce will we be able to sustain a middle-class society, rather than one of techno-plutocrats and their service-sector serfs.
The Closing of the Academic Mind
LONDON – I would wager that I have been Chancellor of more universities than anyone alive today.
This is partly because when I was Governor of Hong Kong, I was made Chancellor of every university in the city.
I protested that it would surely be better for the universities to choose their own constitutional heads.
But the universities would not allow me to resign gracefully.
So for five years I enjoyed the experience of giving tens of thousands of students their degrees and watching what this rite of passage meant for them and their families.
When I came back to Britain in 1997, I was asked to become Chancellor of Newcastle University.
Then, in 2003, I was elected Chancellor by the graduates of Oxford University, one of the world’s greatest institutions of learning.
So it should not be surprising that I have strong views about what it means to be a university and to teach, do research, or study at one.
Universities should be bastions of freedom in any society.
They should be free from government interference in their primary purposes of research and teaching; and they should control their own academic governance.
I do not believe it is possible for a university to become or remain a world-class institution if these conditions do not exist.
The role of a university is to promote the clash of ideas, to test the results of research with other scholars, and to impart new knowledge to students.
Freedom of speech is thus fundamental to what universities are, enabling them to sustain a sense of common humanity and uphold the mutual tolerance and understanding that underpin any free society.
That, of course, makes universities dangerous to authoritarian governments, which seek to stifle the ability to raise and attempt to answer difficult questions.
But if any denial of academic liberty is a blow struck against the meaning of a university, the irony today is that some of the most worrying attacks on these values have been coming from inside universities.
In the United States and the United Kingdom, some students and teachers now seek to constrain argument and debate.
They contend that people should not be exposed to ideas with which they strongly disagree.
Moreover, they argue that history should be rewritten to expunge the names (though not the endowments) of those who fail to pass today’s tests of political correctness.
Thomas Jefferson and Cecil Rhodes, among others, have been targeted.
And how would Churchill and Washington fare if the same tests were applied to them?
Some people are being denied the chance to speak as well – so-called “no platforming”, in the awful jargon of some clearly not very literate campuses.
There are calls for “safe spaces” where students can be protected from anything that assaults their sense of what is moral and appropriate.
This reflects and inevitably nurtures a harmful politics of victimization – defining one’s own identity (and thus one’s interests) in opposition to others.
When I was a student 50 years ago, my principal teacher was a leading Marxist historian and former member of the Communist Party.
The British security services were deeply suspicious of him.
He was a great historian and teacher, but these days I might be encouraged to think that he had threatened my “safe space.”
In fact, he made me a great deal better informed, more open to discussion of ideas that challenged my own, more capable of distinguishing between an argument and a quarrel, and more prepared to think for myself.
Of course, some ideas – incitement of racial hatred, gender hostility, or political violence – are anathema in every free society.
Liberty requires some limits (decided freely by democratic argument under the rule of law) in order to exist.
Universities should be trusted to exercise that degree of control themselves.
But intolerance of debate, of discussion, and of particular branches of scholarship should never be tolerated.
As the great political philosopher Karl Popper taught us, the only thing we should be intolerant of is intolerance itself.
That is especially true at universities.
Yet some American and British academics and students are themselves undermining freedom; paradoxically, they have the liberty to do so.
Meanwhile, universities in China and Hong Kong are faced with threats to their autonomy and freedom, not from within, but from an authoritarian government.
In Hong Kong, the autonomy of universities and free speech itself, guaranteed in the city’s Basic Law and the 50-year treaty between Britain and China on the city’s status, are under threat.
The rationale seems to be that, because students strongly supported the pro-democracy protests in 2014, the universities where they study should be brought to heel.
So the city’s government blunders away, stirring up trouble, clearly on the orders of the government in Beijing.
Indeed, the Chinese authorities only recently showed what they think of treaty obligations and of the “golden age” of Sino-British relations (much advertised by British ministers), by abducting a British citizen (and four other Hong Kong residents) on the city’s streets.
The five were publishing books that exposed some of the dirty secrets of China’s leaders.
On the mainland, the Chinese Communist Party has launched the biggest crackdown on universities since the aftermath of the killings in Tiananmen Square in 1989.
There is to be no discussion of so-called Western values in China’s universities. Only Marxism can be taught.
Did no one tell President Xi Jinping and his Politburo colleagues where Karl Marx came from?
The trouble these days is precisely that they know little about Marx but a lot about Lenin.
Westerners should take a closer interest in what is happening in China’s universities and what that tells us about the real values underpinning scholarship, teaching, and the academy.
Compare and contrast, as students are asked to do.
Do you want universities where the government decides what it is allegedly safe for you to learn and discuss?
Or do you want universities that regard the idea of a “safe space” – in terms of closing down debate in case it offends someone – as an oxymoron in an academic setting?
Western students should think occasionally about their counterparts in Hong Kong and China who must fight for freedoms that they take for granted – and too often abuse.
The New Brain Drain in Science
DUBAI – In December 2013, the Nobel laureate physicist Peter Higgs told The Guardian that if he were seeking a job in academia today, “I don’t think I would be regarded as productive enough.”
Having published fewer than ten papers since his groundbreaking work in 1964, Higgs believes that no university would employ him nowadays.
Academics are well acquainted with the notion of “publish or perish.”
They must publish their work in peer-reviewed journals increasingly often to climb the career ladder, protect their jobs, and secure funding for their institutions.
But what happens to scientists and other scholars, such as those in the Middle East, who have different research concerns from – and scant connections to – the professional journals that can make or break an academic/scientific career?
Scholars and institutions with high publishing rates in the established journals receive better productivity scores, which translate into bigger rewards, in terms of enhanced careers and greater research funding.
Whether the work they are publishing has a measurable impact on their field of study is, sadly, too often a secondary concern.
The incentives they face mean that quantity often comes before quality.
Academic journals determine the various disciplinary rankings that academic institutions are compelled to climb, which leads institutions to hire and retain only those scholars who can produce at high rates.
This has given rise to a deeper, twofold problem: academic journals have become disproportionately influential, and they have placed a premium on empirical research.
With respect to the first problem, journals are gradually replacing institutions as the arbiters of quality within academic communities.
Scholars in almost any discipline seeking jobs at “A-level” institutions must publish in a select few A-level journals that are seen as gateways.
These journals’ editorial boards increasingly privilege positivist theoretical work – meaning research that is based on empirical data analysis.
Qualitative research – such as ethnographies, participatory surveys, and case studies – is often designated as suitable only for B- and C-level journals.
Academics conducting empirical research have a big advantage over those carrying out qualitative work, because they can use efficient software and powerful computers to test their hypotheses quickly and account for different variables in data sets.
This kind of work can be cheaper, too, because a single data set can generate multiple journal articles.
To be sure, there is nothing wrong with scientific practices evolving along with technology, or with academics using richer data sets and better software.
But adoption of this quantitative approach should not be the single most important criterion for assessing scientific excellence and deciding career trajectories.
After all, knowledge is acquired in different ways, and empirical positivism is only one method in a larger epistemological inventory.
The positivist trend in science today is particularly problematic for developing countries, where data sets are scarce and often of poor quality.
Thus, scientists working in developing countries face a dilemma: either work on rich-world problems for which there is abundant data, or risk career advancement by conducting qualitative work that will not make it into A-level journals.
Academics who move from data-rich countries in Europe and North America to data-poor countries in the Middle East and elsewhere often face this problem.
As researchers at my institution in Abu Dhabi know, conducting surveys for qualitative research is feasible; but generating rich data from scratch for theory-building research is extremely difficult.
At the International Conference on Science and Technology Indicators this year, a French academic researching soil in Africa reported that only 5% of the published work in his field has originated from African researchers.
When he dug deeper into his own research, he found that 50% of what he had learned about African soil came from African researchers, who have not or could not publish their work in international academic journals.
Countries where English is not the lingua franca are particularly disadvantaged in science, not because they lack academic excellence, but because English-language journals call the shots.
Non-English academic journals simply do not command the same attention in the science community.
As a result, the scope of research topics that many countries can undertake is limited, and they must struggle to retain scientific talent.
This is particularly true in the Middle East, where governments are struggling to diversify their economies, in order to make them more resilient.
As English-language empirical-research journals consolidate their hold on the channels that determine whether or not a scientist will have a successful career, developing countries will have to invest heavily in their own data infrastructure to place domestic researchers on a more competitive footing.
But even – or especially – if developing countries do make such investments, much will be lost to science.
With (mostly) United States-based academic journals reigning over global science, no one has to move to become part of a new brain drain, whereby scientists’ research priorities, problems, and methods gravitate to the dominant positivist epistemology, at the expense of all alternatives.
Polluters Must Pay
NEW YORK – When BP and its drilling partners caused the Deepwater Horizon oil spill in the Gulf of Mexico in 2010, the United States government demanded that BP finance the cleanup, compensate those who suffered damages, and pay criminal penalties for the violations that led to the disaster.
BP has already committed more than $20 billion in remediation and penalties.
Based on a settlement last week, BP will now pay the largest criminal penalty in US history – $4.5 billion.
The same standards for environmental cleanup need to be applied to global companies operating in poorer countries, where their power has typically been so great relative to that of governments that many act with impunity, wreaking havoc on the environment with little or no accountability.
As we enter a new era of sustainable development, impunity must turn to responsibility.
Polluters must pay, whether in rich or poor countries.
Major companies need to accept responsibility for their actions.
Nigeria has been Exhibit A of corporate environmental impunity.
For decades, major oil companies, including Shell, ExxonMobil, and Chevron, have been producing oil in the Niger Delta, an ecologically fragile environment of freshwater swamp forests, mangroves, lowland rainforests, and coastal barrier islands.
This rich habitat supports remarkable biodiversity – or did before the oil companies got there –&#160;and more than 30 million local inhabitants, who depend on the local ecosystems for their health and livelihoods.
Twenty years ago, the International Union for Conservation of Nature and Natural Resources classified the Niger Delta as a region of high biodiversity of marine and coastal flora and fauna – tree species, fish, birds, and mammals, among other forms of life – and therefore rated it as a very high priority for conservation.
Yet it also noted that the region’s biodiversity was under massive threat, with little or no protection.
The global companies operating in the delta have spilled oil and flared natural gas for decades, without regard for the natural environment and the communities impoverished and poisoned by their actions.
One estimate puts the cumulative spills over the past 50 years at approximately 10 million barrels – twice the size of the BP spill.
The data are uncertain: there have been many thousands of spills during this period – often poorly documented and their magnitude hidden or simply unmeasured by either the companies or the government.
Indeed, just as BP was being hit with new criminal penalties, ExxonMobil announced yet another pipeline leak in the Niger Delta.
The environmental destruction of the delta is part of a larger saga: corrupt companies operating hand in hand with corrupt government officials.
The companies routinely bribe officials to gain oil leases, lie about output, evade taxes, and dodge responsibility for the environmental damage that they cause.
Nigerian officials have become fabulously wealthy, owing to decades of payoffs by international companies that have plundered the delta’s natural wealth.
Shell, the largest foreign operator in the Niger Delta, has been criticized repeatedly for its egregious practices and its unwillingness to be held to account.
Meanwhile, the local population has remained impoverished and beset by diseases caused by unsafe air, poisoned drinking water, and pollution in the food chain.
Local lawlessness has led to gang warfare and persistent illegal tapping into the pipelines to steal oil, leading to further massive oil spills and frequent explosions that kill dozens, including innocent bystanders.
In the colonial era, it was the official purpose of imperial power to extract wealth from the administered territories.
In the post-colonial period, the methods are better disguised.
When oil companies misbehave in Nigeria or elsewhere, they are protected by the power of their home countries.
Don’t mess with the companies, they are told by the United States and Europe.