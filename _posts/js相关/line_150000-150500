LONDON – Yemen has suddenly joined Afghanistan and Pakistan as a risk to global security.
Indeed, it is increasingly seen as a nascent failed state and potential replacement host for Al Qaeda.
The attempted bombing of a Detroit-bound airliner on Christmas Day by a young Nigerian man trained by Al Qaeda in Yemen appeared to open the West’s eyes to the country’s problems.
Following that failed attack, US President Barack Obama and British Prime Minister Gordon Brown jointly pushed a conference in London to propose solutions for the previously overlooked crises in Yemen.
But if the conference focuses too narrowly on Al Qaeda’s presence in Yemen, it will do more harm than good.
Instead, the conference must aim to address broader issues of political and social stability within Yemen.
Al Qaeda is not the primary danger to Yemen’s security and stability, but Yemen’s geography and political problems are well suited to its activities.
A particularly attractive feature is the prevalence of the severe Wahhabi religious dogma, which was exported to Yemen by Saudi Arabia but now provides fertile ground for recruiting disaffected young Yemeni men for assaults on Saudi Arabia.
Yemen ’s central problems are two: the ongoing civil war that the government is waging against the Houthi tribe in the country’s north, and the suppression of a secessionist movement in the south.
It is the Yemeni government’s inability to find a political solution to these problems that has led Yemen to the brink of fragmentation.
So far, Obama and Brown seem unable to fully grasp the fact that Yemen’s problems go well beyond Al Qaeda’s presence in the country.
As a result, they appear to be playing into Yemeni President Ali Abdullah Saleh’s hands.
Saleh wants to use the London conference as a means to leverage Western backing, particularly military aid, to pursue his wars against the Houthis and the southern secessionists.
Saleh has regularly employed the danger of Al Qaeda to obtain additional financial and security support from both the West and Saudi Arabia.
For him, the attempted Christmas Day bombing was a gift from heaven.
Saleh’s dilemma is that Western aid may now come with increased interference in Yemen’s internal affairs at a time when he wants the world to turn a blind eye to his conduct of the country’s civil wars.
The West and Saleh do not have the same enemy.
Al Qaeda is the West’s enemy, while Saleh’s true enemies are the Houthis and the separatists of the south.
But if the West is to curtail Al Qaeda’s activities in Yemen, it will need to push Saleh into reaching accommodations with both the Houthis and the southerners, and this will undoubtedly mean sharing power with them.
Saleh will undoubtedly resist such an effort.
Last December, Saleh called for national dialogue, but on his own terms: the Houthis and the southern leaders are to be excluded from the discussions unless they support the Yemeni constitution that has kept Saleh in power for decades.
But Saleh’s hardline approach is failing.
More than half of Yemen’s territory is falling out of government control.
The US should not be surprised by any of these developments because American involvement in Yemen is not new.
Al Qaeda in Yemen has been targeted since the USS Cole was bombed while in the port of Aden in 2000.
Missile strikes by US drones last December in Abein and Shabwa killed a number of Al Qaeda members, as well as civilians.
Fighting Al Qaeda in Yemen through such means may temporarily reduce terrorism, but it will not end it.
The real question is whether the West will address Yemen’s failed political and military strategies, which are the root cause of Al Qaeda’s mushrooming presence in the country.
Only if Western intervention aims to rescue the Yemeni state from itself will there be any possibility to contain Al Qaeda.
And it is not just the Yemeni state that is at fault.
Yemen’s neighbors have also played a role.
Saudi Arabia exported both its Wahhabism and Al Qaeda to Yemen by funding thousands of madrassas where fanaticism is taught.
Moreover, since the 1991 Gulf War, Saudi Arabia and Kuwait have been expelling Yemeni workers.
Last month alone, 54,000 Yemeni workers were expelled from Saudi Arabia.
Although Yemen is geographically part of the Arabian Peninsula, it was excluded from the Gulf Cooperation Council, primarily because its size – it is the most populous state on the peninsula – would have given it great influence.
In fact, the Yemeni population exceeds the population of all six GCC members combined.
Saleh received a strong endorsement from the GCC last December for his domestic wars, and Saudi Arabia has been in direct military confrontation with the Houthis, its army having crossed Yemen’s border.
But the GCC members’ failure to open their economies – which are always in need of guest workers – to Yemen’s young men is short-sighted.
The US and Britain, both patrons of the GCC, must encourage its members to include Yemen if they want to solve its problems. Yemenis are known as skilled labourers.
So, instead of exporting religious radicalism to Yemen, importing its manpower could neutralize Yemen’s problems.
The forthcoming London conference could prove to be either a trap for the West or the beginning of a true effort at the kind of domestic reform that can prevent Yemen from becoming another Afghanistan.
If the West buys into Saleh’s depiction of a war against Al Qaeda, it will be trapped into supporting him and his failed policies.
But if it looks beyond terrorism to the root causes of the problem, and presses Saleh to begin to share power, Yemen need not become another safe haven for terrorists.
Reserve Reform
NEW YORK – Both China and the United Nations Commission on Reforms of the International Monetary and Financial System have called for a new global reserve system.
That issue should be at the top of the agenda when the IMF’s International Monetary and Financial Committee next meet.
The essential idea is quite simple: in the long run, an international monetary system cannot be built on a national currency – a point made a half-century ago by the Belgian-American economist Robert Triffin.
Recognition of this fundamental problem was the reason why the IMF’s Special Drawing Rights (SDRs) were created in the 1960’s.
The dollar standard with which the world has lived since the early 1970’s has three fundamental flaws.
First, as with all systems that preceded it, it puts the burden of adjustment on deficit countries, not on surplus countries.
The main exception is the United States, which, thanks to its reserve currency status, has so far been able to finance its deficit by issuing dollar liabilities that are held by the rest of the world.
Second, the system is unstable, because it makes the major reserve currency’s value dependent on US macroeconomic policy and the vagaries of the US balance of payments and associated domestic deficits.
Since the abandonment of gold-dollar parity in 1971, the world has experienced increasingly intense cycles in the value of the dollar and the US current account.
The dollar has lost what any reserve asset should have: a stable value.
The governor of China’s central bank recently emphasized this basic point.
Third, the current system is inequitable, because it forces a transfer of resources from developing countries to the industrial nations that provide reserve currencies.
This transfer has dramatically increased over the past two decades. Developing countries’ main defense against world financial instability has been to accumulate international reserves.
At the end of 2007, developing countries, excluding China, held reserves equivalent to 20.6% of their GDP, compared to just 3.7% in 1990.
This generated a huge asymmetry in the world economy, as industrial countries, excluding Japan, hold only 2.6% of GDP in reserves.
One basic reason is that the only “collective insurance” available is limited and highly conditional IMF lending.
It must be emphasized that a system based on competing reserve currencies would not solve the instability and inequities of the current system. In fact, it would add another one: the instability of the exchange rates among major reserve currencies.
Indeed, this problem is already present in the current system.
The deficiencies of current arrangements are why the world monetary system should be based on a truly global reserve currency: a fiduciary currency backed by the world’s central banks.
This is what was hoped for when SDRs were created in the 1960’s, and this process must be completed by transforming the SDRs into such a global currency.
A major advantage of an SDR-based system is that it would provide a mechanism for the IMF to provide finance with its own resources in an agile way during crises, thus operating in the same way as central banks have been doing on a massive scale in recent months.
It would also be a much better mechanism with which to finance the IMF during crises than the credit lines to the IMF from a few countries (“arrangements to borrow”) that the G-20 is again advocating, as it is truly multilateral financing that does not depend on any individual country.
For such a program to work, it is essential that developing countries recognize that IMF financing is good “collective insurance,” so that their demand for foreign-exchange reserves would decline.
This means that the IMF would have to lend rapidly during balance-of-payments crises, and do so without the overburdening conditionality of the past, particularly when crises stem from rapid reversal of capital flows or a sharp deterioration in terms of trade.
The IMF took steps in this direction in March, particularly by creating the Flexible Credit Line for crisis-prevention purposes, as well as expanding other credit lines and overhauling conditionality (relying more on ex-ante conditionality and eliminating structural performance criteria).
The major problem with the new credit line is that it runs the risk of unduly dividing developing countries into two categories, with significant additional risks for the latter.
A better alternative would be to go back to John Maynard Keynes’s proposal of an overdraft (or, in IMF terminology, drawing) facility for all member countries, with countries that continue to use it eventually having to apply to a formal lending program.
The overdraft facility could again be financed with counter-cyclical issues of SDRs.
The reform should also allow for a mechanism for countries to exchange their current dollar (and euro, yen, and pound) assets for SDRs, thus avoiding the disruptions that could be generated by the transition to the new system.
As Fred Bergsten has reminded us, the mechanism is already available in the form of the “substitution account” that was negotiated in the IMF in 1980.
It is time to broaden the agenda of global financial reform, which so far has focused on an essential but still limited set of issues, particularly financial regulation.
Reform of the global reserve system must be part of that broader agenda.
Hugging a Burning Tree
PRAGUE – We are all brought up to recycle paper to save trees.
We get countless e-mail admonitions: “Please consider the environment before printing.”
Indeed, environmentalism was born with a call to preserve the forests.
But now, in the name of saving the planet from climate change, environmentalists are proposing an immense global campaign to cut down and burn trees and scrubs in order to reduce fossil-fuel use.
The initiative could be dismissed as a weird irony, if it weren’t for its phenomenal costs, which include likely destruction of biodiversity, increased water use, and reduced global food production.
And it may end up increasing global CO2 emissions to boot.
When most people think of renewable energy sources, they imagine solar panels and wind turbines.
Globally, however, solar and wind are only a small part of total renewables – less than 7% in 2010.
Hydropower is a much bigger player, at 17%.
But the most important by far is biomass – humanity’s oldest fuel makes up 76% of today’s renewable energy and 10% of all energy.
About 60% of this is wood, twigs, and dung, used by almost three billion people who lack access to modern fuels – and resulting in terrible air pollution and millions of deaths.
But the West uses the other 40% of biomass to produce heat, and it will increasingly use it to generate electricity.
This makes sense, because solar and wind power are inherently unreliable – we still need electricity on cloudy days or when the wind dies down.
Biomass (along with hydropower) can be used to smooth the fluctuations inherent to wind and solar.
Biomass is experiencing a revival, because it is considered CO2-neutral.
The conventional wisdom is that burning wood only releases the carbon sucked up while the tree was growing, and hence the net climate effect is zero.
But a growing number of voices challenge this view.
The European Environment Agency’s Scientific Committee has called it a “mistaken assumption” based on “a serious accounting error,” because if a forest is cut down to burn wood, it will take a long time for new growth to absorb the CO2 emissions.
The climate effect could be a net increase in emissions if forests are cleared to create energy-crop plantations.
According to the Committee’s members, “the potential consequences of this bioenergy accounting error are immense.”
Environmentalists’ plan to obtain 20-50% of all energy from biomass could mean a tripling of current biomass consumption, placing its production in direct competition with that of food for a growing global population, while depleting water supplies, cutting down forests, and reducing biodiversity.
An academic paper published last year makes the point clear in its title: “Large-scale bioenergy from additional harvest of forest biomass is neither sustainable nor greenhouse gas neutral.”
Its authors point out that while the Industrial Revolution caused climate change, reliance on coal was actually good for forests, because our forebears stopped raiding forests for wood.
This is one of the major reasons that forests in Europe and the United States have recovered – and it is why many forests in developing countries are threatened.
The developed world’s re-enchantment with biomass could take it down a similar road.
But the biggest problem is that biomass production simply pushes other agricultural production elsewhere.
Studies are just beginning to estimate the impact.
In Denmark, a group of researchers estimated by how much various energy crops would reduce CO2 emissions.
For example, burning a hectare of harvested willow on a field previously used for barley (the typical marginal crop in Denmark) prevents 30 tons of CO2 annually when replacing coal.
This is the amount that proud green-energy producers will showcase when switching to biomass.
But burning the willow releases 22 tons of CO2.
Of course, all of that CO2 was soaked up from the atmosphere the year before; but, had we just left the barley where it was, it, too, would have soaked up quite a bit, lowering the reduction relative to coal to 20 tons.
And, in a market system, almost all of the barley production simply moves to a previously unfarmed area.
Clearing the existing biomass there emits an extra 16 tons of CO2 per year on average (and this is likely an underestimate).
So, instead of saving 30 tons, we save four tons at most.
And this is the best-case scenario.
Of the 12 production modes analyzed, two would reduce annual CO2 emissions by only two tons, while the other ten actually increase total emissions – up to 14 tons per year.
At the same time, we are paying a king’s ransom for biomass.
Germany alone spends more than $3 billion annually, or $167 per ton of avoided CO2 emissions, which is more than 37 times the cost of carbon reductions in the European Union Emissions Trading System.
And the estimate of avoided emissions ignores indirect land-use changes, making the likely real cost at least eight times higher.
Ten years ago, the EU and the US embraced biofuels as a way to combat global warming. Today, the US turns 40% of its maize output into ethanol to be burned in cars.
This has driven up food prices and caused tens of millions of people to starve, while costing more than $17 billion each year in subsidies and causing agricultural deforestation elsewhere in the world, with more total CO2 emissions than the entire savings from the ethanol.
Biofuels have become an almost unstoppable and unmitigated disaster.
We need to confront the next – and potentially much bigger – biomass boondoggle.
Yes, we should turn waste into energy and be smart about agricultural leftovers.
But we are about to diminish biodiversity, over-extract water, make food more expensive, and waste hundreds of billions of dollars – all while cutting down trees to burn them and potentially increasing CO2 emissions.
We have been brought up to know and do better.
The World’s North Korean Test
DENVER – The most recent North Korean nuclear test is the most dangerous of the three to date. How the international community responds, in both word and deed, will say much about the world we live in.
And, whether the Chinese like it or not, how they respond will speak volumes about what kind of role China will play in global governance.
While details are not yet fully known, the test suggests substantial progress on the part of North Korea’s scientists in increasing the yield of their weaponry.
The October 2006 test suggested the possibility of a faulty design, while there were questions about whether the 2009 effort was even nuclear in nature.
But North Korea’s test this month was by all measures the real thing.
Moreover, though it is hard to evaluate North Korean bluster on these occasions, there appears to be reason to be concerned that North Korean scientists have also made progress in miniaturizing their design, a step needed to mount a nuclear device on a missile.
Most worrisome of all, we cannot rule out that North Korea may have succeeded in enriching uranium to weapons-grade levels.
We have scant information about this, beyond some limited information about international acquisitions and some detection of traces of highly enriched uranium (HEU) on materials brought out of North Korea.
If North Korea is using HEU in its bomb design, we have little knowledge about where it comes from and how much of it they have, or could have in the future.
There are some who would urge that we all remain calm (good advice in most circumstances) and somehow take comfort in the fact that North Korea still faces many serious technical hurdles before it can successfully add nuclear weapons to its considerable conventional arsenal.
After all, it took the United States 12 years to mount a weapon on a missile.
But all indications are that the North Koreans have accelerated their testing schedule and have made nuclear weapons a top priority.
North Korean propaganda is not always a reliable indicator, but the recent upswing in attacks on the US, including a bizarre video to the tune of “We are the World” should not be completely laughed off.
Gone are the hopes that the boy-dictator Kim Jong-un and his regents are more interested in economic development than they are in following the Kim dynasty’s traditional military-first policies.
North Korea’s latest nuclear test is thus a test in many senses of the word.
For starters, it tests whether the 43-year-old Non-Proliferation Treaty is still viable.
After all, the NPT’s objective is to “prevent the spread of nuclear weapons and weapons technology, to promote cooperation in the peaceful uses of nuclear energy, and to further the goal of achieving nuclear disarmament and general and complete disarmament.”
Some might say that just two new wannabes, North Korea and Iran, is not so bad.
But, when it comes to nuclear weapons, even one is more than enough.
The latest detonation also tests whether the world can make addressing North Korea’s dangerous aspirations a priority by uniting around a common policy that is more than rhetorical in its condemnation.
One thing we know about the North Koreans is that, unless they can use such rhetoric in their own propaganda (they got far more traction out of former US President George W. Bush’s infamous “axis of evil” remark than he ever did), they simply don’t care what is said about them.
Similarly, threatening North Korea with isolation, the hardy perennial of diplomatic pressuring, does not work, because isolation is exactly what the North Koreans want.
What the international community needs to do is to come together on a program of action aimed at producing a result.
Economic sanctions alone are dubious against the world’s most sanctioned country.
And threatening more sanctions in the future raises the question of why those sanctions have not already been imposed.
In recent days, US President Barack Obama’s administration has done well to encourage countries big and small to speak out.
Clearly, no single country can solve this problem.
But encouraging countries to make statements and adhere to the sanctions regime will not get us to the goal that we need to achieve.
Every country needs to feel the threat from North Korea, and thus to feel the need to participate in addressing it, regardless of their relationships with or attitudes about the US.
Enmity toward the US should not translate into forbearance toward North Korea.
There are also those – a dwindling minority, fortunately – who continue to suggest that more needs to be done on the negotiating track.
In fact, that track should remain open, but no one should be under the illusion that negotiations are somehow the missing piece of the puzzle.
North Korea looked at every offer on the table during the six-party talks that began in 2003 – a peace treaty, economic and energy assistance, and membership in a regional association – and walked away.
Similarly, bilateral requests that North Korea had made for years, such as repeal of its designation by the US as an enemy in the context of the “Trading with the Enemy Act,” once delivered, were dismissed as unimportant.
Such offers remain on the table, gathering dust, but it has been entirely North Korea’s decision not to pursue them.
Finally, the real test here may be not so much what the world will do, but rather what China, North Korea’s sole remaining ally, will do.
No one expects China to resolve this problem by itself.
But the world is watching China and its incoming leadership for clues about what kind of member of the international community China really aspires to be.
Responsibly Destroying the World’s Peasantry
BRUSSELS – The World Bank, the United Nations Food and Agricultural Organization (FAO), the International Fund for Agricultural Development (IFAD), and the UN Conference on Trade and Development (UNCTAD) Secretariat recently presented seven “Principles for Responsible Agricultural Investment.” The principles seek to ensure that large-scale land investments result in “win-win” situations, benefiting investors and directly affected communities alike.
But, though well-intended, the principles are woefully inadequate.
It has been several years since private investors and states began buying and leasing millions of hectares of farmland worldwide in order to secure their domestic supply of food, raw commodities, and biofuels, or to get subsidies for carbon storage through plantations.
Western investors, including Wall Street banks and hedge funds, now view direct investments in land as a safe haven in an otherwise turbulent financial climate.
The scope of the phenomenon is enormous.
Since 2006, between 15 and 20 million hectares of farmland, the equivalent of the total arable surface of France, have been the subject of negotiations by foreign investors.
The risks are considerable.
All too often, notions such as “reserve agricultural land,” or “idle land,” are manipulated out of existence, sometimes being used to designate land on which many livelihoods depend, and that is subject to long-standing customary rights.
The requirement that evictions take place only for a valid “public purpose,” with fair compensation, and following consultation of those affected, is honored more in the breach than in the observance.
In Africa, rural land is generally considered to be state-owned, and is treated by governments as if it were their own.
In Latin America, the gap between large landowners and small peasants is widening.
In South Asia, many populations are currently being driven off their ancestral land to make room for large palm-oil plantations, special economic zones, or re-forestation projects.  
The set of principles that have been proposed to discipline the phenomenon remain purely voluntary.
But what is required is to insist that governments comply fully with their human rights obligations, including the right to food, the right of all peoples to freely dispose of their natural wealth and resources, and the right not to be deprived of the means of subsistence.
Because the principles ignore human rights, they neglect the essential dimension of accountability.
There is also a clear tension between ceding land to investors for the creation of large plantations, and the objective of redistributing land and ensuring more equitable access to it.
Governments have repeatedly committed themselves to these goals, most recently at the 2006 International Conference on Agrarian Reform and Rural Development.
The underlying problem runs deeper than how the principles have been formulated.
The promotion of large-scale land investment is based on the belief that combating hunger requires boosting food production, and that supply has been lagging because of a lack of investment in agriculture.
Hence, if investment can be attracted to agriculture, it should be welcomed, and whichever rules are imposed should encourage it, not deter it.
But both the diagnosis and the remedy are incorrect.
Hunger and malnutrition are not primarily the result of insufficient food production; they are the result of poverty and inequality, particularly in rural areas, where 75% of the world’s poor still reside.
In the past, agricultural development has prioritized large-scale, capitalized forms of agriculture, neglecting smallholders who feed local communities.
And governments have failed to protect agricultural workers from exploitation in an increasingly competitive environment.
It should come as no wonder that smallholders and agricultural laborers represent a combined 70% of those who are unable to feed themselves today.
Accelerating the shift towards large-scale, highly mechanized forms of agriculture will not solve the problem.
Indeed, it will make it worse. The largest and best-equipped farms are highly competitive, in the sense that they can produce for markets at a lower cost.
But they also create a number of social costs that are not accounted for in the market price of their output.
Smallholders, by contrast, produce at a higher cost. They are often very productive by hectare, since they maximize the use of the soil, and achieve the best complementary use of plants and animals.
But the form of agriculture that they practice, which relies less on external inputs and mechanization, is highly labor-intensive.
If smallholders compete in the same markets as the large farms, they lose.
To re-launch agriculture in the developing world would require an estimated $30 billion per year, representing 0.05% of global GDP.
But how much is invested in agriculture matters less than the type of agriculture that we support.
By supporting further consolidation of large-scale monocultures in the hands of the most powerful economic actors, we risk widening further the gap with small-scale, family farming, while pushing a model of industrial farming that is already responsible for one-third of man-made greenhouse-gas emissions today.
It is regrettable that, instead of rising to the challenge of developing agriculture in a way that is more socially and environmentally sustainable, we act as if accelerating the destruction of the global peasantry could be accomplished responsibly. 
Europe’s Green Recovery
BRUSSELS – The need for clean energy has returned to the top of the global economic agenda.
China’s new leadership now seems to recognize that the thick, hazardous smog that has come to define Beijing and other cities is more than a pollution problem; it is a result of an excessive emphasis on short-term economic planning.
Likewise, in his second inaugural address, US President Barack Obama discussed climate change more than any other issue, saying, “We cannot cede to other nations the technology that will power new jobs and new industries.”
At the World Economic Forum in Davos, International Monetary Fund Managing Director Christine Lagarde and World Bank President Jim Yong Kim surprised business and government leaders with their warnings that genuine economic recovery would be impossible without serious action on climate change.
And, at the most recent EU summit, leaders agreed to commit at least 20% of their entire common budget to climate-related spending.
These developments suggest that global leaders are finally beginning to understand that, beyond the global economic crisis, the world is experiencing a social and employment crisis, as well as a climate and resource crisis.
And none can be resolved without addressing the others.
Moreover, Europe’s main commercial competitors have begun to recognize that pursuing short-term development policies, while ignoring long-term threats to the global economy, is both irresponsible and a strategic mistake for those who aspire to global leadership in the twenty-first century.
Although Europeans have known this for decades, in the wake of the recent economic crisis, immediate goals took priority over –&#160;and often at the expense of – long-term objectives.
With the European Union’s economy growing more slowly than those of its major competitors, its leaders must take a more far-sighted approach to restoring – and preserving – its members’ growth potential.
They must begin by identifying not only what is undermining Europe’s competitiveness today, but also those factors that are putting its long-term prospects at risk.
Analysts often point to Europe’s costly social-welfare systems, high labor costs, and increasing tax rates as a drag on competitiveness.
But other, less widely discussed factors must be considered –&#160;particularly the costs of delayed action on climate change.
For example, Unilever CEO Paul Polman reported that extreme weather cost his company $250-300 million in 2012.
Once considered an issue for the future, action on climate change has become increasingly urgent, as the outlays required to mitigate its negative effects have grown.
Furthermore, with record-high unemployment rates, Europe needs jobs in dynamic, competitive industries that cannot easily be outsourced.
The European Commission has identified the green economy as one of the areas with the highest job-creation potential.
At the same time, Europe’s growing dependence on imported fossil fuels is a further hindrance to competitiveness.&#160;In 2011, the EU’s combined trade deficit was €150 billion ($200 billion).
But the combined oil-import bill was more than double that – €315 billion –and the official figure for 2012 is expected to exceed €335 billion.
If Europe does not address these challenges, it risks being left behind.
But Europe cannot build an industrial strategy on cheap energy.
Unlike the United States and China, which the International Energy Agency estimates possess the world’s largest shale-gas deposits, Europe cannot rely on its limited energy reserves to lower prices – especially given that its greater population density makes extraction more difficult, and thus prohibitively expensive.
As a result, Europe will remain a net energy importer.
And, given rising global demand for oil, particularly in developing countries, energy-import prices will remain high.
Meanwhile, China – the world’s leading investor in renewable-energy projects – is undergoing a transformation from the world’s low-cost factory to a global leader in green innovation and a major exporter of clean technologies.
In the contest for this global market, Europe cannot compete on price alone.
But Europe does have options.
EU leaders can build an economy that is less dependent on imported energy through increased efficiency and greater reliance on domestically produced clean energy.
At the same time, they should tackle other major threats to Europe’s long-term competitiveness, including low productivity, an incomplete internal market, and insufficient innovation.
But Europe must not heed short-sighted calls to relax its environmental standards, which some claim harm its competitiveness vis-à-vis countries with looser rules.
Given their global reputation as guarantors of quality, Europe’s high environmental standards are crucial to its future competitiveness, and should therefore be actively promoted, particularly in trade agreements.
While trade deals have often come at the expense of stronger domestic climate action, the EU’s new trade agreement with Singapore aims to boost trade and investment in clean-energy technologies and promote green public tendering.
This should serve as an environmental benchmark for future agreements – including with the US, despite some American constituencies’ expectations that EU standards could be relaxed in a bilateral trade deal.
By maintaining high environmental standards, and promoting such standards among its trade partners, Europe can bolster the global market for clean-energy technologies.
As a major player in the green-technology market – set to triple in value by 2020 – Europe could regain competitiveness and secure its role in the future global economy.
To be successful, Europe must play to its strengths.
The EU’s most competitive economies are the most innovative and energy-efficient, with the most highly educated workforces.
Indeed, for Europe, cheap is not the answer; quality and innovation are.
Europe’s major competitors are turning climate change into an opportunity to encourage growth and create high-quality jobs in rapidly innovating economic sectors.
If EU leaders hesitate to take action on climate change, they will be sabotaging their own economy’s prospects for sustainable recovery.
The American Comeback Kid
VIENNA – As is customary at the start of a new year, imposing statistics and trend forecasts are being trumpeted worldwide.
For example, in 2016, China is expected to replace the United States as the world’s largest economy.
And, by 2040, India’s population will have reached 1.6 billion, surpassing China’s, which will have stagnated a decade earlier.
Perhaps the most startling projection is that the US will become an energy exporter by 2020, and will become energy self-sufficient 15 years later, owing to the plentiful supply of inexpensive shale gas and the discovery of massive oil reserves everywhere from North Dakota to the Gulf of Mexico.
Despite opposition from environmental groups, these reserves will be easier to exploit than those in Europe, because they are largely located in sparsely populated areas.
As a result, energy will be significantly cheaper in the US than in Europe or China for the foreseeable future.
Indeed, shale-gas extraction is so economically favorable that even American gas exported to Europe would cost 30% less than what the Russian energy giant Gazprom currently charges.
Cheap energy provides a powerful incentive for energy-intensive industries – from steel and glass to chemicals and pharmaceuticals – to locate in the US.
In fact, the decreased cost of manufacturing in America, combined with the country’s business-friendly regulations, strong rule of law, and political stability, will eliminate the competitive advantage that has driven China’s rapid economic growth over the last several decades.
Meanwhile, American universities still attract the world’s best and brightest in many fields, most notably in science and technology.
And the country’s other longstanding advantages – flexibility, capacity for renewal, economic mobility, international regulatory strength, and the world’s main reserve currency – remain in place.
Given these favorable conditions, the US has already begun “on-shoring” its industry – a process that will most likely continue for several decades.
As other advanced economies become increasingly services-based, the US is reindustrializing.
The resulting added value will bolster policymakers’ ability to find long-term solutions to persistent problems, including an inefficient health-care system, inadequate primary and secondary education, and blatant social injustice.
Success in these areas would further enhance America’s appeal as an industrial center.
As part of the Harvard Business School’s US Competiveness Project, Michael Porter and Jan Rivkin recently published an eight-point plan, which could be implemented within the next two to three years.
Each proposed measure has generated broad, bipartisan agreement among policymakers (at least behind closed doors).
The plan highlights the need to take advantage of the opportunities afforded by shale gas and newly discovered oil reserves.
Low-cost domestic energy could help to lower the trade deficit, spur investment, and decrease America’s economic exposure to volatile oil-exporting countries.
A strong federal regulatory framework could help to ensure this result, while minimizing the environmental and safety risks associated with extraction.
Other proposals include easing the immigration of highly skilled individuals, particularly graduates from US universities; addressing distortions in international trade and investment; developing a more sustainable federal budget framework; streamlining taxes and regulations; and initiating an ambitious infrastructure program.
By pursuing these strategies, President Barack Obama could restore America’s position as the engine of the global economy.
But implementing the eight policy proposals would also widen further the wealth gap between the US and Europe, which has been growing for the last three decades.
In 1980-2005, the US economy grew by a factor of 4.45 – a level that no major European economy even approached.
This recognition has already dampened US policymakers’ support for the Arab Spring uprisings: witness Obama’s hesitation to intervene in Libya and his unwillingness, at least so far, to involve America directly in Syria’s bloody civil war.
Although the Arab Spring’s historic significance was initially likened to that of the fall of the Berlin Wall, mounting concern about the Muslim Brotherhood’s increasing political influence is overshadowing the positive potential of change in the region.
Likewise, while the US will not relinquish its bilateral relationship with Israel, relations between Israeli Prime Minister Binyamin Netanyahu and Obama have reached a new low.
In this context, a major American peace initiative in the Middle East is unlikely in the foreseeable future.
Meanwhile, America’s former rival, Russia, is struggling to restore its hegemony over many of the ex-Soviet countries.
And conditions in Africa and Latin America are generally stabilizing.
Given this, America’s foreign-policy priorities have shifted to the Asia-Pacific region, where the most pressing economic, political, and security challenges – including the threat of North Korean missiles and rising tensions between China and its neighbors over competing sovereignty claims in the South and East China Seas – are emerging.
Other global challenges appear relatively minor in comparison.
Although the weight of global politics, economics, and, in turn, influence is largely shifting from the Atlantic to the Pacific, it would be a mistake to underestimate America’s role in the new world order.
America never really stepped out of the spotlight, and it will continue to play a leading role.
Anti-Poverty 2.0
ROME – Global leaders have touted the apparent success of achieving in 2010 – well ahead of the 2015 target – the Millennium Development Goal of halving the share of people who were living below the poverty line in 1990.
But, amid enduring poverty, rising inequality, and lackluster growth in many developing countries, the success of past anti-poverty policies and programs appears dubious.
In fact, outside of East Asia, progress has been modest, with the situation worsening in some countries and regions – despite several economic-growth spurts, sustained expansion in some large developing countries, and public commitments by the international community to the 2000 Millennium Declaration, which led to the MDGs.
This mixed record calls into question the efficacy of conventional poverty-reduction policies, often identified with the Washington Consensus, which transformed the discourse on poverty in the 1980’s.
Washington Consensus reforms – including macroeconomic stabilization (defined as low-single-digit inflation) and market liberalization – were supposed to reduce poverty by accelerating economic growth.
But little attention was paid to poverty’s structural causes, such as inequality of assets and opportunities, or the unequal distributional consequences of growth.
And, because unskilled workers tend to lose their jobs first in economic downturns, while employment generally lags behind output recovery, reduced public investment in health, education, and other social programs ultimately increased the vulnerability of the poor.
Not surprisingly, therefore, the fallout from the global economic crisis that erupted in 2008 – the worst since the Great Depression of the 1930’s – has prompted experts, policymakers, and the international financial institutions to rethink poverty.
Many are rejecting once-dominant perspectives on poverty and deprivation, warning that they lead to ineffective policy prescriptions.
For example, the United Nations’ Report on the World Social Situation: Rethinking Poverty,and its companion book Poor Poverty: The Impoverishment of Analysis, Measurement, and Policies, havesought to advance the debate on poverty by examining the conventional policy framework and assessing popular poverty-reduction programs.
They affirm the need for a shift away from the fundamentalist free-market thinking that has dominated poverty-reduction strategies in recent decades toward context-sensitive measures to promote sustainable development and equality.
Indeed, these reports challenge the prevailing approach, which has left more than one billion people living below the poverty line (defined as the purchasing-power-parity equivalent of $1.25 per day in 2005), and has failed to prevent economic growth from stalling in most countries.
Meanwhile, inequality has increased worldwide – even in countries that have experienced rapid economic expansion, notably China and India.
While growth is usually needed for poverty reduction, it does not necessarily translate into job creation, as many countries’ recent record of jobless or job-poor growth demonstrates.
The UN encourages governments to assume a more proactive development role, which would entail an integrated policymaking approach aimed at promoting structural change while reducing inequality, vulnerability, and economic insecurity.
Growth must become more stable, with a consistently counter-cyclical macroeconomic policy stance, prudent capital-account management, and greater resilience to external shocks.
Generally, economies that have succeeded in terms of both economic growth and poverty reduction over the last three decades have done so by adopting pragmatic, heterodox policies.
Often invoking investor- and market-friendly language, they have generally encouraged private investment, especially in desired economic activities, such as those that create more job opportunities or offer increasing returns to scale.
But aid conditionality and treaty commitments have significantly constrained policymaking in most developing countries, especially the poorest.
In particular, slow growth and revenue losses, owing to economic liberalization programs, have reduced the scope for fiscal policy, with serious consequences for poverty and destitution.
This trend must be reversed.
Moreover, while the programs that most donors like – such as micro-credit, formalization of land titles, and governance reforms – have sometimes helped to ameliorate the conditions facing the poor, they have not reduced poverty significantly.
Leaders must consider, design, and implement pragmatic and innovative alternatives, rather than continue to rely on ineffective policies and programs.
In recent decades, social policies have increasingly involved some form of means testing of eligible beneficiaries, ostensibly to enhance cost effectiveness.
But, in general, universal social policies have been much more effective and politically sustainable, while policies targeting the poor, or the “poorest of the poor,” have often been costly and neglectful of many of those in need.
The right to social security enshrined in the Universal Declaration of Human Rights requires universal social protection to ensure the well-being of all, including people living in poverty and those at risk of poverty.
Social policy, provisioning, and protection must therefore be integral to development and poverty-reduction strategies.
In most countries, a basic social-protection floor – which can help countries to mitigate the negative effects of shocks and prevent people from falling deeper into poverty – is affordable.
However, low-income and least-developed countries need assistance in raising the floor to more acceptable levels.
In order to address global poverty effectively, world leaders must pursue poverty-reduction policies that support inclusive, sustainable economic growth and development – in turn increasing the fiscal resources that are available for social spending.
Only when all citizens benefit from economic development can leaders devoted to poverty reduction claim genuine success.
Re-Thinking Homegrown Terrorism
BERLIN – Homegrown terrorism stands high on the security agenda almost everywhere in Europe.
Its links to international extremist Islamic groups require that governments review and re-think how they respond to it.
International security cooperation, border controls, and transport sector monitoring are all important, but they don’t go far enough.
Governments must also identify radical tendencies within Europe’s Muslim communities and devise strategies to counteract them.
The narrow aim should be to nip terrorism in the bud, and the broader aim should be to open a dialogue with the Muslim community.
This agenda emphasizes intercultural communication as the best way to break down prejudices and negative clichés on both sides.
From the point of view of security policy, intercultural communication is intended to immunize those sections of the Muslim community that are deemed to be potentially receptive to extremist propaganda, with the clear aim being to prevent their radicalization and recruitment to jihad .
For immunization of this sort to be successful, it is important to first clarify what can turn someone into a jihadi .
There appears to be no single answer.
But one common pattern that does emerge is that the jihadi message, because it promises meaning and identity, seems to appeal particularly to people who are unsure about who they are and where they are heading.
Last year, a study of German Muslims analyzed social integration (including obstacles), religion, attitudes to democracy, the rule of law, and politically-inspired violence.
The vast majority of respondents were from a migrant background, and a quarter were Muslims whose families had been in Germany for a generation or more.
In terms of ethnic origin and religious practice, they were a representative sample of Germany’s Muslim community.
The study provided valuable insights into homegrown terrorism, for it revealed a consistently close link between radicalization and “vicarious” experiences of marginalization and discrimination.
A critical factor was the powerful emotions generated by United States-led military interventions in Muslim countries, and by the situation of the Palestinians.
The blanket suspicion that was widely seen as falling on all Muslims after every terrorist attack was also strongly resented.
But the study revealed no automatic correlation between Islamist sympathies and condoning the use of violence.
And, while individuals who had Islamist sympathies and condoned violence were more likely than others to become involved in homegrown terrorism, a mere 1.1% of Germany’s Muslims fell into this category.
The authors also noted that the mechanisms that turn Muslims into potential terrorists are the same as those that make German teenagers susceptible to xenophobic propaganda and right-wing extremism.
The study’s conclusions applied only to Germany, but its main findings were largely corroborated by a Gallup global survey.
According to Gallup, only 7% of Muslims are politically radical and condone the use of violence, and their motives are not so much religious as inspired by the demeaning political and social treatment that Muslims are perceived to suffer.
This is in marked contrast to the conventional view in the West that the prime motive for Islamist terrorism is religious.
Many Muslims see that erroneous assumption as evidence of the West’s contempt for Islam, reinforcing their view that Muslims as a whole are victims of discrimination and creating a vicious cycle that may generate further radicalization on both sides.
Indeed, certain aspects of the host-country environment may contribute to radicalization.
While external factors also play a major role, Muslim attitudes toward their host country are strongly influenced by the perception that Muslims are subjected to humiliation and oppression.
This is liable to be reinforced by any negative experiences that Muslims may suffer themselves.
Finally, there is the quantitative question.
The number of potential terrorist recruits can only be estimated, but it corresponds more or less to the percentage of the population in any Western society that is likely to be involved in violent crime.
In no sense do these people amount to a mass movement. But, as protagonists in the complex web of interaction between the Muslim and the Western worlds, they view militant terrorism as their preferred option.
There are, of course, other forms of interaction, and this brings us back to the question of how to respond to homegrown terrorism.
A distinction is often made between “hard” tools for combating terrorism – i.e. executive, including military measures – and “soft” tools such as programs promoting the integration of Muslim immigrants, efforts to stabilize and develop problem countries, and strategies for intercultural dialogue.
The slogan “war for Muslim hearts and minds” is a grotesque yet revealing attempt to link both sets of tools.
Even when such interaction is to communicate a political message, an image of war is used.
If we adopt the parlance employed by al-Qaeda while at the same time proclaiming our intention to communicate with the whole Muslim world, we run the risk of reinforcing al-Qaeda’s message.
The “competition of ideas” appears to be confined to al-Qaeda’s jihadi agenda.
Such a response is hardly likely to alter the mindset of potential recruits.
Today’s tendency to view Muslim grievances in one-dimensional terms reduces political discourse to a very simplistic level.
Our interactions with Muslims may be unconsciously influenced by this defensive reflex, and that may encourage equally defensive attitudes on their part.
Rethinking Hunger
ROME – The world has a nutrition problem.
Though great strides have been made toward the Millennium Development Goal of halving the proportion of undernourished people in developing countries, the problem remains persistent, pervasive, and complex.
After all, the issue goes beyond merely providing more food; effective efforts to reduce undernourishment must ensure that people have access to enough of the right types of food – those that give them the nutrients they need to live healthy, productive lives.
Since 1945, food production has tripled, and food availability has risen by 40%, on average, per person.
Over the last decade alone, vegetable production in the Asia-Pacific region, where more than three-quarters of the world’s vegetables are grown, increased by one-quarter.
But, despite these gains in expanding the food supply, at least 805 million people still go hungry every day, of whom some 791 million live in developing countries.
Many more go hungry seasonally or intermittently.
And more than two billion people suffer from “hidden hunger” – one or more micronutrient deficiencies.
Hunger and undernourishment damage the health and productivity of adults, undermining their ability to learn and work.
Moreover, they impede children’s physical and cognitive development, and leave them more susceptible to illness and premature death.
Stunted growth due to malnutrition affects one in four children under the age of five.
Adequate nutrition is most vital during the first 1,000 days of life (from conception to a child’s second birthday).
But, even after that, hunger and undernourishment continue to diminish children’s chances of surviving to adulthood, much less reaching their full potential.
Ironically, in many parts of the world, pervasive hunger coexists with rising levels of obesity.
More than 1.5 billion people are overweight, with one-third of them considered obese.
These people are particularly vulnerable to non-communicable diseases like heart disease, stroke, and diabetes.
Contrary to popular belief, obesity is often related less to an overabundance of food than to inadequate access to affordable, diverse, and balanced diets.
The challenge facing the international community is thus to ensure adequate consumption of the right kinds of food.
This means developing food systems that are more responsive to people’s needs, particularly those of the socially excluded and economically marginalized.
Mothers, young children, the elderly, and the disabled are among the most vulnerable to the pitfalls of undernourishment, and should be given special attention in efforts to end food insecurity and undernourishment.
In order to ensure that today’s efforts benefit future generations, strategies to improve global food systems must emphasize environmental sustainability.
Specifically, world leaders must reassess prevailing food-production processes, which often put considerable stress on natural resources by exhausting freshwater supplies, encroaching on forests, degrading soils, depleting wild fish stocks, and reducing biodiversity.
Making matters worse, the lack of adequate infrastructure for storing and transporting food to consumers contributes to massive losses.
Of course, it is essential to strike the right balance between producing enough nutrient-dense food and preserving the environment.
Consider livestock production, which accounts for many foods – including milk, eggs, and meat – that have enriched diets in developing countries and provide livelihoods for millions.
Unsustainable production systems, combined with wasteful and excessive consumption in some regions of the world, have had serious consequences in terms of climate change, disease transmission, and nutritional balance.
But, with a strong political commitment, global food-production systems can be transformed.
An obvious step would be to ensure that all food-related programs, policies, and interventions account for nutrition and sustainability.
Likewise, food-related research and development should focus on facilitating the production of nutrient-rich foods and the diversification of farming systems.
Finding ways to use water, land, fertilizer, and labor more efficiently, and with minimal adverse impact, is essential to ecological sustainability.
Equally important will be interventions that empower local communities to improve their diets.
This requires comprehensive public-health and education campaigns, social protection to enhance resilience, and initiatives to boost employment and income generation.
Finally, producers and distributors need support and encouragement to transform their existing systems.
After all, a shift toward sustainability cannot come at the expense of farmers’ livelihoods.
Better nutrition makes economic sense.
Malnutrition in all of its forms reduces global economic welfare by about 5% annually, through foregone output and additional costs incurred.
The economic gains of reducing micronutrient deficiencies are estimated to have a cost/benefit ratio of almost 1:13.
The upcoming Second International Conference on Nutrition in Rome will provide a historic opportunity to galvanize political commitment to enhance nutrition for all through better policies and international solidarity.
Failure to make the needed investments in food access, nutrition, and sustainability is morally – and economically – unjustifiable.
Rethinking Inflation Targeting
ZURICH – Over the last two decades, inflation targeting has become the predominant monetary-policy framework.
It has been essentially (though not explicitly) adopted by major central banks, including the US Federal Reserve, the European Central Bank, and the Swiss National Bank.
But the 2008 global economic crisis, from which the world has yet to recover fully, has cast serious doubt on this approach.
The Bank for International Settlements has long argued that pure inflation targeting is not compatible with financial stability.
It does not take into account the financial cycle, and thus produces excessively expansionary and asymmetric monetary policy.
Moreover, a major argument in favor of inflation targeting – that it has contributed to a decline in inflation since the early 1990s – is questionable, at best.
Disinflation actually began in the early 1980s – well before inflation targeting was invented – thanks to the concerted efforts of then-US Federal Reserve Board Chair Paul Volcker.
And, from the 1990s on, globalization – in particular, China’s integration into the world economy – has probably been the main reason for the decline in global inflationary pressure.
A more recent indication that inflation targeting has not caused the disinflation seen since the 1990s is the unsuccessful effort by a growing number of central banks to reflate their economies.
If central banks are unable to increase inflation, it stands to reason that they may not have been instrumental in reducing it.
The fact is that the original objective of central banks was not consumer-price stability; consumer-price indices did not even exist when most of them were founded.
Central banks were established to provide war financing to governments.
Later, their mission was expanded to include the role of lender of last resort.
It was not until the excessive inflation of the 1970s that central banks discovered – or, in a sense, rediscovered – the desirability of keeping the value of money stable.
But how to measure the value of money?
One approach centers on prices, with the consumer price index appearing to be the most obvious indicator.
The problem is that the relationship between the money supply (which ultimately determines the value of money) and prices is an unstable one.
For starters, the lag time between changes in the money supply and price movements is long, variable, and unpredictable.
Given this, targeting consumer prices in the next 2-3 years will not guarantee that the value of money remains stable in the long term.
Moreover, different methods of collecting consumer prices yield different results, depending on how housing costs are treated and the hedonic adjustment applied.
In short, monetary policy has been shaped by an imprecise, small, and shrinking subset of prices that exhibits long and variable lags vis-à-vis changes in the money supply.
Unfortunately, monetary policymakers’ effort to operationalize the objective of ensuring that the value of money remains stable has taken on a life of its own.
Today’s economics textbooks assume that a primary objective of central banks is to stabilize consumer prices, rather than the value of money.
Furthermore, economists now understand inflation as a rise in consumer prices, not as a decline in the value of money resulting from an excessive increase in the money supply.
Making matters worse, central banks routinely deny responsibility for any prices other than consumer prices, ignoring that the value of money is reflected in all prices, including commodities, real estate, stocks, bonds, and, perhaps most important, exchange rates.
In short, while price stabilization through inflation targeting is a commendable objective, central banks’ narrow focus on consumer prices – within a relatively short time frame, no less – is inadequate to achieve it.
This was highlighted by the surge in many countries’ housing prices in the run-up to the 2008 financial crisis, the steep decline in asset and commodity prices immediately after Lehman Brothers collapsed, the return to asset-price inflation since then, and recent large currency fluctuations.
All are inconsistent with a stable value of money.
Central banks’ exclusive focus on consumer prices may even be counterproductive.
By undermining the efficient allocation of capital and fostering mal-investment, CPI-focused monetary policy is distorting economic structures, blocking growth-enhancing creative destruction, creating moral hazard, and sowing the seeds for future instability in the value of money.
Within a complex and constantly evolving economy, a simplistic inflation-targeting framework will not stabilize the value of money.
Only an equally complex and highly adaptable monetary-policy approach – one that emphasizes risk management and reliance on policymakers’ judgment, rather than a clear-cut formula – can do that.
Such an approach would be less predictable and eliminate forward guidance, thereby discouraging excessive risk-taking and reducing moral hazard.
History hints at what a stability-oriented framework could look like.
In the last quarter of the twentieth century, many central banks used intermediate targets, including monetary aggregates.
Such targets could potentially be applied to credit, interest rates, exchange rates, asset and commodity prices, risk premiums, and/or intermediate-goods prices.
Short-term consumer-price stability does not guarantee economic, financial, or monetary stability.
It is time for central banks to accept this fact and adopt a comprehensive, long-term monetary-policy approach – even if it means that, in the short term, consumer-price inflation deviates from what is currently understood as “price stability.”
Temporary fluctuations in a narrow and imprecisely measured CPI are a small price to pay to secure the long-term stability of money.
Rethinking Sovereignty
For 350 years, sovereignty – the notion that states are the central actors on the world stage and that governments are essentially free to do what they want within their own territory but not within the territory of other states – has provided the organizing principle of international relations.
The time has come to rethink this notion.
The world’s 190-plus states now co-exist with a larger number of powerful non-sovereign and at least partly (and often largely) independent actors, ranging from corporations to non-government organizations (NGO’s), from terrorist groups to drug cartels, from regional and global institutions to banks and private equity funds.
The sovereign state is influenced by them (for better and for worse) as much as it is able to influence them.
The near monopoly of power once enjoyed by sovereign entities is being eroded.
As a result, new mechanisms are needed for regional and global governance that include actors other than states.
This is not to argue that Microsoft, Amnesty International, or Goldman Sachs be given seats in the United Nations General Assembly, but it does mean including representatives of such organizations in regional and global deliberations when they have the capacity to affect whether and how regional and global challenges are met.
Moreover, states must be prepared to cede some sovereignty to world bodies if the international system is to function.
This is already taking place in the trade realm.
Governments agree to accept the rulings of the World Trade Organization because on balance they benefit from an international trading order even if a particular decision requires that they alter a practice that is their sovereign right to carry out.
Some governments are prepared to give up elements of sovereignty to address the threat of global climate change.
Under one such arrangement, the Kyoto Protocol, which runs through 2012, signatories agree to cap specific emissions.
What is needed now is a successor arrangement in which a larger number of governments, including the United States, China, and India, accept emissions limits or adopt common standards because they recognize that they would be worse off if no country did.
All of this suggests that sovereignty must be redefined if states are to cope with globalization.
At its core, globalization entails the increasing volume, velocity, and importance of flows – within and across borders – of people, ideas, greenhouse gases, goods, dollars, drugs, viruses, emails, weapons, and a good deal else, challenging one of sovereignty’s fundamental principles: the ability to control what crosses borders in either direction.
Sovereign states increasingly measure their vulnerability not to one another, but to forces beyond their control.
Globalization thus implies that sovereignty is not only becoming weaker in reality, but that it needs to become weaker.
States would be wise to weaken sovereignty in order to protect themselves, because they cannot insulate themselves from what goes on elsewhere.
Sovereignty is no longer a sanctuary.
This was demonstrated by the American and world reaction to terrorism.
Afghanistan’s Taliban government, which provided access and support to al-Qaeda, was removed from power.
Similarly, America’s preventive war against an Iraq that ignored the UN and was thought to possess weapons of mass destruction showed that sovereignty no longer provides absolute protection.
Imagine how the world would react if some government were known to be planning to use or transfer a nuclear device or had already done so.
Many would argue – correctly – that sovereignty provides no protection for that state.
Necessity may also lead to reducing or even eliminating sovereignty when a government, whether from a lack of capacity or conscious policy, is unable to provide for the basic needs of its citizens.
This reflects not simply scruples, but a view that state failure and genocide can lead to destabilizing refugee flows and create openings for terrorists to take root.
The NATO intervention in Kosovo was an example where a number of governments chose to violate the sovereignty of another government (Serbia) to stop ethnic cleansing and genocide.
By contrast, the mass killing in Rwanda a decade ago and now in Darfur, Sudan, demonstrate the high price of judging sovereignty to be supreme and thus doing little to prevent the slaughter of innocents.
Our notion of sovereignty must therefore be conditional, even contractual, rather than absolute.
If a state fails to live up to its side of the bargain by sponsoring terrorism, either transferring or using weapons of mass destruction, or conducting genocide, then it forfeits the normal benefits of sovereignty and opens itself up to attack, removal, or occupation.
The diplomatic challenge for this era is to gain widespread support for principles of state conduct and a procedure for determining remedies when these principles are violated.
The goal should be to redefine sovereignty for the era of globalization, to find a balance between a world of fully sovereign states and an international system of either world government or anarchy.
The basic idea of sovereignty, which still provides a useful constraint on violence between states, needs to be preserved.
But the concept needs to be adapted to a world in which the main challenges to order come from what global forces do to states and what governments do to their citizens rather than from what states do to one another.
Rethinking the Growth Imperative
CAMBRIDGE – Modern macroeconomics often seems to treat rapid and stable economic growth as the be-all and end-all of policy.
That message is echoed in political debates, central-bank boardrooms, and front-page headlines.
But does it really make sense to take growth as the main social objective in perpetuity, as economics textbooks implicitly assume?
Certainly, many critiques of standard economic statistics have argued for broader measures of national welfare, such as life expectancy at birth, literacy, etc.
Such appraisals include the United Nations Human Development Report, and, more recently, the French-sponsored Commission on the Measurement of Economic Performance and Social Progress, led by the economists Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi.
But there might be a problem even deeper than statistical narrowness: the failure of modern growth theory to emphasize adequately that people are fundamentally social creatures.
They evaluate their welfare based on what they see around them, not just on some absolute standard.
The economist Richard Easterlin famously observed that surveys of “happiness” show surprisingly little evolution in the decades after World War II, despite significant trend income growth.
Needless to say, Easterlin’s result seems less plausible for very poor countries, where rapidly rising incomes often allow societies to enjoy large life improvements, which presumably strongly correlate with any reasonable measure of overall well-being.
In advanced economies, however, benchmarking behavior is almost surely an important factor in how people assess their own well-being.
If so, generalized income growth might well raise such assessments at a much slower pace than one might expect from looking at how a rise in an individual’s income relative to others affects her welfare.
And, on a related note, benchmarking behavior may well imply a different calculus of the tradeoffs between growth and other economic challenges, such as environmental degradation, than conventional growth models suggest.
To be fair, a small but significant literature recognizes that individuals draw heavily on historical or social benchmarks in their economic choices and thinking.
Imagine that per capita national income (or some broader measure of welfare) is set to rise by 1% per year over the next couple of centuries.
This is roughly the trend per capita growth rate in the advanced world in recent years.
With annual income growth of 1%, a generation born 70 years from now will enjoy roughly double today’s average income.
Over two centuries, income will grow eight-fold.
Now suppose that we lived in a much faster-growing economy, with per capita income rising at 2% annually.
In that case, per capita income would double after only 35 years, and an eight-fold increase would take only a century.
Finally, ask yourself how much you really care if it takes 100, 200, or even 1,000 years for welfare to increase eight-fold.
Wouldn’t it make more sense to worry about the long-term sustainability and durability of global growth?
Wouldn’t it make more sense to worry whether conflict or global warming might produce a catastrophe that derails society for centuries or more?
Even if one thinks narrowly about one’s own descendants, presumably one hopes that they will be thriving in, and making a positive contribution to, their future society.
Assuming that they are significantly better off than one’s own generation, how important is their absolute level of income?
Perhaps a deeper rationale underlying the growth imperative in many countries stems from concerns about national prestige and national security.
In his influential 1989 book The Rise and Fall of the Great Powers, the historian Paul Kennedy concluded that, over the long run, a country’s wealth and productive power, relative to that of its contemporaries, is the essential determinant of its global status.
Kennedy focused particularly on military power, but, in today’s world, successful economies enjoy status along many dimensions, and policymakers everywhere are legitimately concerned about national economic ranking.
An economic race for global power is certainly an understandable rationale for focusing on long-term growth, but if such competition is really a central justification for this focus, then we need to re-examine standard macroeconomic models, which ignore this issue entirely.
Of course, in the real world, countries rightly consider long-term growth to be integral to their national security and global status.
Highly indebted countries, a group that nowadays includes most of the advanced economies, need growth to help them to dig themselves out.
But, as a long-term proposition, the case for focusing on trend growth is not as encompassing as many policymakers and economic theorists would have one believe.
In a period of great economic uncertainty, it may seem inappropriate to question the growth imperative.
But, then again, perhaps a crisis is exactly the occasion to rethink the longer-term goals of global economic policy.
Re-Thinking the Iranian Nuclear Threat
Would it be a great disaster if Iran had nuclear weapons?
As a habitual contrarian, I pose the question because almost everyone seems to believe that it would, and that it must be prevented at all costs.
But is that true?
John Bolton, the former United States Ambassador to the United Nations, said in April that “if the choice is [Iran] continuing [towards a nuclear bomb] or the use of force, I think you’re at a Hitler marching into the Rhineland point.”