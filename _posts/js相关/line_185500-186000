But if the share of the highest income groups keeps rising, the problem will remain chronic.
And, at some point, when public debt has become too large to allow continued deficit spending, or when interest rates are close to their zero lower bound, the system runs out of solutions.
This story has a counterintuitive dimension.
Is it not the case that the problem in the US has been too little savings, rather than too much?
Doesn’t the country’s persistent current-account deficit reflect excessive consumption, rather than weak effective demand?
The recent work by Rajan, Stiglitz, Kumhof and Ranciere, and others explains the apparent paradox: those at the very top financed the demand of everyone else, which enabled both high employment levels and large current-account deficits.
When the crash came in 2008, massive fiscal and monetary expansion prevented US consumption from collapsing.
But did it cure the underlying problem?
Although the dynamics leading to increased income concentration have not changed, it is no longer easy to borrow, and in that sense another boom-and-bust cycle is unlikely.
But that raises another difficulty.
When asked why they do not invest more, most firms cite insufficient demand.
But how can domestic demand be strong if income continues to flow to the top?
Consumption demand for luxury goods is unlikely to solve the problem.
Moreover, interest rates cannot become negative in nominal terms, and rising public debt may increasingly disable fiscal policy.
So, if the dynamics fueling income concentration cannot be reversed, the super-rich save a large fraction of their income, luxury goods cannot fuel sufficient demand, lower-income groups can no longer borrow, fiscal and monetary policies have reached their limits, and unemployment cannot be exported, an economy may become stuck.
The early 2012 upturn in US economic activity still owes a lot to extraordinarily expansionary monetary policy and unsustainable fiscal deficits.
If income concentration could be reduced as the budget deficit was reduced, demand could be financed by sustainable, broad-based private incomes.
Public debt could be reduced without fear of recession, because private demand would be stronger.
Investment would increase as demand prospects improved.
This line of reasoning is particularly relevant to the US, given the extent of income concentration and the fiscal challenges that lie ahead.
But the broad trend toward larger income shares at the top is global, and the difficulties that it may create for macroeconomic policy should no longer be ignored.
Regime Change in China?
CLAREMONT, CALIFORNIA – One question that should have been asked about the Chinese Communist Party’s just-completed leadership transition is whether the entire elaborately choreographed exercise was akin to rearranging the deck chairs on the Titanic.
The installation of a new leadership may matter little if the end of CCP rule is both foreseeable and highly probable.
Many observers would find this assertion shocking.
The CCP, they insist, has proved its resilience since the Tiananmen crisis in 1989 and the collapse of Soviet communism in 1991.
Why should predictions of the collapse of CCP rule be taken seriously now?
While the future of China is unpredictable, the durability of its post-totalitarian regime can be estimated with some confidence.
China may be unique in many ways, but its one-party rule is hardly exceptional.
Indeed, its political order suffers from the same self-destructive dynamics that have sent countless autocratic regimes to their graves.
Among many of the systemic flaws of autocracy, degeneration at the top, epitomized by ever-weaker leaders, is progressive and incurable.
The exclusive and closed nature of autocracy bars many talented individuals from rising to senior government positions, owing to a pattern of succession that rewards political loyalty over capabilities.
In fact, savvy autocratic rulers favor less talented successors, because they are easier to groom and control on their way to power.
Leadership degeneration accelerates as the autocratic regime ages and grows more bureaucratic.
As individuals in such regimes ascend the hierarchy, patronage and risk-aversion become the most critical factors in determining their chances for promotion.
Consequently, such regimes grow increasingly sclerotic as they select leaders with stellar resumes but mediocre records.
The most lethal strain of leadership degeneration is escalating predation among the ruling elites.&#160; The most visible symptom is corruption, but the cause is intrinsic to autocratic rule.
Typically, first-generation revolutionaries have a strong emotional and ideological attachment to certain ideals, however misguided they may be.
But the post-revolutionary elites are ideologically cynical and opportunistic.
They view their work for the regime merely as a form of investment.
And, like investors, they seek ever-higher returns.
As each preceding generation of rulers cashes in its illicit gains from holding power, the successors are motivated by both the desire to loot even more and the fear that there may not be much left by the time they get their turn at the trough.
This is the underlying dynamic driving corruption in China today.
In fact, the consequences of leadership degeneration are easy to see: faltering economic dynamism and growth, rising social tensions, and loss of government credibility.
The puzzle is why neither the compelling self-destructive logic of autocratic rule nor the mounting evidence of deteriorating regime performance in China has persuaded even some of the most knowledgeable observers that the end of CCP rule is now a distinct possibility.
An obvious explanation is the power of conventional thinking.
Long-ruling regimes – think of the Soviet Communist Party, Indonesia’s Suharto, and Egypt’s Hosni Mubarak – are typically considered invulnerable, even just before they collapse.
But those who believe that the CCP can defy both the internal degenerative dynamics of autocracy and the historical record of failed one-party regimes might benefit from reading Leon Trotsky, who knew something about revolutions.
Dictatorships are regarded as indestructible before they fall, Trotsky reminds us, but their demise is viewed as inevitable once they are toppled.
Another explanation is fear of contemplating the unknown.
CCP rule may not last, but the alternative – state failure and civil chaos – could be far worse than the status quo.
But the record of democratic transitions since 1974 suggests that regime change in China is unlikely to be calamitous.
The decisive factor will be whether it is initiated and managed by the ruling elites, as in Taiwan, Mexico, Brazil, and Spain.
Managed transitions produce more stable democracies.
Should such a process occur in China, the CCP could transform itself into a major political party competing with others for power, as formerly autocratic parties have done in Mexico and Taiwan.
Even a disorderly regime transition, however traumatic and chaotic in the short term, could yield a system that, on balance, is an improvement over a stagnant, repressive, and corrupt autocracy.
Indonesia’s new democracy may be imperfect, but has thrived despite its initial poor prospects.
Likewise, Vladimir Putin’s Russia, a deeply flawed hybrid autocracy, is nonetheless a far better place to live than the Soviet Union was.
If there is one lesson to be learned from the remarkable history of the democratic transitions of the past 38 years, it is that when elites and the public reject authoritarian rule, they do their best to make the new system work.
Should such a transition occur in China, there is no reason to think that the process and the outcome will be fundamentally different.
The Inevitable Re-Birth of European Integration
ATHENS – European integration implies successive transfers of national sovereignty to the Union.
But, while member states readily comply with decisions that abolish protective measures – say, import duties – they hesitate to formulate or advance policies that would grant the European Union discretionary powers to take initiatives.
Typical examples of this are the stalled Lisbon Strategy, the incomplete Economic and Monetary Union, and now, following the Irish public’s blocking maneuver, the uncertain fate of the new EU Constitutional Treaty (the “Lisbon Treaty”).
A similar weakness is evident in the EU’s attempt to define itself in the global system.
Energy security, climate change, the rise of China, and the revival of Russia are among the many issues that require effective responses.
Often, however, the EU cannot respond or can do so only slowly, because its structure obstructs quick decisions and fast action.
This structure was appropriate in an era when the free market was practically the only issue that the EU had to confront at the global level.
But that era is over.
Change is rendered more difficult by the insufficient democratic legitimacy of EU bodies.
The lack of a direct relationship with Europe’s people deprives these bodies of the pressure required to bring about rapid action and responsive policies.
There is no easy solution to these problems.
Democracy in the EU cannot be guaranteed by the models and rules that apply in the member states.
The scale of the problems makes more elaborate solutions necessary.
When dealing with the EU, member states usually aim for arrangements and regulations that ensure cooperation within agreed-upon frameworks.
And, as the Irish referendum showed, they do not readily accept unifying initiatives that would make the EU an autonomous center of power.
But, assuming that the obstacle thrown up by Ireland’s voters can be overcome, experience has shown that the EU’s future evolution will be marked by increasingly centralized power in Brussels and the retreat of individual states.
The lever for this process will continue to be the EU’s central bureaucracy, the mechanism that formulates the member states’ common interests and whose field of action is determined by loose, and periodic, inter-governmental collaboration.
The more responsibilities the Brussels bureaucracy acquires, the more independent it will become.
The prime concern for the EU bureaucracy is to find compromises that meet the wishes of member states, and to accommodate often divergent and contradictory national preferences.
A common will usually emerges without any emphasis on conflicting political tendencies and ambitions.
In fact, de-politicization is seen to be advisable because it allows for the easy achievement of balances.
Bolstering democracy, however, requires emphasizing the political dimension and public debate that is free and open to all.
National forums must make it their concern to discuss common European issues, thereby ensuring information for all, transparency, control, and accountability.
Instituting public debate on European policy throughout the Union will help clarify the aims of the unification project, make common interests apparent, and determine Europe’s institutional shape.
This is why the Lisbon Treaty is so important.
The provisions in the chapter on democratic principles, which concern the EU’s institutions and procedural functions, lay the groundwork for a system that is more open to public debate on EU policies.
The Lisbon Treaty also succeeds in tackling another major weakness of the EU ­– its ineffectiveness.
By means of extensive reforms to the structure, operation, and decision-making processes of EU institutions, the treaty enhances their effectiveness and enables greater dynamism.
True, many ordinary Europeans do not think that Europe should play an important part in global developments.
They believe that foreign policy should be handled by their national governments.
This view is outdated.
Even in bilateral or regional crises, viable solutions are possible only at the supranational level.
The Lisbon Treaty envisages the EU as a highly outward-looking entity on the international stage.
But there are limits to what the EU can do, limits determined by the Union’s inter-governmental nature.
Indeed, on crucial issues, it is the 27 members, not the Union, that will decide.
Internally, the treaty takes a big step toward creating a Europe of freedom, security, and justice.
It both sets the normative framework for EU policy and establishes institutions and procedures for policies on border controls, asylum, reception of migrants, and cooperation between judicial and police authorities.
Thus, it creates greater security for people in their everyday lives.
The treaty is less detailed in other areas, particularly in achieving an economic union equivalent to the currency union, because it was simply not possible to reach agreement.
Nonetheless, the Lisbon Treaty paves the way for planning and implementing policies needed to tackle crucial contemporary challenges, such as climate change, energy, research, technology, and tourism.
Whatever Irish voters intended by rejecting the treaty, socio-economic change will continue to force the EU to seek new forms of organization, combining inter-governmental and federal approaches.
For example, the need to adapt the operation of the European Central Bank to a policy of development for Europe, as laid out by the Ecofin Council, is already apparent.
The EMU is evidence of that.
More generally, however the problem of the Irish referendum result is addressed, the EU will gradually acquire its definitive shape on the basis of the ongoing problems that it must address, because as the dimensions of these problems grow, their management will require solid and durable forms of cooperation.
The Pope and the Party
ALBERTA – As the twentieth century neared, Pope Leo XIII, grieving for humanity’s choice between atheistic socialism and venal liberalism, commissioned Catholic intellectuals to devise a better solution.
Named Corporatism and set forth in the 1891 encyclical Rerum Novarum, Leo’s interwar successor, Pius XI, recounted that it “laid down for all mankind the surest rules to solve aright that difficult problem of human relations called ‘the social question.’”
Corporatism (which should be distinguished from the tripartite bargaining structures that emerged in many countries in the 1970’s under the name “neo-corporatism”)became the most influential ethically motivated intervention into economics in modern history.
As Catholic social doctrine until the late twentieth century, Corporatism still shapes constitutions, laws, and attitudes throughout the world.
It can be distilled into four tenets:
·&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Equality is a cruel illusion: people are happiest if rightly placed in a hierarchy legitimized by Catholic teachings.
·&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Competition is spiritually demeaning.
Associations – committees of Catholic business owners, labor leaders, and officials – must set quotas, prices, and wages within vertically connected swaths of the economy called corporations.
A typical Corporatist economy might contain thirty or so corporations – foods, heavy industry, textiles, chemicals, etc. – each encompassing raw materials, production, distribution, and retailing firms.
International trade and new firms are undesirable, because they undermine associations’ power.
·&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Private property is legitimatized by owners’ obedience to Church and association, but delegitimized by competition.
·&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; The principle of subsidiarity devolves authority unneeded at higher levels to the lowest feasible level throughout the hierarchy.
Mussolini established the first Corporatist economy, albeit substituting “Fascist” for “Catholic” throughout.
State holding companies controlled key listed firms directly; and associations controlled the rest, reconciling totalitarianism with nominally private ownership.
Italy, its foreign trade peremptorily suppressed, escaped the trade wars of the Great Depression.&#160; In 1931, Pius XI took credit.
“Anyone who gives even slight attention to the matter will easily see…the obvious advantages in the system….The various classes work together peacefully; socialist organizations and their activities are repressed.”
He exulted that Leo’s “Catholic principles on the social question have…passed little by little into the patrimony of all human society…not only in non-Catholic books and journals, but also in legislative halls and courts of justice.”
Indeed, Corporatism spread to country after country. In 1932, it was embraced by clerico-fascist Austria, under Engelbert Dollfuss.
Falangist Spain under Francisco Franco and Portugal under António de Oliveira Salazar followed.
Interwar Poland, Greece, Albania, Bulgaria, Estonia, Latvia, and Lithuania adopted forms of Corporatism.
So did Hitler’s Germany, though in greatly modified form.
Vichy France embraced Catholic Corporatism, as did German protectorates over Belgium and the Czech lands, as well as nominally independent Slovakia under Monsignor Jozef Tiso.
By the 1960’s, most Latin American countries were avowedly corporatist dictatorships.
Lebanon’s Falangist Party gave voice to its Maronite Catholics.
Corporatism spread beyond Christendom – to Turkey under Atatürk and, using aliases to hide its Catholic provenance, to other Arab countries.
Elite Catholic schools taught Corporatism to independence leaders in French, Spanish, and Portuguese colonies.
Indeed, the prominence of Corporatism in Catholic education is remarkable.
In their book Young Trudeau, Max Nemni, Monique Nemni, and William Johnson quote from Canadian leader Pierre Trudeau’s 1930’s notes on Corporatism from a class at the elite Jesuit academy where he studied: “The democratic principle has contributed to the undermining of civilization by impeding the development of the elite.”
Likewise, “Liberalism leads to excesses: to unemployment, anarchy.
The ideal is corporatism, which does not separate people into parties, but unites their interests.”
Today, Catholic and Islamic countries, as well as former French, Spanish, and Portuguese colonies, all of which tend to have Corporatist institutional residues, also correlate with depressed living standards.
That is not surprising: Corporatist institutions plausibly retard development.
Sanctified hierarchies stifle initiative.
Pius XI thought that “the leadership and teaching guidance of the Church...in this field also” precluded abuse of authority.
It seems to have evaded him that unchecked power might be more spiritually demeaning than competition.
Corporatist subsidiarity lets the top of the hierarchy determine its own powers, while banishing competition and lauding private property generates inequality and inefficiency simultaneously.
As these failings grew manifest, the Church back-pedaled in the 1960’s, and John Paul II finally repudiated Corporatism.
Today, few Catholics even know of the doctrine.
But interwar Corporatism is resurrected.
Forsaking socialism, China did not adopt capitalism, but kept the Communist Party atop a self-legitimized hierarchy.
True, central planners no longer set wages, prices, interest rates, and quotas; but party cadres, not market forces, control the economy’s commanding heights.
Industry ministries oversee vertical swaths of firms.
State-controlled banks allocate capital.
State-owned enterprises, or their subsidiaries, dominate key markets, as in interwar Italy.
A subsidiarity principle even grants senior cadres discretion in delegating powers to underlings.
How odd of Mao’s heirs, however accidentally, to resurrect this forsaken Catholic ideology.
China’s rapid growth has rescued multitudes from abject poverty, and quasi-Corporatist arrangements are clearly better than Mao’s Great Leap Forward and Cultural Revolution.
But that is faint praise.
Corporatism elsewhere begat vast inequalities, corruption, and dictatorships that eventually proved unsustainable.
The saga of Corporatism cautions economists against dismissing ethical concerns about markets.
But it also warns theologians that economics contains real truths, however unattractive.
Finally, it counsels Chinese technocrats against dogma-driven economic policies.
The Information State
MOSCOW – The new American administration of Barack Obama is planning to appoint a chief technology officer, following the lead of most large corporations nowadays.
Should other countries have one, too?
Rather than slavishly copy the United States, I think most countries should have a chief information officer – someone who thinks about information as an agent of change, not just as an agent of efficiency.
The free flow of information constrains official power and gives individuals the power to act for themselves.
It is the essence of a free market, and it should also be the essence of a free democracy.
People should know what their officials are doing, what their government’s policies cost, who pays for them, and who benefits.
Of course, such a government CIO couldn’t make all that happen alone.
But he or she could encourage it – everywhere information is used and everywhere it’s hidden. 
The CIO wouldn’t have policymaking authority per se , but he or she should sit high in the government – ideally with little bureaucratic encumbrance.
The CIO would not have a fiefdom and would not compete for power or turf with other senior officials, but would simply remind the president or other people of the role of information every time it was appropriate.
To the extent that more than persuasion is required, that would be up to the president [head of government].
Indeed, the CIO would illustrate the power of information’s power by using it , rather than legal power, to persuade people to do the right thing.
No governmental “information agency” is needed.
The CIO would operate more like a “virus,” spreading everywhere, rather than being an additional appendage grafted onto the government.
This virus would continuously nudge government agencies to become more effective and accessible, rather than merely more efficient.
Ideally, it would affect employees and citizens by changing their expectations.
It wasn’t laws that created online banking; it was changed expectations and consumer demand.
(Yes, there were a few laws about liability, but they played only an enabling role.)
In short, the CIO would not press the government on “tech policy,” such as the rollout of broadband, but would set strategy about how to collect, manage, and disseminate information for public use and analysis.
The CIO would consult agencies on their public relations – but public relations would be redefined, as in the non-government world, to mean listening as well as talking, and to mean admitting faults as a way to correct them.
What the CIO should not do is almost as important.
He or she should not attempt to centralize information, but rather to make it flow more smoothly.
Call it information liquidity, akin to financial liquidity.
What specific goals should the CIO pursue?
Openness and transparency do not mean simply making everything available online, but making information easy to get to, compare, and analyze.
This includes everything from actual data to how those data are represented, facilitating comparisons of data across organizations and across time.
The data also need to be easy to manipulate so that third parties - whether commercial data services or journalists – can create compelling and revealing visualizations.
A government CIO also needs to remind us that as wonderful as the Internet is, it is most useful to people who can read and write.
The digital divide isn’t much of a divide anymore; it’s the literacy divide that is holding many people back. 
The CIO should not be the minister of education, but he or she can provide advice on the use of information technology for everything from curriculum content to the institutional changes that can be wrought by better teacher-parent-student communications concerning homework assignments, grades, and even school-bus schedules and lunch menus.
Just as consumers can rate products, parents and students should be able to rate and provide feedback on the performance of individual teachers.
The CIO also needs to take a serious look at information security – in conjunction with the CIOs of other countries (since cybercrime knows no borders).
A big factor that contributes to the existence of viruses, spam, and the like is their lucrativeness.
Unlike the drug trade, legalization would not make much of the problem go away, but the issue is similar, because economics plays a much bigger role than is generally acknowledged. 
Two big changes are necessary.
First, sending an e-mail will need to cost more than it does currently.
Second, Internet service providers, which are in the best position to manage the security of individual users’ machines, need to get more involved – again with a system that understands the costs of poor security and charges those costs back to the people responsible.
(Just as we require drivers to have insurance, maybe we should require users either to buy security services from their ISP or to post some kind of bond/insurance.)
There are many such examples; all we need is better information to find them and take action, whether inside the government or in the private sector. 
A government can do three major things: make (and enforce) laws, spend (or take) money, and inspire people.
Every country needs a CIO who can inspire its people with the power of information.
The Instability of Inequality
NEW YORK – This year has witnessed a global wave of social and political turmoil and instability, with masses of people pouring into the real and virtual streets: the Arab Spring; riots in London; Israel’s middle-class protests against high housing prices and an inflationary squeeze on living standards; protesting Chilean students; the destruction in Germany of the expensive cars of “fat cats”; India’s movement against corruption; mounting unhappiness with corruption and inequality in China; and now the “Occupy Wall Street” movement in New York and across the United States.
In Anglo-Saxon countries, the response was to democratize credit – via financial liberalization – thereby fueling a rise in private debt as households borrowed to make up the difference.
In Europe, the gap was filled by public services – free education, health care, etc. – that were not fully financed by taxes, fueling public deficits and debt.
In both cases, debt levels eventually became unsustainable.
Firms in advanced economies are now cutting jobs, owing to inadequate final demand, which has led to excess capacity, and to uncertainty about future demand.
But cutting jobs weakens final demand further, because it reduces labor income and increases inequality.
Because a firm’s labor costs are someone else’s labor income and demand, what is individually rational for one firm is destructive in the aggregate.
The result is that free markets don’t generate enough final demand.
In the US, for example, slashing labor costs has sharply reduced the share of labor income in GDP.
With credit exhausted, the effects on aggregate demand of decades of redistribution of income and wealth – from labor to capital, from wages to profits, from poor to rich, and from households to corporate firms – have become severe, owing to the lower marginal propensity of firms/capital owners/rich households to spend.
The problem is not new.
Karl Marx oversold socialism, but he was right in claiming that globalization, unfettered financial capitalism, and redistribution of income and wealth from labor to capital could lead capitalism to self-destruct.
As he argued, unregulated capitalism can lead to regular bouts of over-capacity, under-consumption, and the recurrence of destructive financial crises, fueled by credit bubbles and asset-price booms and busts.
Even before the Great Depression, Europe’s enlightened “bourgeois” classes recognized that, to avoid revolution, workers’ rights needed to be protected, wage and labor conditions improved, and a welfare state created to redistribute wealth and finance public goods – education, health care, and a social safety net.
The push towards a modern welfare state accelerated after the Great Depression, when the state took on the responsibility for macroeconomic stabilization – a role that required the maintenance of a large middle class by widening the provision of public goods through progressive taxation of incomes and wealth and fostering economic opportunity for all.
Thus, the rise of the social-welfare state was a response (often of market-oriented liberal democracies) to the threat of popular revolutions, socialism, and communism as the frequency and severity of economic and financial crises increased.
Three decades of relative social and economic stability then ensued, from the late 1940’s until the mid-1970’s, a period when inequality fell sharply and median incomes grew rapidly.
Some of the lessons about the need for prudential regulation of the financial system were lost in the Reagan-Thatcher era, when the appetite for massive deregulation was created in part by the flaws in Europe’s social-welfare model.
Those flaws were reflected in yawning fiscal deficits, regulatory overkill, and a lack of economic dynamism that led to sclerotic growth then and the eurozone’s sovereign-debt crisis now.
But the laissez-faire Anglo-Saxon model has also now failed miserably.
To stabilize market-oriented economies requires a return to the right balance between markets and provision of public goods.
That means moving away from both the Anglo-Saxon model of unregulated markets and the continental European model of deficit-driven welfare states.
Even an alternative “Asian” growth model – if there really is one – has not prevented a rise in inequality in China, India, and elsewhere.
Any economic model that does not properly address inequality will eventually face a crisis of legitimacy.
Unless the relative economic roles of the market and the state are rebalanced, the protests of 2011 will become more severe, with social and political instability eventually harming long-term economic growth and welfare.
The International Atomic Energy Agency at 50
This summer’s 50th anniversary of the International Atomic Energy Agency’s (IAEA) founding offers an opportunity for stocktaking about the world’s most important nuclear watchdog.
It comes at a time when the Agency has assumed increasing responsibility for nuclear security.
The recent dispatch of inspectors to verify the shutdown of North Korea’s weapons reactor and the continuing efforts to ferret out Iran’s nuclear intentions are only the most visible signs of its monitoring function.
But, while there is much to celebrate, questions remain about whether the IAEA can increase its capacity both to combat proliferation and promote nuclear power plant safety.
History suggests that without greater authority, the Agency will be incapable of dramatically reducing global nuclear risks.
The IAEA traces its lineage to the early dark days of the Cold War.
In his December 1953 UN General Assembly “Atoms for Peace” address, US President Dwight Eisenhower sought to relax atomic weapons competition with the Soviet Union by calling for the creation of an international nuclear fuel bank stocked with superpower fissile material. Management would come from a new global nuclear organization.
Although the fuel bank never emerged, the seed for the organization took root, giving rise to the IAEA in 1957.
In time, the IAEA became a nuclear brain bank, assisting developing countries with their peaceful nuclear needs.
It educated nuclear operators, generated safety and protection standards, promoted the benefits of the atom for industry, agriculture, and medicine, and much more.
In 1970, boosted by the new Nuclear Nonproliferation Treaty (NPT), the IAEA acquired expanded authority to safeguard nuclear elements against diversion.
Today, this responsibility extends to more than 180 countries.
In practice, however, the IAEA has an uneven record in tracking down nuclear cheats.
In 1981, following Israel’s bombing of Iraq’s Osirak reactor, IAEA inspector Roger Richter told the US Congress how Saddam had blocked IAEA access to nuclear sites.
After the 1991 Persian Gulf War, Iraq once again embarrassed the IAEA, when inspectors uncovered a major covert nuclear weapons program.
In 2003, Libya’s renunciation of its secret weapons revealed how another country had hoodwinked the Agency.
And, only recently, the IAEA discovered that Egypt conducted non-safeguarded nuclear experiments decades ago.
Despite these errors, in the past few years, the IAEA has demonstrated its worth as an effective nuclear sleuth.
In 2002, its inspectors revealed North Korea’s cheating, prompting UN sanctions.
(Unfortunately, this did not halt the country’s subsequent withdrawal from the NPT.)
The IAEA’s tenacity also has smoked out much of Iran’s nuclear program, albeit only after an Iranian dissident group disclosed the existence of some of the regime’s secret facilities.
In early 2003, its onsite presence in Iraq established that Saddam did not have a nuclear weapons program, contrary to the Bush administration’s representations.
And in 2004, the IAEA blew the whistle on South Korea’s non-safeguarded plutonium and enrichment experiments.
Beyond truth seeking, the IAEA has taken a leadership role in promoting new initiatives to reduce weapons proliferation.
It has pressed for multinational nuclear fuel facilities that would eliminate the need for national installations that draw proliferation concerns.
Director General Mohamed El Baradei has called upon the US to negotiate directly with Iran over its atomic program.
He also has attempted to generate momentum for a Middle East nuclear weapons-free zone.
Looking forward, the IAEA confronts both internal and external challenges.
Internally, mandatory retirements will soon cost the Agency 50% of its senior management.
Nor has the Agency’s budget kept pace with its increasing workload.
More significant is the IAEA’s failure to overcome resistance to conclude comprehensive safeguard agreements.
Some countries oppose the application of NPT safeguards to small quantities of nuclear materials.
The unwillingness of more than half the NPT membership to apply the Additional Protocol, which gives the IAEA new flexibility to uncover clandestine nuclear activity, undermines global security.
The projected rapid growth of nuclear energy will generate yet more challenges.
To date, reactor construction has been concentrated in the industrial world.
In coming years, developing countries with little nuclear experience – Indonesia, Vietnam, Thailand, Jordan, Turkey, Egypt, and others – will attempt to leap onto the atomic bandwagon.
Without authority to enforce safe practices, the IAEA cannot better assure nuclear safety in these or other countries.
And without better intelligence and backing from the UN Security Council to impose significant, timely, and certain penalties on nuclear violators, the IAEA will have difficulty in fulfilling its security mandate.
So, although the IAEA will continue to play a vital role, it will be more of a band-aid than a cure for the atomic risks that we will all have to assume.
The World in 2030
CAMBRIDGE – What will the world look like two decades from now?
Obviously, nobody knows, but some things are more likely than others.
Companies and governments have to make informed guesses, because some of their investments today will last longer than 20 years.
The US will remain “first among equals” in hard and soft power, but “the ‘unipolar moment’ is over.”
It is never safe, however, to project the future just by extrapolating current trends.
Surprise is inevitable, so the NIC also identifies what it calls “game-changers,” or outcomes that could drive the major trends off course in surprising ways.
First among such sources of uncertainty is the global economy: will volatility and imbalances lead to collapse, or will greater multipolarity underpin greater resilience?
Similarly, will governments and institutions be able to adapt fast enough to harness change, or will they be overwhelmed by it?
Moreover, while interstate conflict has been declining, intrastate conflict driven by youthful populations, identity politics, and scarce resources will continue to plague some regions like the Middle East, South Asia, and Africa.
And that leads to yet another potentially game-changing issue: whether regional instability remains contained or fuels global insecurity.
Then there is a set of questions concerning the impact of new technologies.
Will they exacerbate conflict, or will they be developed and widely accessible in time to solve the problems caused by a growing population, rapid urbanization, and climate change?
The final game-changing issue is America’s future role.
In the NIC’s view, the multi-faceted nature of US power suggests that even as China overtakes America economically – perhaps as early as the 2020’s – the US will most likely maintain global leadership alongside other great powers in 2030.
“The potential for an overstretched US facing increased demands,” the NIC argues, “is greater than the risk of the US being replaced as the world’s preeminent political leader.”
Is this good or bad for the world?
In the NIC’s view, “a collapse or sudden retreat of US power would most likely result in an extended period of global anarchy,” with “no stable international system and no leading power to replace the US.”
The NIC discussed earlier drafts of its report with intellectuals and officials in 20 countries, and reports that none of the world’s emerging powers has a revisionist view of international order along the lines of Nazi Germany, Imperial Japan, or the Soviet Union.
But these countries’ relations with the US are ambiguous.
They benefit from the US-led world order, but are often irritated by American slights and unilateralism.
One attraction of a multipolar world is less US dominance; but the only thing worse than a US-supported international order would be no order at all.
The question of America’s role in helping to produce a more benign world in 2030 has important implications for President Barack Obama as he approaches his second term.
The world faces a new set of transnational challenges, including climate change, transnational terrorism, cyber insecurity, and pandemics.
All of these issues require cooperation to resolve.
Obama’s 2010 National Security Strategy argues that the US must think of power as positive-sum, not just zero-sum.
In other words, there may be times when a more powerful China is good for the US (and for the world).
For example, the US should be eager to see China increase its ability to control its world-leading greenhouse-gas emissions.
US Secretary of State Hillary Clinton has referred to the Obama administration’s foreign policy as being based on “smart power,” which combines hard and soft power resources, and she argues that we should not talk about “multipolarity,” but about “multi-partnerships.”
Likewise, the NIC report suggests that Americans must learn better how to exercise power with as well as over other states.
To be sure, on issues arising from interstate military relations, understanding how to form alliances and balance power will remain crucial.
But the best military arrangements will do little to solve many of the world’s new transnational problems, which jeopardize the security of millions of people at least as much as traditional military threats do.
Leadership on such issues will require cooperation, institutions, and the creation of public goods from which all can benefit and none can be excluded.
The NIC report rightly concludes that there is no predetermined answer to what the world will look like in 2030.
NEW YORK – Viruses, phishing, spyware, spam, denial-of-service attacks, botnets… You have probably heard these words, and perhaps even suffered from what they signify, with or without knowing it.
I’d like to lay out a simple path to addressing (not resolving) these security problems, one that does not require agreement among all governments (or people) on what really constitutes a crime, much less a global police force or unenforceable global treaties.
If we can increase security in general, then governments can focus on the real criminals. 
A better approach is to view computer security as an issue of public health and economics, in which people can protect themselves but must pay for the costs they impose on others.
We need to enable people to defend themselves against others; prevent innocent, well-meaning people from becoming infected and harming others; and reduce the incentives and ability of the ill-intentioned to do harm.
That sounds like a lot of different challenges.
But there are effective approaches to each of them that do not require tracking everyone online, or requiring IDs for every interaction.
Tracking user IDs will not enable us to catch or stop bad guys, and it will render the Internet impossible to use.
We can’t save cyberspace by destroying its openness.
To implement effective security, the entities best equipped to do so, the Internet service providers, must take the lead.
They are technically savvy organizations with the ability (more or less) to protect users and detect abusers; they have a direct (though impersonal) relationship with their users; and they compete for users’ business, so that, unlike governments, they will suffer if they perform badly. 
The ISPs (rather than governments) should provide basic security – anti-virus, anti-phishing, anti-spam, and the like – as a regular feature of consumer Internet services.
This is not hard to do.
A number of anti-virus companies compete to offer consumer security services; each ISP could select one, or offer its customers a choice of three, for example.  The trick is to get consumers to use these tools – which will require an awareness campaign along the lines of public health messages.  The result should be something closer to widespread hand-washing than to a system of acute-care hospitals.  
As for spam, ISPs (including mail service providers) could limit their users to, say, 100 e-mails a day; for more, you have to pay or at least post a security bond, or pass some good behavior test.
At the same time, all ISPs should implement an e-mail ID system (there are two good standards, called Domain Keys and SPF). This is not to track everyone’s mail, but to prevent bad guys from spoofing good guys.
ISPs would throttle traffic from ISPs that did not join the security collective, and pretty soon their customers would complain, forcing them either to join or find themselves relegated to the underworld, from which it would be hard to launch attacks because no one would accept their traffic.
The point is to create economic incentives to reduce cybercrime. Real criminals won’t be deterred, but such a system would prevent the rest of us from being pulled along or becoming victims.
With fewer victims, crime will pay less.  
There are several reasons why this has not yet happened.
The first is inertia, combined with (or disguised as) idealism – the mistaken idea that the Internet should be free not just for speech, but also from payment.  Yet it costs something to maintain an infrastructure that keeps people safe.
Indeed, cost – both to users and to ISPs – is the second obstacle.
The challenge is to acknowledge the costs (as we are now doing with pollution) and assign them to people who can – and can be compelled to – pay for them.
After all, we accept the costs of police forces and health care, including not just hospitals, but also clean water, safe food, etc.
So how do we make this happen?
ISPs need to pass these costs on to their customers.
These changes would not create some digital nervous system with a centralized brain that could solve all problems.
Instead, they would result in something like an immune system of competing ISPs and evolving security services, local and omnipresent.  That would vastly improve the overall computer-security situation: Ordinary people would feel secure and law enforcement and security specialists could focus on the biggest threats.
The Intervention Dilemma
CAMBRIDGE – When should states intervene militarily to stop atrocities in other countries?
The question is an old and well-traveled one.
Indeed, it is now visiting Syria.
In 1904, US President Theodore Roosevelt argued that, “there are occasional crimes committed on so vast a scale and of such peculiar horror” that we should intervene by force of arms.
A century earlier, in 1821, as Europeans and Americans debated whether to intervene in Greece’s struggle for independence, President John Quincy Adams warned his fellow Americans about “going abroad in search of monsters to destroy.”
More recently, after a genocide that cost nearly 800,000 lives in Rwanda in 1994, and the slaughter of Bosnian men and boys at Srebrenica in 1995, many people vowed that such atrocities should never again be allowed to occur.
When Slobodan Milošević engaged in large-scale ethnic cleansing in Kosovo in 1999, the United Nations Security Council adopted a resolution recognizing the humanitarian catastrophe, but could not agree on a second resolution to intervene, given the threat of a Russian veto.
Instead, NATO countries bombed Serbia in an effort that many observers regarded as legitimate but not legal.
In the aftermath, then-UN Secretary-General Kofi Annan created an international commission to recommend ways that humanitarian intervention could be reconciled with Article 2.7 of the UN Charter, which upholds member states’ domestic jurisdiction.
The commission concluded that states have a responsibility to protect their citizens, and should be helped to do so by peaceful means, but that if a state disregarded that responsibility by attacking its own citizens, the international community could consider armed intervention.
The idea of a “responsibility to protect” (R2P) was adopted unanimously at the UN’s World Summit in 2005, but subsequent events showed that not all member states interpreted the resolution the same way.
Russia has consistently argued that only Security Council resolutions, not General Assembly resolutions, are binding international law.
Meanwhile, Russia has vetoed a Security Council resolution on Syria, and, somewhat ironically, Annan has been called back and enlisted in a so-far futile effort to stop the carnage there.
Until last year, many observers regarded R2P as at best a pious hope or a noble failure.
But in 2011, as Colonel Muammar el-Qaddafi prepared to exterminate his opponents in Benghazi, the Security Council invoked R2P as the basis for a resolution authorizing NATO to use armed force in Libya.
In the United States, President Barack Obama was careful to wait for resolutions by the Arab League and the Security Council, thereby avoiding the costs to American soft power that George W. Bush’s administration suffered when it intervened in Iraq in 2003.
But Russia, China, and other countries felt that NATO exploited the resolution to engineer regime change, rather merely protecting citizens in Libya.
In fact, R2P is more about struggles over political legitimacy and soft power than it is about hard international law.
Some Western lawyers argue that it entails the responsibility to combat genocide, crimes against humanity, and war crimes under the various conventions of international humanitarian law.
But Russia, China, and others are reluctant to provide a legal or political basis for actions such as what occurred in Libya.
There are other reasons why R2P has not been a success in the Syrian case.
Drawn from traditional “just war” theory, R2P rests not only on right intentions, but also on the existence of a reasonable prospect of success.
Many observers highlight the important physical and military differences between Libya and Syria that would make Syrian no-fly zones or no-drive zones problematic.
Some Syrians who oppose President Bashar al-Assad’s regime, pointing to Baghdad in 2005, argue that the one thing worse than a cruel dictator is a sectarian civil war.
Such factors are symptomatic of larger problems with humanitarian interventions.
For starters, motives are often mixed (Roosevelt, after all, was referring to Cuba).
Moreover, we live in a world of diverse cultures, and we know very little about social engineering and how to build nations.
When we cannot be sure how to improve the world, prudence becomes an important virtue, and hubristic visions can pose a grave danger.
Foreign policy, like medicine, must be guided by the principle, “First, do no harm.”
Prudence does not mean that nothing can be done in Syria.
Other governments can continue to try to convince Russia that its interests are better served by getting rid of the current regime than by permitting the continued radicalization of his opponents.
Tougher sanctions can continue to delegitimize the regime, and Turkey might be persuaded to take stronger steps against its neighbor.
Moreover, prudence does not mean that humanitarian interventions will always fail.
In some cases, even if motives are mixed, the prospects of success are reasonable, and the misery of a population can be relieved at modest expense.
Military interventions in Sierra Leone, Liberia, East Timor, and Bosnia did not solve all problems, but they did improve the lives of the people there.
Other interventions – for example, in Somalia – did not.
Recent large-scale interventions in Iraq and Afghanistan, though not primarily humanitarian, have eroded public support for military action.
But we should recall Mark Twain’s story about his cat. After sitting on a hot stove, it would never sit on a hot stove again, but neither would it sit on a cold one.
Interventions will continue to occur, though they are now more likely to be shorter, involve smaller-scale forces, and rely on technologies that permit action at greater distance.
In an age of cyber warfare and drones, the end of R2P or humanitarian intervention is hardly foretold.
The Intervention Syndrome
Kosovo is often held up as a test case for the concept of “humanitarian” intervention.
But as Iraq spirals into chaos, diplomats and leaders everywhere are again asking themselves if it is ever appropriate for alliances of nations or the international community as a whole to intervene when a sovereign country appears unable or unwilling to defend its citizens from genocide, war crimes, or ethnic cleansing.
At the center of this debate is the so-called doctrine of the “responsibility to protect.”
As the United Nations-appointed Ombudsperson in Kosovo for the past five years, I have had the unique opportunity to observe the aftereffects of that doctrine following NATO’s intervention in the former Yugoslavia in 1999.
Kosovo has subsequently become an international experiment in society building, led by the UN Interim Administration Mission in Kosovo (UNMIK).
Experiment is the right word here.
Indeed, Kosovo has become a Petri dish for international intervention.
Having lived and worked long enough in Kosovo to see the outcome so far, I contend that such experiments require further research.
Clearly, the need for international intervention in crises is often time-specific and a fairly swift response is frequently required.
However, apart from military factors, where such intervention is being considered, it is of vital importance to focus international policy discussion on the rapid deployment of a linked civilian and security presence.
This is especially true where human suffering is caused by communal conflict, as was the case in Kosovo.
An immediate deployment of an adequate civilian and security presence during the months immediately after the end of the 1999 NATO bombing campaign might well have provided suitable protective mechanisms against the backlash that allowed victims to become victimizers.
NATO peacekeeping troops were not directed to stop the abductions, disappearances, retaliation killings, and massive property destruction by groups of ethnic Albanians, which led to a vast reverse ethnic cleansing of the non-Albanian (mainly Serb) population.
As a result of this neglect, a noxious social and political residue pervades today’s Kosovo.
Instead of cooling communal conflict, interethnic hatred remains as heated as ever.
In addition to the lack of an adequate civilian and security presence to reassure every community of its safety, the overall lack of legal mechanisms to deliver swift justice for crimes committed during and after the intervention created additional tension.
So pervasive is this tension, in fact, that any chance of even beginning the much-needed reconciliation process must now be pushed far into the future.
Similarly, in Kosovo the international community has devoted little time to helping former combatants contemplate their collective responsibility for atrocities, no matter how direct or indirect their personal involvement.
Without such an effort, attempting to improve the situation is like building a house on a sand dune.
This lack of foresight about and planning for the aftermath of armed intervention is not only dangerous to the people who have ostensibly been “saved”; it is counterproductive.
Unless a humanitarian intervention is structured in such a way that it guarantees basic security, the underlying antagonisms that inspired the intervention in the first place will merely be reinforced, not diminished.
So, six years after NATO’s intervention, Kosovo seems as far from stability and social peace as ever.
Despite the frequent assurances of UN authorities that Kosovo is on a path toward reconciliation and true home rule, NATO officials indicate that there are plans to maintain a long-term military presence in the province in order to “guarantee that the political process will be concluded successfully.”
This brings me to another key point: a workable exit strategy is just as important to the success of any future humanitarian intervention as the entry strategy.
If an international intervention is to have any credible chance of success, clear criteria for what constitutes “success” are needed from the start.
Only such clarity can allow for a proper end to international actors’ engagement.
In Kosovo, such clarity is and has been absent; as a result, NATO and the UN have no clear idea about when and how both should leave.
Someone once rightly said that it is easy to bomb, but much harder to build; it is relatively easy to defeat a regime militarily, but it is far more difficult to create a solid, sustainable, civil society in its place.
The UN General Assembly should keep this in mind as it starts to codify the doctrine of the “responsibility to protect.”
First, Europe is as much a frontline region in the war on terror as it was during the Cold War.  As last year’s aborted attack on ten airliners heading to the US from London revealed, the likelihood of a terrorist attack on American citizens emanating from a European country remains high.  America may not need the French military, but it certainly needs the French intelligence services. 
Second, Europeans’ linguistic skills and cultural knowledge alone ensure that they can make indispensable contributions to US security.
The spread of English as the world’s language has had a paradoxical effect on American national security, making the United States transparent to people around the world, while making the rest of the world increasingly opaque to Americans.
Fourth, NATO can not only bring important military capabilities to the table, reducing the drain on American forces in a turbulent world; it also offers a much more plausible vehicle for serious foreign-policy multilateralism than either the EU or the UN.  
Fifth, and perhaps most important, elementary human psychology teaches that individuals who shun contact with others have a weak grasp of reality.
Individuals who are never criticized by companions whom they trust, and with whom they share a basic value orientation, have a hard time remaining mentally balanced.
The same is true of nations.
What makes allies indispensable to an effective national-security policy is the ability of like-minded nations to provide the reality checks without which a fallible superpower is, as we have regretfully seen, unable to keep its balance on swiftly evolving and treacherous international terrain. 
The Iranian Nuclear Threat Goes Global
TEL AVIV – The current drive to prevent Iran from developing a nuclear arsenal reflects two important, and interrelated, changes. From Israel’s perspective, these changes are to be welcomed, though its government must remain cautious about the country’s own role.
The first change is the escalation of efforts by the United States and its Western allies to abort the Iranian regime’s nuclear quest.
This was instigated in part by the International Atomic Energy Agency’s finding in November 2011 that Iran is indeed developing a nuclear weapon, and that it is getting perilously close to crossing the “red line” – the point beyond which its progress could no longer be stopped. Moreover, the US and its allies understand that failure to take serious action might prompt Israel to launch its own unilateral military offensive.
The second change is the perception that Iran’s nuclear capacity would threaten not only Israel.
In a speech to the Union for Reform Judaism in December, US President Barack Obama stated that “another threat to the security of Israel, the US, and the world is Iran’s nuclear program.”
But, by this February, Obama was saying of Iran that “my number-one priority continues to be the security of the US, but also the security of Israel, and we continue to work in lockstep as we proceed to try to solve this…”
That choice of words was no accident; rather, it was a sign that the US is changing tack when it comes to Iran.
For more than a decade, the question “Whose issue is it?” has been part of the policy debate about Iran’s nuclear ambitions. Israel’s former prime minister, Ariel Sharon, used to caution his colleagues against “rushing to the head of the line” on Iran.
He argued that if Israel were to take the lead in sounding the alarm on Iran’s nuclear ambitions, the issue would be perceived as yet another “Israeli problem.”
Indeed, Israel’s critics were already arguing that this was another case of the tail wagging the dog – that Israel and its American lobby were trying to push the US into serving Israel’s interests rather than its own.
The most egregious examples of this view were statements made by the political scientists John Mearsheimer and Stephen Walt.
In a paper published prior to the release of their much-debated book The Israel Lobby, they argued:
“… Iran’s nuclear ambitions do not pose an existential threat to the US.
If Washington could live with a nuclear Soviet Union, a nuclear China, and even a nuclear North Korea, then it can live with a nuclear Iran.
And that is why the [Israel] Lobby must keep constant pressure on US politicians to confront Tehran.”
Israel’s current prime minister, Benjamin Netanyahu, has been less worried than Sharon was about Israel’s perceived role.
He is too busy being directly engaged in the attempt to eliminate the deadly threat that a nuclear-armed Iran would pose to the Jewish state.
Prior to the 2009 election that brought him to power, Netanyahu campaigned on the Iranian danger, and his government made the issue its cardinal concern.
Together with his defense minister, Ehud Barak, Netanyahu succeeded in persuading Obama and the rest of the world that Israel was preparing a military attack as a last resort, should the US and its allies fail to stop the Iranian program in time.
That policy has been effective, but it has also drawn attention to Israel’s influence on the Iran question.
Curiously, this has not been held against Israel, at least not so far, partly because Obama and other leaders now regard Iran as a more serious threat, and therefore feel the need to take appropriate action.
The international community must underscore that its members are acting in the service of their national interests, and not simply for Israel’s sake.
But their willingness to engage could wane, particularly if sanctions exact a high financial price or military action causes a large number of casualties.
Israel would therefore be wise to remember Sharon’s cautionary words, and reinforce its pressure on the US administration with a broader diplomatic campaign.
Like it or not, Israel must urge the world to remember that Iran is everyone’s problem.
The Iraqi Public Speaks
Most of what we think about Iraq is shaped by the daily violence that plagues the country.
Intelligence and military analysts debate how much of the violence is due to the presence of foreigners, though it is widely conceded that most of the attacks can be attributed to what American officials call “former regime elements,” with the Iraqi Sunni community the main pillar of the resistance.
Having dominated Iraq under Saddam Hussein, and despite numbering less than a quarter of the overall population, Sunnis, it is said, are fighting to prevent their communal interests from being overwhelmed by the majority Shi’ites and the Kurds, a distinct ethnic group concentrated in the north.
Late last year, I was an organizer of a major national survey of Iraqi public opinion that demonstrated the complexity of the country’s communal relations.
To be sure, Iraqis of different ethnic or religious backgrounds are divided over many issues, but they also embrace a common national identity, as well as a desire for democracy.
To begin, we asked Iraqis to reflect on the fall of Saddam: Was Iraq better off without him?
Among Sunnis, only 23% thought so.
Among Shi’ites, however, 87% saw a better Iraq without Saddam.
Kurds exceeded this number, with 95% claiming an improvement.
At the same time, overwhelming majorities of Kurds, Sunnis, and Shi’ites – more than eight out of ten – preferred to be seen as Iraqis first, believing that “Iraq will be a better society if people treat one another as Iraqis.”
Strong majorities also endorsed the idea of a democratic system for Iraq.
Important divergences re-emerge on social questions.
Kurds have a much more egalitarian view of gender relations than either Sunnis or Shi’ites.
Asked if university education was more important for boys than girls, 78% of Kurds disagreed.
Among Shi’ites, the number disagreeing was 50%.
Among Sunnis, the number fell below a majority: only 44% believed in the equal importance of higher education for girls and boys.
Similarly, 78% of Kurds rejected polygamy, compared to just 49% of both Shi’ites and Sunnis.
While these findings demonstrate the shadings of opinion across Iraq’s dominant communities, they do not explain attitudes that may be behind the continuing violence that disfigures Iraqi life.
The most radical differences in opinion can be found in communal perceptions of control of the future – the possibility of building a better life in post-Saddam Iraq – and security.
We asked respondents to indicate how much control they had over their lives and how optimistic they were about the future, using a scale on which ten indicated a highly optimistic sense of control and one a deep level of powerlessness and pessimism.
Kurds had the highest perception of control and optimism, with 19% indicating the highest level of control over their lives and 17% the greatest degree of hope in the future.
The comparable figures for Shi’ites were 10 and 14%, respectively, but were just 4% and 5%, respectively, for Sunnis.
The results for extreme pessimism were skewed in the opposite direction: 14% of Sunnis thought things were as bad as could be, while only 2% of Kurds and 3% of Shi’ites shared this opinion.
The effects of localized violence were also made clear in the survey: 17% of Kurds, 41% of Shi’ites, and 77% of Sunnis felt that life in Iraq is unpredictable and dangerous, a clear demonstration of the effects of the ongoing resistance that is centered in the Sunni Triangle.
This disparity in attitudes toward the future could determine what eventually happens in Iraq.
Widespread political violence in both Iran and Latin America in the 1960’s and 1970’s demonstrated a connection between popular feelings of powerlessness and the growth of urban guerilla movements.
Leaders of these groups often defended terrorism by insisting that violence was the only means of bringing hope to demoralized people.
This argument, long discredited, resonates in the actions of the Iraqi insurgents and their fanatical allies.
This is not to say that an insecure and demoralized community supports violence.
By its immobilization, however, such a community may simply be too passive to oppose the violent men acting in its name.
If Iraqi Shi’ite leader Muqtada al-Sadr and his followers chose to stop their own violence, it was not simply because of the superior firepower of the coalition forces. It was because Shi’ite religious leadership felt empowered, optimistic, and secure enough to press the Sadrists to end their revolt.
If the Sunni leadership were to feel that it has a stake in the outcome of political events now unfolding, it would also be able to stop the insurgency.
That’s why the political talks – open and clandestine – now reported to be underway are vital.
They offer the possibility for the Sunni community to participate in the new system.
The American-led coalition can help by taking more steps that would reinforce a message of hope and optimism for the exhausted and demoralized Sunnis of Iraq.
Military action by itself may simply make matters worse.
The Iraq War Ten Years Later
CAMBRIDGE – This month marks the tenth anniversary of the controversial American-led invasion of Iraq.
What has that decision wrought over the last decade?
More important, was the decision to invade rightly made?
On the positive side, analysts point to the overthrow of Saddam Hussein, the creation of an elected government, and an economy growing at nearly 9% per year, with oil exports surpassing their pre-war level.
Some, such as Nadim Shehadi of Chatham House, go further, arguing that, while “the US certainly bit off more than it could chew in Iraq,” America’s intervention “may have shaken the region out of [a] stagnation that has dominated the lives of at least two generations.”
Skeptics reply that it would be wrong to link the Iraq War to the “Arab Spring,” because events in Tunisia and Egypt in 2011 had their own origins, while President George W. Bush’s actions and rhetoric discredited, rather than advanced, the cause of democracy in the region.
Removing Saddam was important, but Iraq is now a violent place governed by a sectarian group, with one corruption index ranking it 169th out of 174 countries.
Whatever the benefits of the war, skeptics argue, they are too meager to justify the costs: more than 150,000 Iraqis and 4,488 American service members killed, and an estimated cost of nearly $1 trillion (not including long-term health and disability costs for some 32,000 wounded US soldiers.)
Perhaps this balance sheet will look different a decade from now, but at this point most Americans have concluded that the skeptics are right, and that thinking has influenced current US foreign policy.
In the next decade, it is very unlikely that the US will try another prolonged occupation and transformation of another country.
As former Secretary of Defense Robert Gates put it shortly before stepping down, any adviser recommending such action “should have his head examined.”
Some call this isolationism, but it might better be called prudence or pragmatism.
After all, President Dwight D. Eisenhower refused in 1954 to send US troops to save the French at Dien Bien Phu because he feared that they would be “swallowed up by the divisions” in Vietnam.
And Ike was hardly an isolationist.
While a decade may be too soon to render a definitive verdict on the long-term consequences of the Iraq War, it is not too soon to judge the process by which the Bush administration made its decisions.
Bush and his officials used three main arguments to justify invading Iraq.
The first tied Saddam to Al Qaeda.
Public-opinion polls show that many Americans accepted the administration’s word on the connection, but the evidence has not sustained it.
Indeed, the evidence that was presented publicly was thin and exaggerated.
The second argument was that replacing Saddam with a democratic regime was a way to transform Middle East politics.
A number of neoconservative members of the administration had urged regime change in Iraq well before taking office, but were unable to turn it into policy during the first eight months of the administration.
After September 11, 2001, they quickly moved their policy through the window of opportunity that the terrorist attacks had opened.
Bush spoke often of regime change and a “freedom agenda,” with supporters citing the role of American military occupation in the democratization of Germany and Japan after World War II.
But the Bush administration was careless in its use of historical analogies and reckless in its inadequate preparation for an effective occupation.
The third argument focused on preventing Saddam from possessing weapons of mass destruction.
Most countries agreed that Saddam had defied United Nations Security Council resolutions for a dozen years.
Moreover, Resolution 1441 unanimously put the burden of proof on Saddam.
While Bush was later faulted when inspectors failed to find WMDs, the view that Saddam possessed them was widely shared by other countries.
Prudence might have bought more time for the inspectors, but Bush was not alone in this mistake.