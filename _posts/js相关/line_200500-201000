But, critically, it does mean that even if you believe that the BOJ’s actions are necessary for Japan to emerge from its economic malaise, they certainly are not sufficient.
Japan’s experiment requires meeting two additional conditions if it is to avoid going the way of previous failed policy initiatives: meaningful structural reforms that essentially change how segments of the economy respond and operate; and other countries’ continued acquiescence in the currency depreciation needed to boost the impact of slower-moving domestic dynamics through meaningful gains in global market share.
Meeting the first condition is in the hands of Japanese citizens and their elected representatives.
The required reforms, though achievable, will test the government’s resolve and implementation capabilities, as well as the population’s willingness to face immediate disruptions in exchange for the promise of longer-term gains.
The second requirement is very different.
It can be achieved only if other countries are willing to sacrifice output, either because they have no choice, or because they believe that, over the medium-term, a stronger Japanese economy will benefit them as the longer-term income effects offset the impact of immediate market disruptions.
But will the rest of the world accommodate Japan’s bold policy experiment, or will it take protective steps and thus impede the operation of a crucial policy transmission mechanism?
While initial indications are encouraging, the jury is still out.
Many affected countries – including those hit by the trade effects (such as China, South Korea, Taiwan, and eurozone members) and those susceptible to the capital-flow channel (such as Brazil, Indonesia, and Mexico) – have not yet had enough time to react.
Japan’s policy change was big and abrupt, and several of the countries on the receiving end have been focused on complex domestic challenges.
A few countries – particularly Brazil, China, and South Korea – have noticed.
But their reactions have been generally muted by Japan’s success in getting a US-led initiative at the G-20 to classify its policy response as constituting the use of “domestic tools” to pursue “domestic objectives.”
It is just a matter of time until the rest of the world catches up with the reality of how Japan’s experiment affects them.
The hope is that, bolstered by evidence of Japan’s serious pursuit of structural reforms, they will accommodate the experiment in two ways: by not retaliating, and by undertaking their own domestic reforms that compensate for the output lost to Japan.
In other words, a growing pie for all better accommodates all.
The fear is that neither Japan’s subsequent actions nor the affected countries’ domestic realities will justify the risk of lost market share, especially at a time when the global economy as a whole – and global policy coordination – is struggling.
Here the risk involves currency wars and other beggar-thy-neighbor disruptions.
There is currently insufficient data to predict either outcome confidently.
As we await additional evidence, let us appreciate how rarely we witness, in real time, such a momentous policy shift.
The Undeserving One Percent?
CHICAGO – It is amazing how the “one percent” epithet, a reference to the top 1% of earners, has caught on in the United States and elsewhere in the developed world.
In the United States, this 1% includes all those with a 2006 household income of at least $386,000.
In the popular narrative, the 1% is thickly populated with unscrupulous corporate titans, greedy bankers, and insider-trading hedge-fund managers.
Reading some progressive economists, it might seem that the answer to all of America’s current problems is to tax the 1% and redistribute to everyone else.
Of course, underlying this narrative is the view that this income is ill-gotten, made possible by Bush-era tax cuts, the broken corporate governance system, and the conflict-of-interest-ridden financial system.
The 1% are not people who have earned money the hard way by making real things, so there is no harm in taking it away from them.
Clearly, this caricature is based on some truth.
For instance, corporations, especially in the financial sector, reward too many executives richly despite mediocre performance.
But apart from tarring too many with the same brush, there is something deeply troubling about this narrative’s reductionism.
It ignores, for example, the fact that many of the truly rich are entrepreneurs.
It likewise ignores the fact that many of the wealthy are sports stars and entertainers, and that their ranks include professionals such as doctors, lawyers, consultants, and even some of our favorite progressive economists.
In other words, the rich today are more likely to be working than idle.
But what might be the most important overlooked fact is that the rise in income inequality is not just at the very top, though it is most pronounced there.
Academic studies suggest that the top tenth percentile of income distribution in the US, and elsewhere, is also moving farther away from the median earner.
This is an inconvenient fact for the progressive economist.
“We are the 90%,” sounds less dramatic than “we are the 99%.”
And, for some of the protesters, it may not even be true.
Perhaps most problematic, though, is that something other than plutocrat-friendly policies is largely responsible for the growing inequality.
That something is education and skills.
True, not every degree is a passport to a job.
Freshly-minted degree holders, especially from lower-quality programs, are finding it particularly hard to get a job nowadays, because they are competing with experienced workers who are also jobless.
Nevertheless, the unemployment rate for those with degrees is one-third the unemployment rate for those without a high school diploma.
Close examination suggests that the single biggest difference between those at or above the top tenth percentile of the income distribution and those below the 50th percentile is that the former have a degree or two while the latter, typically, do not.
Technological change and global competition have made it impossible for American workers to get good jobs without strong skills.
As Harvard professors Claudia Golden and Larry Katz put it, in the race between technology and education, education is falling behind.
To acknowledge the fact that the broken educational and skills-building system is responsible for much of the growing inequality that ordinary people experience would, however, detract from the larger populist agenda of rallying the masses against the very rich.
It has the inconvenient implication that the poor have a role in pulling themselves out of the morass.
There are no easy and quick fixes to education – every US president since Gerald Ford in the mid-1970’s has called for educational reforms, with little effect.
In contrast, blaming the undeserving 1% offers a redistributive policy agenda with immediate effects.
The US has tried quick fixes before.
Income inequality grew rapidly in the last decade, but consumption inequality did not.
The reason: easy credit, especially subprime mortgages, which helped those without means to keep up with the Joneses.
The ending, as everyone knows, was not a happy one.
The less-well-off ultimately became even worse off as they lost their jobs and homes.
The US needs to improve the quality of its workforce by developing the skills that are relevant to the jobs that its firms are creating.
Several steps can be taken towards these goals, including improving community attitudes towards education, reforming schools, tying the curriculum in community colleges and vocational institutions more closely to the needs of local firms, making higher education more affordable, and finding effective ways to retrain unemployed workers.
None of this is easy or likely to produce results quickly, and some of it may require more resources.
While eliminating inefficient spending, especially inefficient tax subsidies, can generate some of these funds, more tax revenues may be needed.
The rich can certainly afford to pay more, but if governments increase taxes on the wealthy, they should do it with the aim of improving opportunities for all, rather than as a punitive measure to rectify an imagined wrong.
The Unexamined Crisis
CHICAGO – Three years have now passed since the collapse of Lehman Brothers, which triggered the start of the most acute phase of the 2007-2008 financial crisis.
Is the financial world a safer place today?
Within days after the 9/11 terrorist attacks in 2001, the US had erected new and enormous security measures at airports throughout the country.
Within a month, the US military was on the ground in Afghanistan.
Within three years the US had an official report on the causes of the events of 9/11; the well-resourced expert commission that produced it identified the weaknesses of America’s national-security agencies and provided recommendations for addressing them.
But what do we have three years after the financial crisis began?
To be sure, America has the 2,000-page Dodd Frank Act to show for its efforts.
Unfortunately, few of those pages address any problem suspected to have caused the financial crisis.
Bond investors’ heavy reliance on credit-rating agencies, which tend to be laxer with powerful issuers, has not been fixed.
The shadow banking sector’s dependence on the official banking sector’s liquidity and guarantees, and thus ultimately on the government, has not even been touched.
And limits on financial institutions’ leverage will change only in the next decade.
The list of shortcomings goes on and on.
Money-market funds’ perverse incentives to take on excessive risk remain largely intact.
Problems with incentive pay have been ignored.
The most highly touted change – the separation between proprietary trading and commercial banking (also known as the “Volker rule,” after former US Federal Reserve chairman Paul Volker) – has nothing to do with what caused the crisis, and most likely was approved because it was ineffective.
The Financial Crisis Inquiry Commission, chaired by Phil Angelides, did produce a report on the crisis – in fact, three reports.
By contrast, the Rogers Commission, which investigated the causes of the Space Shuttle Challenger disaster, produced just one report.
The best minds of the time, including the Nobel laureate physicist Richard Feynman, were part of the investigation, and no stone was left unturned in finding the cause.
Ultimately, the culprit was precisely identified as a defective O-ring that became too stiff at low temperatures and caused a leak.
To convince the public, Feynman demonstrated that conclusion with a televised experiment.
Economics is not as precise a science as physics, but this cannot justify the failure of the   Angelides Commission.
With sufficient data, we do have methods to identify the likely causes of an economic phenomenon.
We are even better at refuting potential explanations.
The major limits are imposed by the availability of data, not by our methodologies.
But the data were not made available, because the interested parties were (and remain) afraid to share it, knowing full well what would be revealed.
Unfortunately, the Commission – composed mostly of elected officials, rather than experts – wasted its time in political squabbles.
Its members could not agree even on how to define subprime mortgages and calculate how many such mortgages there were in the United States at the time of the crisis.
A subsequent investigation into the work of the Commission was more thorough than the Commission’s investigation of the crisis.
After reading through the emails, a congressional report found evidence that the Commission’s work was guided by politics rather than fact-finding.
After all, the fixes had already been decided and approved, before any fact was found; the Commission’s focus was on supporting or discrediting (depending on the commissioner’s political party) the Dodd/Frank legislation, rather than on establishing the truth.
It was a great opportunity lost.
With its subpoena power, the Angelides Commission could have collected and made available to researchers the data needed to answer many crucial questions about the crisis.
Did companies that compensated their traders (and not just their CEOs) more highly take more risk?
Was financial institutions’ assumption of excessive risk the result of incompetence or stupidity, or was it a rational response to the implicit guarantee offered by the government?
Did the market see the spread of lax lending standards and price the relevant pools of loans accordingly, or was it fooled?
Who were the ultimate buyers of these toxic products, and why did they buy them?
How important a role was played by fraud?
These are the questions that needed to be answered.
Unfortunately, they are likely to remain unanswered without a major mandatory disclosure of data.
Barring that, there is the risk that we will find out what caused this crisis after the next one.
Austerity’s Children
WASHINGTON, DC – When economists discuss “fiscal adjustment,” they typically frame it as an abstract and complex goal.
But the issue is actually simple: Who will bear the brunt of measures to reduce the budget deficit?
Either taxes have to go up for some people, or spending must fall – or both.
“Fiscal adjustment” is jargon; what austerity is always about is the distribution of income.
Much of Europe is already aware of this, of course.
Now it’s America’s turn.
And current indications there suggest that the people most directly in line for a fiscal squeeze are those who are least able to defend themselves – relatively poor children.
For example, the current budget sequester (that is, across-the-board spending cuts) is already hurting programs like Head Start, which supports pre-school education.
The American comedian Jimmy Kimmel recently poked fun at his compatriots’ lack of fiscal knowledge by asking pedestrians on Hollywood Boulevard what they thought of &#160;“Obama’s decision to pardon the sequester and send it to Portugal.”
The segment is hilarious, but also sad, because the impact on some people’s lives is very real.
Around 70,000 children are likely to lose access to Head Start on our current fiscal course.
And much larger cuts are in store for early-childhood nutrition programs and health care.
Perhaps most shocking are the dramatic cuts to the Medicaid health-insurance program that the House of Representatives’ Republican majority have embraced in their latest budget proposal.
Paul Ryan, the chairman of the House Budget Committee, proposes to balance the budget over the next 10 years largely by slashing the program.
About half of all people covered by Medicaid are children.
Is it fair to force low-income children to bear the burden of fiscal adjustment?
According to data available on the economist Emmanuel Saez’s invaluable Web site, from 1993 to 2011, average real income for the bottom 99% of the population (by income) rose by 5.8%, while the top 1% experienced real income growth of 57.5%.
The top 1% captured 62% of all income growth over this period, partly owing to a sharp rise in returns to higher education in recent decades.
(On average, those with only a high school education or less have few good income prospects.)
This implies that, if anything, the tax system should become more progressive, with the proceeds invested in public goods that are not sufficiently provided by the private sector –&#160;things like early childhood education and preventive health care to minimize educational disruption resulting from common ailments like childhood asthma.
Think of it this way: In recent decades, some families chose locations and occupations that seemed to offer a reasonable means of support – and good prospects for their children.
Many of these decisions turned out badly, largely because information technology (computers and how they are used) eliminated many middle-class jobs.
Increasing globalization of trade also did not help in this regard.
In addition, as Till von Wachter of Columbia University has documented, prolonged periods of unemployment for parents have a severe and lasting negative impact on their children.
Children whose families cannot provide a decent start in life deserve help.
But America has not provided it – a point recently made by Jeb Bush, a leading contender for the Republican presidential nomination in 2016.
Today’s children did not play a role in any of these policy mistakes.
The preschoolers who are about to lose access to Head Start weren’t even born when they were made.
Imposing austerity on poor children is not just unfair; it is also bad economics.
When economists, again with their dry jargon, talk about a country’s “human capital,” what they really mean is the cognitive and physical abilities of its people.
As I pointed out in recent Congressional testimony, poor education leads to poor job prospects, poor families, and back to poor education – if not with a detour through incarceration, which makes it even harder to break the cycle.
Unfortunately, no one in a position of power is likely to heed such arguments.
They should.
When you travel to a foreign country for the first time, and you see neglected, ill-fed, and uneducated children, do you regard that country as likely to be one of the world’s great economic powers over the next half-century? Or do you worry for its future?
The Unfinished Cold War
MOSCOW – This November will mark the twentieth anniversary of the fall of the Berlin Wall.
But the end of confrontation in Europe may be proving only temporary.
One year after last summer’s war in Georgia, old divisions seem to be re-emerging in a different form.
Although the Cold War in Europe was declared over, the truth is, it never really finished.
When the Soviet Union withdrew from Central and Eastern Europe, we Russians believed that NATO would not be extended to the countries and territories from which we had withdrawn.
Our hope was for unification with Europe, a “common European home,” and the creation of a Europe “united and free.”
Our hopes were not starry-eyed self-deception.
After all, the leaders of the United States and Germany had promised Mikhail Gorbachev that NATO would not expand eastward.
At first, after they had vanquished communism, Russians regarded themselves as victors.
But, after a few euphoric years, the West began acting more and more like the Cold War’s winners.
Once the potential “military threat” posed by the Soviet Union had vanished into thin air, successive waves of NATO enlargement served neither a military nor an ideological purpose.
The West’s logic for enlargement was geopolitical: to bring the former Soviet republics and socialist states of Central and Eastern Europe into the Western sphere of political and economic influence.
At first, NATO’s new members were declared to have met both democratic and military criteria.
Later, these criteria were abandoned when NATO began to invite even the most backward and corrupt states to join.
NATO, moreover, not only enlarged its membership, but also transformed itself from an anti-Communist defensive alliance into an offensive grouping (with operations in Yugoslavia, Iraq and Afghanistan).
NATO’s expansion towards Russia’s own borders, and the membership of countries whose elites have historical complexes in regard to Russia, increased anti-Russian sentiment inside the alliance.
For all its efforts to improve its image, many Russians now view NATO as a much more hostile organization than they did in the 1990’s, or even before then.
Moreover, NATO enlargement has meant that Europe itself has still not emerged from the Cold War.
No peace treaty ended the Cold War, so it remains unfinished.
Even though the ideological and military confrontation of those times is far behind us, it is being replaced with a new stand-off – between Russia, on one hand, and the US and some of the “New Europeans” on the other.
My hope is that, when historians look back at Georgia’s attack on South Ossetia of last summer, the Ossetians, Russians, and Georgians killed in that war will be seen as having not died in vain.
Russian troops crushed Georgia’s army on the ground, but they also delivered a strong blow against the logic of further NATO expansion, which, if not stopped, would have inevitably incited a major war in the heart of Europe.
For the time being, the situation remains open.
The US failed to unleash some new form of Cold War after the South Ossetian episode, not least because of the global financial and economic crisis.
It is my hope that the global economic crisis and Barack Obama’s presidency will put the farcical idea of a new Cold War into proper perspective.
Greater Europe, in which I include not only Russia, but also the US, needs a new peace treaty, or rather system of accords, that draw a line under Europe’s horrible twentieth century and thus prevent a historical relapse.
What is needed is a new pan-European treaty on collective security, signed either by individual countries or by NATO and the EU, as well as by Russia and the Commonwealth of Independent States.
Countries not included in any of the current security systems would be able to join in the treaty and receive multilateral guarantees.
NATO enlargement would
With the break-up of the Soviet Union and Yugoslavia in mind, we must seek to prevent the further fragmentation of states, and also their forcible reunification.
Kosovo, South Ossetia, and Abkhazia must be the last of the states that break away through force.
The “Pandora’s box” of self-determination must be closed.
Once the legacy of confrontation inherited from the twentieth century has been overcome, perhaps deep cuts in the Russia and US nuclear arsenals may become possible, together with coordination of military-strategic policies.
In this scenario, Russian-US cooperation in crisis situations like Afghanistan, or in countering the proliferation of weapons of mass destruction, would become much more profound.
In Europe proper, a union between Russia and the EU should be founded, based on a common economic space, a common energy space – with cross-ownership of companies that produce, transport, and distribute energy – and a common human space that would be visa-free and include coordinated Russian and EU international policies.
Emphasis should also be placed on establishing a new system for governing the global economy and finance, whose creation will be even more difficult if the confrontations of the Cold War are not resolved.
Europe, Russia, and the US must finish the “unfinished war.”
Then, perhaps in 2019, the year that will mark the 100th anniversary of the Treaty of Versailles, we may finally bid farewell to the twentieth century.
The Euro on the Mend
PARIS – A year ago the eurozone was in serious trouble.
A series of policy actions – the creation of a rescue fund, a fiscal treaty, and the provision of cheap liquidity to the banking system – had failed to impress financial markets for long.
The crisis had moved from the monetary union’s periphery to its core.
Southern Europe was experiencing a sell-off of sovereign debt and a massive withdrawal of private capital.
Europe was fragmenting financially.
Speculation about a possible breakup was widespread.
Then came two major initiatives.
In June 2012, eurozone leaders announced their intention to establish a European banking union.
The euro, they said, had to be buttressed by transferring banking supervision to a European authority.
For the first time since the onset of the crisis in Greece, it was officially recognized that the root of the eurozone’s problem was not the flouting of fiscal rules, and that the very principles underlying the monetary union had to be revisited.
The endeavor was bound to be ambitious.
In the eyes of most observers, to reach the leaders’ goal of “break[ing] the vicious circle between banks and sovereigns” required centralizing authority for bank resolution and rescue.
The second initiative came a month later.
Speaking on July 26, European Central Bank President Mario Draghi announced that the ECB was ready to do “whatever it takes” to preserve the euro: “Believe me,” he said, “it will be enough.”
The meaning of these words became clear with the subsequent announcement of the ECB’s “outright monetary transactions” (OMT) scheme, under which it would purchase short-term government bonds issued by countries benefiting from the European rescue fund’s conditional support.
Both measures had an immediate and profound impact on financial markets.
Seen from Wall Street, the euro was moving closer to becoming a normal currency.
Turmoil in bond markets began to abate.
A year later, where are we?
First, the two initiatives resulted in markedly improved borrowing conditions for southern European governments (at least until Federal Reserve Board Chairman Ben Bernanke created new shockwaves with his indication in mid-June that the US would wind down more than three years of so-called quantitative easing).
Capital stopped flowing out of southern Europe and speculation eased.
Second, an agreement on authorizing the ECB to oversee the banking sector was reached at the end of last year.
In a year, the new regime will be fully operational – not a trivial achievement in view of the complexity of the issue.
Third, discussions are being held to prepare the next steps, namely how to arrange the resolution of failed banks and support for ailing ones.
Ministers recently agreed upon a template for action.
So there are clear positive outcomes. But questions remain.
One problem is architectural: any banking union is only as strong as its weakest component.
What matters for markets is not what happens in normal times, or even what happens when uncertainty and volatility rise; what markets care about are possible scenarios in truly adverse conditions.
Breaking the negative feedback loop between distressed sovereigns and distressed banks – whereby bank rescues exhaust fiscal resources and make it likely that the next financial institution in trouble will not be able to count on government support – requires ensuring that it will not recur even in extreme circumstances.
Merely “weakening” this loop, as European officials recently advocated, could prove deeply insufficient.
There are two ways to eliminate the feedback loop.
One is to exclude bank rescues altogether: only creditors would have to pay for bankers’ mistakes.
This type of rule could insulate governments from banking risk only if applied systematically, even at the expense of financial stability.
Simply put, governments should be ready to let banks fail.
The other solution is to mutualize the cost of rescue at the margin.
States could be involved and accept losses, but catastrophic risks would have to be shared among all eurozone members.
Europe these days is vacillating between these two approaches.
France does not want to rule out state-financed bailouts; Germany is reluctant to mutualize budgetary costs.
A compromise is being worked out, but it must pass the test of reality.
Unfortunately, the middle way between two logically consistent solutions may itself not be a logically consistent one.
Meanwhile, the credibility of Draghi’s atomic weapon is being undermined.
The miracle of the OMT scheme is that, since it was announced a year ago, it has had its intended effect without ever being used.
Strong opposition on the part of the Bundesbank and many German academics, however, has raised questions about whether and how it could ever be used.
To defend its legality in hearings before Germany’s Constitutional Court, the ECB itself has argued that the OMT program is a less potent instrument than many believe.
Although the German government has been adamant that it is not a German court’s role to rule on the legality of ECB instruments, markets have taken note.
In a few months, it will be four years since the eurozone crisis began –&#160;almost an eternity by historical standards.
Much has been done to overcome it.
But it is still too early to declare the job done and claim victory.
DNA at 60
LONDON – On April 25, 1953, Francis Crick and James Watson published a one-page paper that many believed would revolutionize biological research.
Building on the work of Rosalind Franklin and Maurice Wilkins, they had discovered DNA’s double-helix structure, providing the first glimpse into how organisms inherit and store biological information.
But, 60 years later, has their discovery really had the transformative impact that the world expected?
The media marked the publication’s 60th anniversary with much fanfare, hailing the breakthrough that “ushered in the age of genetics,” and calling it “one of the most important scientific discoveries of all time.”
The British newspaper The Guardian featured the headline, “Happy Birthday, DNA!
The golden moment that changed us all.”
To some extent, they are right.
The finding forms the basis of genetics and has opened up promising new research areas, such as synthetic biology, in which biological systems are created or modified to perform specific functions.
Likewise, it has facilitated important innovations, such as pharmacogenetic cancer treatment, in which drugs target specific genetic defects within cancer cells.
Moreover, DNA has acquired a certain mystique in popular culture.
According to Dorothy Nelkin and Susan Lindee, it has become a sacred entity – the modern equivalent of the Christian soul, an individual’s essence.
While some forms of biological determinism, such as the belief that race or gender dictates a person’s destiny, have been widely rejected, the idea that a person can be genetically predisposed, say, to get into debt, become a ruthless dictator, or vote regularly in elections remains socially acceptable.
But, almost from the beginning –&#160;and most intensely since 1971, when Time magazine published a special section entitled “The New Genetics: Man into Superman” – science and society alike have tended to overestimate the impact of genetics.
When the Human Genome Project published the first draft of the fully sequenced human genome in 2000, Henry Gee, an editor of the journal Nature, predicted that scientists would be able “to alter entire organisms out of all recognition to suit our needs and tastes” by 2099.
“We will have extra limbs, if we want them,” he asserted, “maybe even wings to fly.”
Thirteen years later, Gee’s prediction looks increasingly unlikely, with the Human Genome Project so far having failed to meet expectations.
Indeed, in 2010, the science writer Nicholas Wade lamented that, a decade after the project was launched, geneticists were “almost back to square one in knowing where to look for the roots of common disease.”
For example, a 12-year study of 19,000 white American women found that 101 genetic markers that had been statistically linked to heart disease had no predictive value.
Self-reported family histories, by contrast, proved very accurate in predicting the disease.
In fact, most diseases are not caused by single genes.
As a result, after a few early successes with atypical single-gene disorders such as Huntington’s disease, progress has stalled.
Common variants typically explain a small fraction of genetic risk.
Genetics has been a source of particularly high hopes when it comes to cancer treatment.
Between 1962 and 1985, cancer-related deaths in the United States rose by 8.7%, despite the use of aggressive chemotherapy drugs and radiation therapy, highlighting the dangers of a one-size-fits-all approach to treatment.
An understanding of the genetic determinants of patients’ therapeutic response, it was believed, would enable doctors to develop individualized treatment programs, sparing more responsive patients from harmful overtreatment.
But patients are not the only variable.
Cancer, too, is heterogeneous, even in patients with the same diagnosis.
After sequencing the entire genomes of 50 patients’ breast cancer tumors, researchers found that only 10% of the tumors had more than three mutations in common.
According to a recent study mapping genetic mutations in 2,000 tumors, breast cancer can actually be divided into ten subgroups.
Similarly, a genome-wide analysis of malignant cells from four kidney-cancer patients showed that, while they were related, they had mutated in many different directions.
Two-thirds of the genetic faults identified were not repeated in the same tumor, let alone in any other metastasized tumors in the body.
Given that a pharmacogenetic drug targets one mutation in the tumor, it will not necessarily work on the other mutations.
In addition, as the cancer adjusts to the drug, further mutations are likely to occur, diminishing the drug’s efficacy.
To be sure, pharmacogenetics has made a profound difference for some patients.
Barbara Bradfield, one of the original subjects in research trials for the pharmacogenetic cancer drug Herceptin, has now been stable on the drug for more than 20 years.
But such success stories are far too rare to constitute a “golden age” of genetics.
The high price of such drugs is limiting their impact as well.
Herceptin can cost up to $40,000 annually, and newer cancer drugs cost even more, making them prohibitively expensive for most patients.
The US Supreme Court is currently faced with the question of whether genes can be patented.
If the court upholds the biotechnology company Myriad Genetics’ patents on two genes which, in some variants, are linked to higher risk for breast and ovarian cancer, the company will retain exclusive rights to use the genes in research, diagnosis, and treatment for two decades, preventing rivals from developing cheaper alternatives.
Women have already been denied access to a diagnostic test, because insurers refuse to pay the company’s high prices.
Manufacturers claim that gene patents, which now cover 25-40% of the human genome, are vital to recouping their investments.
But such patents mar DNA’s “birthday” celebrations for the patients who stand to benefit from the fruits of genetic research – if only they could afford them.
The Ungreening of the World
Everyone I meet claims to love trees -- I mean 
 really
 love trees -- yet collectively the human race behaves as if it abhors green things.
If you take a step back from whatever biome you are in at the moment and look at the entire Earth and its forests through recorded history, you will see that the relationship between humans and trees looks 
 Strangely Like War
 (the title of a recent book on forests by Derrick Jensen and George Draffan). 
The exact extent of the damage is difficult to discern, because for many years records were not kept, but the estimates are that 75% of the world’s original forests have been logged or burned by humans.
Some of them have grown back of course, or have been replanted, but it is thought that we now have only half of the amount of forest land we once had on this planet. 
In some places, particularly the drier places of the globe, the deforestation was so severe, and was followed by such intense grazing, that forests have not been able to grow back.
The landscape has been permanently altered. 
When you imagine Greece, Italy, and Iraq, it is likely that you imagine a dry landscape with open views, the way they look today.
Historical records indicate, however, that these places were once covered by dense forests.
The forests fell as civilizations flourished, so the earlier a place became “civilized” the sooner it became deforested. 
This march of so-called progress resulting in the loss of forests was documented by John Perlin in his 1989 book 
 A Forest Journey
 .
So today we sit on a planet with only 50% of its forest cover remaining.
And here’s the part that should bring tears to your eyes: we continue to lose more forest cover every year. 
The more recent losses are well documented.
Every five years the United Nations produces a summary report called the Global Forest Resource Assessment; The team in charge of assembling the assessment relies on internet reporting and satellite surveillance to come up with the figures.
According to the most recent report, between 2000 and 2005, we lost forest acreage equivalent to the land mass of Panama -- more than 77 thousand square kilometers of forest gone, some of it never to return. 
The next report is due to be released in 2010.
I will not be surprised when it is released and I read that the global forest area has continued to shrink. 
If this happens when we claim to love trees, I shudder to think what would happen if we were ambivalent about them?
Or thought we could live without them? 
In the United States, deforestation began as soon as the colonies were settled.
Before long, the colonies were exporting wood to the many nations that no longer had the timber they needed for ships, casks, shingles, and other construction materials.
Trees were also cut to clear cropland, provide heat, and the fledgling nation was using up its forests to build its own ironworks and railroads as well. 
By 1920, more than three-quarters of the US’s original forests had been cut.
Similar to the global figures, today the US has only half the forest cover that it had in 1600.
And we continue to destroy forest land. 
At the UN Conference on Environment and Development in Rio de Janeiro it was agreed that, “efforts should be undertaken towards the greening of the world.”
The UN recognizes that “forests are essential to economic development and all forms of life.”
But the UN Charter also reads: “states have the sovereign right to exploit their own resources.”
And so we do.
Although the UN and my country recognize the value of forests, both ecologically and economically, such recommendations are not strong enough to stop my local council from voting “yes” to deforestation.
Last week, I went to a zoning meeting in the town where I live.
A real estate housing project developer wanted to cut many acres of trees so he could build houses.
That forest land will be lost, probably forever, and a few more numbers will be added to the global deforestation total next year. 
Why do local politicians, tree lovers all, allow yet more forest destruction?
Why do humans all claim to love trees, but their actions deny their claim?
I think it has to do with fear.
When a would-be exploiter of trees stands before a politician and requests, or demands, the right to clear a forest, the politician, out of fear, complies.
But we do not fear trees.
We do not fear their retaliation. 
Trees stand mute despite our betrayal.
Perhaps that is one of the reasons we really love them.
But if we want to do more than love them, if we want to save them, we must become fearless.
The Unilateral Road to Peace
In the next three weeks, the Israeli army will do something unprecedented: instead of defending the country against external enemies or terrorism, it will evacuate – by force if necessary – 9,000 Israelis from their homes.
Israel’s unilateral withdrawal is the outcome of a deep political shift that has been caused by two somewhat contradictory convictions that have characterized Prime Minister Ariel Sharon’s policies since 2003: first, that the US-initiated “road map” is going nowhere and, second, that the status quo is untenable.
Certainly, according to Sharon’s thinking, there is no future for 9,000 Jewish settlers living among 1.2 million Palestinians in the Gaza Strip.
Hence, in the absence of negotiations, a unilateral withdrawal is the only meaningful step towards de-escalation and stabilization.
This policy has deeply divided Israel: the settlers – mostly, but not exclusively religious – feel betrayed by Sharon, “The Father of the Settlements.”
It is now the Israeli left that, however reluctantly, realizes that Sharon’s new pragmatism may be the first step in the right direction.
Like de Gaulle in Algeria, Sharon has reshuffled the cards of Israel’s politics.
The last weeks have seen massive demonstrations, some of them verging on violence; many of the settlers have declared that they will not obey government orders to evacuate.
The army and the police have mobilized almost 50,000 troops to carry out evictions.
This forcible approach is traumatic not only for the settlers, but also for many other Israelis.
Only the next days and weeks will tell whether the evacuation proceeds peacefully or not.
In the meantime, Sharon has paid heavily for his policies.
Two right-wing parties have left his ruling coalition, he has lost his majority within his own Likud Party, and his finance minister, Binyamin Netanyahu, has quit his government.
Only by bringing Shimon Peres’s Labor Party into his cabinet did Sharon manage to retain his parliamentary majority.
The question, of course, is what will happen after the Gaza disengagement.
Many governments were initially skeptical about disengagement, but realized that – as the Washington saying goes – “this is the only game in town.”
Among them, as well as among the Palestinians, one hears the hope that after the Gaza withdrawal, it will be possible to return to the road map and to resume negotiations leading to a final agreement between Israel and the Palestinians.
This is a major mistake.
While motivated by the best of intentions, the hope of reviving the road map is out of touch with reality.
Future negotiations will have to deal with the ultimate borders between Israel and Palestine, the fate of 200,000 Jewish settlers in the West Bank, the status of Jerusalem, and with the problem of the 1947-48 Palestinian refugees.
On all of these issues, the gulf between the Israeli and Palestinian positions has not narrowed since the failed negotiations at Camp David in 2000, while fear and distrust have increased.
To attempt negotiations under these circumstances would not only be an exercise in futility, but may merely deepen alienation and suspicion on both sides.
The failure of the Annan Plan for Cyprus suggests that good intentions are not enough – and the disagreements in Cyprus were miniscule compared to what divides Israelis and Palestinians.
So what can be done?
Probably the only rational way to proceed would be to acknowledge that unilateral steps on both sides can still further the cause of de-escalation and ultimate reconciliation.
On the Israeli side, further disengagement from dozens of isolated and small settlements on the West Bank, entailing the evacuation of between 20,000 and 30,000 settlers, may be politically feasible and would give the Palestinians a contiguous territory on the West Bank.
On the Palestinian side, consolidation of the Palestinian Authority’s control over a dozen security services and militias would be an important step.
The Palestinian leader, Abu Mazen, recognizes this, but the question is whether he can deliver.
The Palestinian leadership could also start down the difficult road of telling the refugees that – contrary to almost 50 years of Palestinian propaganda – they will not return to Israel, but will have to be settled in the West Bank and Gaza, the areas that will become part of the eventual Palestinian state.
In an ideal world, conflicts end by agreements and treaties.
But in the real world – and Cyprus, Bosnia, and Kosovo may be examples – stabilization and the slow de-escalation of violence can sometimes achieve the same end, even without formal agreement.
In the absence of plausible alternatives, one hopes that this will also be the case for Israelis and Palestinians.
The Unraveling of Europe’s Peace
COPENHAGEN – The European Commission recently unveiled long-awaited measures to bring neighboring countries in the Mediterranean and the former Soviet Union closer to Europe.
On the same day, another department of the same Commission presented proposals aimed at curbing visa-waiver programs for some non-European nationals.
Few missed the irony of formulating two plans that pointed in opposite directions.
Attracting neighbors has long been a noble aspiration – and something of a European specialty.
The European Union’s embrace of post-communist republics in Central Europe represented a most powerful symbol of the reach of Western liberal democracy.
In today’s neighborhood, where EU expansion is not in the cards, Europe hopes to shore up its presence by opening its huge internal market and increasing assistance.
Crucially, the Commission’s recent proposals include the creation of “mobility partnerships” with Tunisia, Morocco, and Egypt, aimed at facilitating travel for local students and businesspeople.
By contrast, the proposed restrictions on the visa-waiver program include “safeguard clauses” that would temporarily suspend access to Europe’s Schengen area, most likely for those from Balkan countries.
This is controversial enough: the decision is motivated by a large influx of asylum-seekers, often offering frivolous reasons, originating from Serbia.
But visa liberalization has been the main concrete signal of Europe’s goodwill towards this neglected backyard, which dreams of joining the EU.
Whatever this plan’s impact in practice, the political message is clear: when in doubt, Europe is better off sealing its borders.
The same Janus-faced approach is evident in Europe’s response to the Arab Spring.
After a lukewarm reaction to the uprisings, Europe was eager to show its support for democratic movements in the region. At the same time, with boatloads of migrants arriving from Tunisia, some rather drastic measures have been adopted.
A recent dispute between Italy (the main port of arrival) and France (the principal final destination) ended with the French reintroducing border controls.
In an unrelated move, Denmark did the same, ostensibly to prevent cross-border crime.
To its credit, the European Commission also issued strong calls to member states for better legislation and practices concerning migration.
But there is a clear correlation between unrest at the EU’s doorstep and Europe’s irresistible instinct to keep trouble at arm’s length.
For once, the rot is not in Brussels, but rather in a growing number of European capitals.
The case of Italy is instructive: “human tsunami” is the unfortunate phrase used by senior policymakers to warn against the possible flood of migrants.
But, almost six months into the North African upheavals, the number of arrivals on the southern island of Lampedusa has reached roughly 30,000.
By comparison, Sweden, with one-sixth the population of Italy, accepted the same number of asylum-seekers in 2009.
Italian officials privately confirm that the current figures are not unmanageable.
The problem for Italian officials, as for the other governments concerned by the recent migration flows, is the pressure of right-wing populist parties, which no longer need to be on the defensive.
The case for openness, inclusion, and diversity in European societies has become much harder to make.
Not coincidentally, mainstream leaders, from German Chancellor Angela Merkel to British Prime Minister David Cameron, have caught up with the current mood by deeming European multiculturalism a failure.
This turn of events comes at a price.
The genius of modern Europe has consisted in linking long-term stability to the pursuit of ever-deeper economic and political integration.
For the past half-century, this has represented Europe’s revolutionary recipe for peace, and has served as something of a microcosm of globalization.
The ever-freer and faster flow of capital, labor, goods, and ideas found in the EU a model and a forerunner.
Free movement of people within Europe constitutes this visionary project’s most tangible feat.
One unintended effect of the Arab revolutions is that the link between security and integration that forms Europe’s foundation is decoupling.
The advantages of pooling sovereignty and resources ring increasingly hollow to ordinary Europeans.
Governments find it more politically rewarding to pursue security by erecting administrative or physical barriers.
As election campaigns beckon in some of the countries that are now debating immigration controls, this trend is unlikely to be reversed any time soon. But Europeans should make no mistake about the consequences.
Opposing Europe now means not only standing up to an unelected behemoth in Brussels, as Euro-skeptics would have it.
Nor is it merely about questioning the sources of Europe’s influence in a fast-changing world. Unraveling the nexus between security and integration means nothing less than rejecting the formula of Europe’s peace.
Grand Mal Economics
BERKELEY – Across the North Atlantic region, central bankers and governments seem, for the most part, helpless in restoring full employment to their economies.
Europe has slipped back into recession without ever really recovering from the financial/sovereign-debt crisis that began in 2008.
The United States’ economy is currently growing at 1.5% per year (about a full percentage point less than potential), and growth may slow, owing to a small fiscal contraction this year.
Industrial market economies have been suffering from periodic financial crises, followed by high unemployment, at least since the Panic of 1825 nearly caused the Bank of England to collapse.
Such episodes are bad for everybody – workers who lose their jobs, entrepreneurs and equity holders who lose their profits, governments that lose their tax revenue, and bondholders who suffer the consequences of bankruptcy – and we have had nearly two centuries to figure out how to deal with them.
So why have governments and central banks failed?
There are three reasons why the authorities might fail to restore full employment rapidly after a downturn.
For starters, unanchored inflation expectations and structural difficulties might mean that efforts to boost demand show up almost entirely in faster price growth and only minimally in higher employment.
That was the problem in the 1970’s, but it is not the problem now.
The second reason might be that even with anchored inflation expectations (and thus price stability), policymakers do not know how to keep them anchored while boosting the flow of spending in the economy.
And here I stop, flummoxed.
At least as I read the history, by 1829, Western Europe’s technocratic economists had figured out why these periodic grand mal economic seizures occurred.
That year, Jean-Baptiste Say published his Cours Complet d’Economie Politique Pratique, admitting that Thomas Malthus had been at least half right in arguing that an economy could suffer for years from a “general glut” of commodities, with nearly everybody trying to reduce spending below income – in today’s jargon, to deleverage.
And, because one person’s spending is another’s income, universal deleveraging produces only depression and high unemployment.
Over the following century, economists like John Stuart Mill, Walter Bagehot, Irving Fisher, Knut Wicksell, and John Maynard Keynes devised a list of steps to take in order to avoid or cure a depression.
1.
Don’t go there in the first place: avoid whatever it is – whether external pressure under the gold standard, asset-price bubbles, or leverage-and-panic cycles such as that of 2003-2009 – that creates the desire to deleverage.
2.
If you do find yourself there, stop the desire to deleverage by having the central bank buy bonds for cash, thereby pushing down interest rates, so that holding debt becomes more attractive than holding cash.
3.
If you still find yourself there, stop the desire to deleverage by having the Treasury guarantee risky assets, or issue safe ones, in order to raise the quality of debt in the market; this, too, will make holding debt more attractive.
4.
If that fails, stop the desire to deleverage by promising to print more money in the future, which would raise the rate of inflation and make holding cash less attractive than spending it.
5.
In the worst case, have the government step in, borrow money, and buy stuff, thereby rebalancing the economy as the private sector deleverages.
There are many subtleties in how governments and central banks should attempt to accomplish these steps.
And, indeed, the North Atlantic region’s governments and central banks have tried to some degree.
But it is clear that they have not tried enough: the “stop” signal of unanchored inflation expectations, accelerating price growth, and spiking long-term interest rates – all of which tell us that we have reached the structural and expectational limits of expansionary policy – has not yet been flashed.
So we remain far short of full employment for the third reason.
The issue is not that governments and central banks cannot restore employment, or do not know how; it is that governments and central banks will not take expansionary policy steps on a large enough scale to restore full employment rapidly.
And here I reflect on the 1930’s, and on how historical events recur, appearing first as tragedy and then, pace Karl Marx, as yet another tragedy.
Keynes begged the policymakers of his time to ignore the “austere and puritanical souls” who argue for “what they politely call a ‘prolonged liquidation’ to put us right,” and professed that he could “not understand how universal bankruptcy can do any good or bring us nearer to prosperity.”
Today’s policymakers, so eager to draw a bold line under expansionary measures, should pause and consider the same question.
The Unreality of the “Real” Business Cycle
LONDON – Testifying recently before a United States congressional committee, former Federal Reserve Chairman Alan Greenspan said that the recent financial meltdown had shattered his “intellectual structure.”
I am keen to understand what he meant.
Since I have had no opportunity to ask him, I have to rely on his memoirs, The Age of Turbulence , for clues.
But that book was published in 2007 – before, presumably, his intellectual structure fell apart.
In his memoirs, Greenspan revealed that his favorite economist was Joseph Schumpeter, inventor of the concept of “creative destruction.”
In Greenspan’s summary of Schumpeter’s thinking, a “market economy will incessantly revitalize itself from within by scrapping old and failing businesses and then reallocating resources to newer, more productive ones.”
Greenspan had seen “this pattern of progress and obsolescence repeat over and over again.”
Capitalism advanced the human condition, said Schumpeter, through a “perennial gale of creative destruction,” which he likened to a Darwinian process of natural selection to secure the “survival of the fittest.”
As Greenspan tells it, the “rougher edges” of creative destruction were legislated away by Franklin Roosevelt’s New Deal, but after the wave of de-regulation of the 1970’s, America recovered much of its entrepreneurial, risk-taking ethos.
As Greenspan notes, it was the dot-com boom of the 1990’s that “finally gave broad currency to Schumpeter’s idea of creative destruction.”
This was the same Greenspan who in 1996 warned of “irrational exuberance” and, then, as Fed chairman, did nothing to check it.
Both the phrase and his lack of action make sense in the light of his (now shattered) intellectual system.
It is impossible to imagine a continuous gale of creative destruction taking place except in a context of boom and bust.
Indeed, early theorists of business cycles understood this.
(Schumpeter himself wrote a huge, largely unreadable book, with that title in 1939.)
In classic business-cycle theory, a boom is initiated by a clutch of inventions – power looms and spinning jennies in the eighteenth century, railways in the nineteenth century, automobiles in the twentieth century.
But competitive pressures and the long gestation period of fixed-capital outlays multiply optimism, leading to more investment being undertaken than is actually profitable.
Such over-investment produces an inevitable collapse.
Banks magnify the boom by making credit too easily available, and they exacerbate the bust by withdrawing it too abruptly.
But the legacy is a more efficient stock of capital equipment.
Dennis Robertson, an early twentieth-century “real” business-cycle theorist, wrote: “I do not feel confident that a policy which, in the pursuit of stability of prices, output, and employment, had nipped in the bud the English railway boom of the forties, or the American railway boom of 1869-71, or the German electrical boom of the nineties, would have been on balance beneficial to the populations concerned.”
Like his contemporary, Schumpeter, Robertson regarded these boom-bust cycles, which involved both the creation of new capital and the destruction of old capital, as inseparable from progress.
Contemporary “real” business-cycle theory builds a mountain of mathematics on top of these early models, the main effect being to minimize the “destructiveness” of the “creation.”
It manages to combine technology-driven cycles of booms and recessions with markets that always clear (i.e., there is no unemployment).
How is this trick accomplished?
When a positive technological “shock” raises real wages, people will work more, causing output to surge.
In the face of a negative “shock,” workers will increase their leisure, causing output to fall.
These are efficient responses to changes in real wages.
No intervention by government is needed.
Bailing out inefficient automobile companies like General Motors only slows down the rate of progress.
In fact, whereas most schools of economic thought maintain that one of government’s key responsibilities is to smooth the cycle, “real” business-cycle theory argues that reducing volatility reduces welfare!
It is hard to see how this type of theory either explains today’s economic turbulence, or offers sound instruction about how to deal with it.
First, in contrast to the dot-com boom, it is difficult to identify the technological “shock” that set off the boom.
Of course, the upswing was marked by super-abundant credit.
But this was not used to finance new inventions: it was the invention.
It was called securitized mortgages.
It left no monuments to human invention, only piles of financial ruin.
Second, this type of model strongly implies that governments should do nothing in the face of such “shocks.”
Indeed, “real” business-cycle economists typically argue that, but for Roosevelt’s misguided New Deal policies, recovery from the Great Depression of 1929-1933 would have been much faster than it was.
Equivalent advice today would be that governments the world over are doing all the wrong things in bailing out top-heavy banks, subsidizing inefficient businesses, and putting obstacles in the way of rational workers spending more time with their families or taking lower-paid jobs.
It reminds me of the interviewer who went to see Robert Lucas, one of the high priests of the New Business Cycle school, at a time of high American unemployment in the 1980’s.
“My driver is an unemployed Ph.D. graduate,” he said to Lucas.
“Well, I’d say that if he is driving a taxi, he’s a taxi-driver,” replied the 1995 Nobel Laureate.
Although Schumpeter brilliantly captured the inherent dynamism of entrepreneur-led capitalism, his modern “real” successors smothered his insights in their obsession with “equilibrium” and “instant adjustments.”
For Schumpeter, there was something both noble and tragic about the spirit of capitalism.
But those sentiments are a world away from the pretty, polite techniques of his mathematical progeny.
The UN’s Mandate Gap
The United Nations peacekeeping operations now underway in Lebanon offer a big opportunity for the UN to demonstrate its relevance and impact on the world stage in the 21st century.
If only those member states who claim to be the UN’s biggest supporters put their money where their mouths are.
Many world leaders, particularly those in Europe, decry the Bush administration’s undermining of the UN, especially since 2003.
Yet leaders in France, who expressed outrage when the US sidestepped the UN and invaded Iraq without the international community’s blessing, stunned the world in August when they backed down from their promise to send 2,000 peacekeepers to intervene in southern Lebanon, and instead only committed 200.
Fortunately, France has reconsidered, Germany will provide limited naval assistance, and Italy has stepped up to contribute 3,000 peacekeepers. In addition, China has recently pledged 1,000 troops.
But Europe’s response, like the US response in other cases, highlights a critical issue for all supporters of the UN and international institutions more generally.
If we cannot do what it takes to make them more effective, we will increasingly find that nations will bypass them altogether.
UN Security Council Resolution 1701 “calls for Israel and Lebanon to support a permanent ceasefire.”
It thus set the stage for UN officials to establish the “Rules of Engagement” (ROEs) for its peacekeepers, which dictate when and under what circumstances UN troops can fire their weapons to defend themselves.
But as the current UN mission in Lebanon (UNIFIL) well knows, defending yourself is not the same as protecting yourself from hostile fire in the first place.
In this context, the French are understandably worried about the fate of their soldiers – soldiers charged with supporting the Lebanese government in its efforts to establish control over the Hezbollah-controlled south.
The terrible French peacekeeping experience in Bosnia in the early 1990s, in which France lost 84 soldiers serving in a humanitarian capacity under restrictive ROEs, justifies their fears.
But ROEs are only the symptom of a deeper problem.
The real issue is a yawning gap between paper and practice.
In the heat of an international crisis, the Security Council passes resolutions to great public fanfare, establishing an official UN “mandate.”
But then the UN Secretary General is left, resolution in hand, to ask UN member states for the actual, tangible resources necessary to implement what has been commanded.
In the overwhelming majority of cases, those resources fall far short of what is required to successfully intervene in a crisis.
A 2006 UN mandate review finds that UN member states adopt hundreds of mandates each year, conferring “additional responsibilities with neither corresponding funds nor guidance” on how resources should be used.
In American domestic politics, these kinds of commands from the US Congress to states are known as “unfunded mandates;” ordering results without providing the resources necessary to achieve them.
It’s political theater - big headlines, small results.
The UN’s experience in Lebanon is not encouraging.
According to the UN’s Department of Peacekeeping Operations, up to now UNIFIL has operated on an annual budget of $94 million and suffers chronic budget shortfalls due to unpaid assessments from member states.
Now consider what an expanded UNIFIL is mandated to do under Resolution 1701: peacekeepers must monitor the ceasefire between Israel and Hezbollah; support and accompany the Lebanese armed forces as they deploy in southern Lebanon; assist Lebanon’s government in securing the country’s borders and ports to keep illegal weapons from getting into Hezbollah’s hands; and “help ensure humanitarian access to civilian populations and the voluntary and safe return of displaced persons.”
This is a Herculean task.
But over a month after the resolution, barely 5,000 troops are deployed.
Homes and livelihoods have been destroyed.
Who will build or re-build hospitals?
How will communications infrastructure be rebuilt?
Who will repair Lebanon’s bridges and roads?
The mandate gap reflects the way the world has done business with the UN for decades – big promises, small pay-outs, much scapegoating if the UN then fails.
But today the international community has a tremendous window of opportunity to secure a small but significant peace in the Middle East along the volatile border of Lebanon and Israel.
How member states respond now – especially those who believe in the purpose and value of the UN - may help shape the outcomes of other, wider ongoing regional conflicts, notably with Iran and Syria, both of which support Hezbollah.