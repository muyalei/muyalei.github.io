Only the United Kingdom (which, like France itself, last resorted to the veto in 1989) has given any hint of support for the French initiative.
The right to veto was the price demanded by China, France, Great Britain, Russia, and the US for joining the UN.
No one believes that a formal Charter amendment to abolish or limit this right is remotely likely.
But international pressure on the P5 has been mounting for the last 15 years – and especially since the General Assembly’s unanimous embrace in 2005 of the “responsibility to protect” (R2P) principle.
Advocates of the French position want these countries to forswear their veto when a clear majority supports proposed action to mitigate the risk of a mass-atrocity crime.
Distaste for the blocking of the Syrian resolutions has been particularly intense, and, at last count, 68 countries had given explicit support to the French proposal in various UN forums.
The moral argument that the veto should not be used in cases of mass-atrocity crimes is overwhelming.
The P5 have obligations under the UN Charter, as well as international humanitarian and human rights law, not to undermine the effectiveness of the UN or that body of law.
And the political argument against using the veto in these situations – that it jeopardizes the credibility and legitimacy of the Security Council, whose structure is already seen as not reflecting the geopolitical realities of the twenty-first century – should also weigh heavily on the P5.
But is it possible to craft a veto-restraint proposal to which all of the P5 can agree?
In January, at a conference I attended in Paris that brought together French policymakers and international experts, it became clear that a draft agreement could meet most, if not all, objections.
But it would need to have at least three key elements.
First, the agreement would have to define the relevant cases clearly – neither too widely nor too narrowly – and build on well-established R2P language.
The definition might be something like “situations where populations are suffering, or at imminent risk of, genocide, other crimes against humanity, or major war crimes.”
Second, an agreement would need to include a mechanism to determine when such cases had actually arisen.
This would need to be speedy, provide some assurance of objective assessment, and ideally generate strong concern across a wide cross-section of the international community.
One way to meet these needs would be to have a double trigger.
The first requirement would be a certification, communicated to the Security Council by the UN Secretary-General and his Office of Special Advisers on the Prevention of Genocide and R2P (which has the necessary resources, expertise, and credibility) that the case meets the agreed definition.
The other would be a request for veto-restraint by at least 50 member states, including at least five members from each of the recognized geographical groupings.
A third key element, unattractive ethically but probably necessary politically to win the support of the US and others, would be a provision allowing any P5 member to veto when it claimed a “vital national interest” to be at stake.
The consolation is that trying to rely on such an escape clause in most atrocity cases would not pass the laugh test.
Could Russia and China really have used it to veto Security Council resolutions on Myanmar and Zimbabwe in, respectively, 2007 and 2008?
Even given the intensity of Russia’s political and military relationship with Bashar al-Assad’s regime in Syria, could it really claim that a resolution would place its own vital interests at risk?
Many kinds of pushback can be expected, not least the argument that the veto exists not to protect the P5’s interests, but to ensure unanimity of the major players (conspicuously missing in the UN’s ill-fated predecessor, the League of Nations) in any action undertaken, in order to maintain international peace and security.
We will be told that it is unconscionable to ask a P5 member to forgo a veto when it genuinely believes that a proposed resolution will cause more harm than good.
One response is that it is almost impossible to find any such genuinely high-minded rationale for any veto ever cast in a mass-atrocity situation.
Another is that any UN Security Council resolution requires at least nine affirmative votes (in a Council of 15).
If there are genuine concerns on the merits, that requirement will prove a very high bar.
The point of the veto restraint is, at minimum, to raise the political cost for those who would block action designed to ensure that there are no more Cambodias, Rwandas, Srebrenicas, or Syrias.
The French proposal, though still evolving, has already struck a responsive chord internationally.
The other P5 members will ignore it at their peril.
Financial Regulators’ Global Variety Show
PARIS – In the early phases of the financial crisis, it was fashionable to argue that the United States’ system of regulation needed a fundamental structural overhaul.
Differences of opinion between the Securities and Exchange Commission (SEC) and the Commodity Futures Trading Commission (CFTC) had obstructed effective oversight of investment banks and derivatives trading (only the US believes that it makes sense to regulate securities and derivatives separately).
Indeed, the plethora of separate banking regulators had created opportunities for banks to arbitrage the system in search of a more indulgent approach to capital.
Likewise, the lack of a federal insurance regulator had left AIG regulated by the Office of Thrift Supervision (OTS) and the New York State Insurance Department, which proved to be a wholly inadequate arrangement.
Little has come of these arguments.
The Dodd-Frank Act did succeed in putting the OTS out of its misery, but jealous congressional oversight committees have prevented a merger of the SEC and CFTC, and nothing has been done to rationalize banking supervision.
So the American system looks remarkably similar to the one that turned a collective blind eye to the rise of fatal tensions in the early 2000’s.
One factor that contributed to institutional stasis was the absence of a persuasive alternative.
In the decade or so leading up to the meltdown of 2007-2008, the global trend was toward regulatory integration.
Almost 40 countries had introduced single regulators, merging all types of oversight into a single all-powerful entity.
The movement began in Scandinavia in the early 1990’s, but the most dramatic change came in 1997, when the United Kingdom introduced its Financial Services Authority (I was its first chairman).
Other countries adopted slightly different models.
A fashionable approach was known as “twin peaks,” whereby one regulator handled prudential regulation – setting capital requirements – while another oversaw adherence to business rules.
But twin peaks itself was further subdivided.
The Dutch model brigaded the prudential regulators inside the central bank, while the Australian version was built on a separate institution.
These integrated structures seemed to offer many advantages.
There were economies of scale and scope, and financial firms typically like the idea of a one-stop (or, at worst, a two-stop) shop.
A single regulator might also be expected to develop a more coherent view of trends in the financial sector as a whole.
Unfortunately, these benefits did not materialize, or at least not everywhere.
It is hard to argue that the British system performed any more effectively than the American, so the single-regulator movement has suffered reputational damage.
And the continuing travails of the Dutch banking system – another bank was nationalized last month – suggest that it is easy to fall into the gap between twin peaks.
The truth is that it is hard to identify any correlation between regulatory structure and success in heading off or responding to the financial crisis.
Among the single-regulator countries, Singapore and the Scandinavians were successful in dodging most of the fatal bullets, while the UK evidently was not.
Among the twin peak exponents, the Dutch system performed very poorly indeed, while Australian financial regulation may be considered a success.
Does it matter whether the central bank is directly involved?
Many central bankers maintain that the central bank is uniquely placed to deal with systemic risks, and that it is essential to carry out monetary and financial policies in the same institution.
Again, it is hard to find strong empirical support for that argument.
The Dutch and American central banks, with direct oversight of their banking systems, were no more effective in identifying potentially dangerous systemic issues than were non-central bank regulators elsewhere.
Canada is often cited as a country that steered its banks away from trouble, even though they sit uncomfortably close to US markets.
But the Bank of Canada is not now, and has never been, a hands-on institutional supervisor.
So perhaps the US Congress has been right to conclude that changing the structure of regulatory bodies is less important than getting the content of regulation right.
Elsewhere, though, a lot of structural change is under way.
In the UK, every financial disturbance leads to calls to revamp the system.
There were major overhauls in 1986, and again in 1997, when the Bank of England lost its banking supervision responsibilities as a delayed response to the collapse of Barings.
Next month, it gets them back – and more.
For the first time, the Bank of England will supervise insurance companies as well.
A similar change has been introduced in France, where a new Prudential Control Authority has been created.
The British and French rarely agree on anything; one is tempted to say that when they do, they are highly likely to be wrong.
It is difficult now to discern a coherent pattern.
Certainly, the trend toward full-service single regulators outside the central bank has slowed to a crawl (though Indonesia is consolidating regulators at present).
There is no consensus on the role of the central bank: in around a third of countries, it is the dominant player, in another third it has responsibilities for banks only, while in the remaining third it is a system overseer only.
We could see this as a controlled experiment to try to identify a preferred model.
After all, financial systems are not so different from one another, particularly in OECD countries.
But there is no sign of a considered assessment being prepared, which might at least help countries to make better-informed choices, even if it did not conclude that one model was unambiguously best.
The G-20, under its current Russian presidency, is now in search of a role.
Here is a useful practical task it might take on.
Selecting Our Children
MELBOURNE – In April, Germany’s parliament placed limits on the use of genetic diagnosis.
Is the new German law a model for other countries to follow as we grapple with the ethical issues posed by our growing knowledge of human genetics?
Some provisions of the German law draw on the widely shared ethical principles of respect for individual autonomy and privacy.
No one can be tested without his or her consent.
Neither employers, nor insurance companies, may require genetic testing.
Individuals are granted both the right to know – to be informed of the results of any genetic test about themselves – and the right to choose to live in ignorance of what a genetic test may predict about their future.
To discriminate against or stigmatize anyone on account of their genetic characteristics is prohibited.
Desirable as these provisions seem, they could impose a heavy cost on German companies.   If insurance companies outside Germany are permitted to require genetic tests while German companies are prohibited from doing so, then people who know they have life-shortening genetic diagnoses will get their life insurance from German insurance companies.
These companies will then find themselves making more payments for premature deaths relative to their competitors.
To cover the increased costs, they will have to raise premiums, making themselves uncompetitive.
In an attempt to mitigate this problem, the law specifies that anyone taking out an insurance policy valued at more than €300,000 may be required to disclose the results of prior genetic tests.
But if people lie about whether they have previously been tested, that provision will be moot.
As genetic testing becomes increasingly able to predict not only health, but also some cognitive and personality traits, the prohibition on employer testing may also put German employers at a disadvantage in the international marketplace. They will invest resources in training employees whom their competitors will exclude from the initial pool of recruits.
This may be a humane thing to do, for it gives every individual a chance, irrespective of the genetic odds against their paying their way for the company.
But, in the long term, if we are serious about prohibiting such tests, we need an international agreement – on both insurance and employment – to ensure a level playing field for all countries.
That will not be easy to achieve in a globally competitive economy in which some nations have demonstrably less respect for individual human rights than others.
The German law’s most controversial feature is a prohibition on prenatal genetic testing for diseases that will manifest themselves only in adulthood.
Consider, for example, a woman who knows that her partner has the gene for Huntington’s disease. Any child of his has a 50% chance of inheriting the condition.
If the child does inherit it, he or she will, at around 40, begin to suffer uncontrollable movements, personality changes, and a slow deterioration of cognitive capacities.
People with Huntington’s disease usually die about 15 years after the onset of the symptoms. There is no cure.
No parents could want this future for their children.
Prenatal testing of fetuses (or of in vitro embryos before transfer to the uterus) is now well established throughout the developed world.
If the test is positive, the pregnancy can be terminated, or, in the case of pre-implantation diagnosis, the embryo will be discarded.
The new German law makes such tests a crime. The same is true of tests for the genes that strongly predispose women to breast cancer.
As genetics advances, more such late-onset conditions will become detectable prenatally.
What could be the thinking behind such a law?
One might take the view that 40 years of life before the onset of Huntington’s disease or breast cancer is better than no life at all.
But if we take that into account, should we not also take into consideration the life of the child who the parents would have had, if they had been able to use prenatal diagnosis and be sure of having a child who does not carry the gene for the disease?
Surely that child has better life prospects. When we have a choice between lives with such different prospects – and can make the choice before the embryo or fetus has any awareness at all – shouldn’t we be able to choose the child with the better prospects?
It is not surprising that questions about genetic tests should receive special attention in Germany, given the national imperative of avoiding any repetition of the crimes of the Nazi era.
But, in their laudable desire to distance themselves as much as possible from those atrocities, Germany’s legislators have enacted a law that makes it a crime to use modern science to avoid undoubted human tragedies.
That is an absurd outcome. The pendulum has swung too far.
Self and the City
BEIJING – What is the big story of our age?
It depends on the day, but if we count by centuries, then surely humanity’s urbanization is a strong contender.
Today, more than half of the world’s population lives in cities, compared to less than 3% in 1800.
By 2025, China alone is expected to have 15 “mega-cities,” each with a population of at least 25 million.
Are social critics right to worry about the atomized loneliness of big-city life?
True, cities cannot provide the rich sense of community that often characterizes villages and small towns. But a different form of community evolves in cities.
People often take pride in their cities, and seek to nourish their distinctive civic cultures.
Pride in one’s city has a long history.
In the ancient world, Athenians identified with their city’s democratic ethos, while Spartans prided themselves on their city’s reputation for military discipline and strength.
Of course, today’s urban areas are huge, diverse, and pluralistic, so it may seem strange to say that a modern city has an ethos that informs its residents’ collective life.
Yet the differences between, say, Beijing and Jerusalem, suggest that cities do have such an ethos.
Both are designed with a core surrounded by concentric circles, but Jerusalem’s core expresses spiritual values, while Beijing’s represents political power.
And a city’s ethos shapes more than its leaders.
Beijing attracts China’s leading political critics, while Jerusalem’s social critics argue for an interpretation of religion that holds people, rather than inanimate objects, sacred.
In both cases, despite objections to the ruling ideology’s specific tenets, few reject the ethos itself.
Or consider Montreal, whose residents must navigate the city’s tricky linguistic politics.
Montreal is a relatively successful example of a city in which Anglophones and Francophones both feel at home, but language debates nonetheless dominate the political scene – and structure an ethos for the city’s residents.
Hong Kong is a special case, where the capitalist way of life is so central that it is enshrined in the constitution (the Basic Law).
Yet Hong Kong-style capitalism is not founded simply on the pursuit of material gain. It is underpinned by a Confucian ethic that prioritizes caring for others over self-interest, which helps to explain why Hong Kong has the highest rate of charitable giving in East Asia.
Paris, on the other hand, has a romantic ethos.
But Parisians reject Hollywood’s banal concept of love as a story that ends happily ever after.
Their idea of romance centers on its opposition to staid values and predictability of bourgeois life.
In fact, many cities have distinctive identities of which their residents are proud.
Urban pride – what we call “civicism” – is a key feature of our identities today.
This matters in part because cities with a clear ethos can better resist globalization’s homogenizing tendencies.
It is worrying when countries proclaim their timeless and organic ideals, but affirming a city’s particularity can be a sign of health.
Chinese cities seek to counter uniformity via campaigns to recover their unique “spirit.”
Harbin, for example, prides itself on its history of tolerance and openness to foreigners.
Elsewhere, Tel Aviv’s official Web site celebrates, among other attractions, the city’s progressive role as a world center for the gay community.
Urban pride can also prevent extreme nationalism.
Most people need a communal identity, but it may well be better to find it in one’s attachment to a city than in attachment to a country that is armed and willing to engage in conflict with enemies.
Individuals who have a strong sense of civicism can make decisions based on more than mere patriotism when it comes to national commitments.
Cities with a strong ethos can also accomplish political goals that are difficult to achieve at the national level.
China, the United States, and even Canada may take years to implement serious plans to address climate change.
Yet cities like Hangzhou, Portland, and Vancouver take pride in their “green” ethos, and go far beyond national requirements in terms of environmental protection.
Urbanization is blamed for a wide variety of modern social ills, ranging from crime and incivility to alienation and anomie.
But, by infusing us with their unique spirit and identity, our cities may, in fact, help to empower humanity to face the most difficult challenges of the twenty-first century.
Self-Financing Development
NEW YORK – A remarkable feature of the international financial system in the last decade has been the rapid and vast accumulation of foreign-exchange reserves by developing countries.
World foreign reserves tripled from $2.1 trillion in December 2001 to an unprecedented $6.5 trillion in early 2008, according to IMF data.
Developing countries as a whole accounted for more than 80% of global reserve accumulation during this period, and their current level of reserves approaches $5 trillion.
Half of this volume is concentrated in developing Asia, but Latin America and Africa have also been amassing international assets at a remarkable pace.
This pool of reserves surpasses developing countries’ immediate liquidity needs, leading to their increased creation and expansion of sovereign wealth funds, which have an additional level of assets of more than $3 trillion.
The unprecedented increase in developing countries’ foreign exchange reserves is due both to their current-account surpluses and large net capital inflows.
Practically all developing countries’ reserves are invested in developed countries’ assets, leading to an increasing net transfer of resources from the developing to the developed world, which, according to UNDESA estimates, reached $720 billion in 2007 alone.
Although economic growth and poverty reduction in many developing countries has been impressive in recent years, a significant increase in investment in areas such as infrastructure is required to sustain such growth in the future.
We propose that a very small portion of developing countries’ total foreign-exchange reserves – say, 1% – be channeled to the expansion of existing regional development banks or the creation of new ones that would invest in infrastructure and other crucial sectors.
Indeed, infrastructure investment is recognized as a key ingredient in sustaining and accelerating growth.
However, there is a large financing gap.
According to the World Bank, developing countries spend an average of 3-4 % of GDP on infrastructure every year, compared to an estimated 7% of GDP required to meet existing infrastructure needs for maintaining rapid growth.
This translates into an annual gap of at least $300 billion at current prices.
High expectations for private-sector financing of infrastructure have gone largely unmet.
Private investment remains limited and concentrated by both country and sector.
National governments still account for the large majority of financing.
Official development assistance and multilateral bank lending, though valuable, remain insufficient.
In particular, there are large gaps in the provision of crucial regional and cross-border investments, for example in energy and roads.
Multilateral financial institutions must maintain their central function in the international development architecture, and in particular in financing infrastructure investment.
But regional and sub-regional financial institutions owned by developing countries can and should play an important and valuable complementary role.
These institutions give a greater voice and sense of ownership to developing countries, are more likely to rely on moral suasion rather than conditionality, and tend to benefit from smaller information asymmetries.
Moreover, regional and sub-regional development banks are particularly suited to provide regional public goods.
The growing importance of trade integration and regional trade flows makes the provision of regional infrastructure urgent.
The European experience offers valuable lessons in this regard.
Trade integration was initially supported by massive investments in regional infrastructure, financed to an important extent by a large, specifically created institution, the European Investment Bank.
If developing countries allocate only 1% of their foreign exchange reserves to the paid-in capital of regional and sub-regional institutions, this would amount to $50 billion at current levels of reserves.
Assuming a ratio of loans-to-capital of 2.4 times – an estimate based on the ratio of the successful and financially sound Andean Development Corporation – the expanded regional and sub-regional development banks or new ones could generate additional lending of approximately $120 billion.
With time, they could leverage retained earnings, increasing their lending potential without additional paid-in capital.
This would imply the ability to finance an important proportion of unmet needs for infrastructure financing.
Based on these initial calculations, the additional lending capacity generated would be significantly larger than total disbursements currently made by existing multilateral development banks.
Obviously, more detailed calculations and analyses are required, along with discussions with governments, existing institutions, rating agencies, and other stakeholders.
By expanding or creating new regional and sub-regional financial institutions, developing countries could lay the basis for their own current and future lending capacity, which would eventually help them meet their development goals.
Given their large foreign-exchange reserves, we believe the time to begin such an initiative is now.
Mine Your Digital Business
BOSTON – Nearly everyone has a digital footprint – the trail of so-called “passive data” that is produced when you engage in any online interaction, such as with branded content on social media, or perform any digital transaction, like purchasing something with a credit card.
A few seconds ago, you may have generated passive data by clicking on a link to read this article.
Passive data, as the name suggests, are not generated consciously; they are by-products of our everyday technological existence.
As a result, this information – and its intrinsic monetary value – often goes unnoticed by Internet users.
But the potential of passive data is not lost on companies.
They recognize that such information, like a raw material, can be mined and used in many different ways.
For example, by analyzing users’ browser history, firms can predict what kinds of advertisements they might respond to or what kinds of products they are likely to purchase.
Even health-care organizations are getting in on the action, using a community’s purchasing patterns to predict, say, an influenza outbreak.
Indeed, an entire industry of businesses – which operate rather euphemistically as “data-management platforms” – now captures individual users’ passive data and extracts hundreds of billions of dollars from it.
According to the Data-Driven Marketing Institute, the data-mining industry generated $156 billion in revenue in 2012 – roughly $60 for each of the world’s 2.5 billion Internet users.
As impressive as this figure sounds, it is just the first step for the data economy.
By 2020, the global Internet population will reach five billion; ten billion new machine-to-machine connections will be created; and mobile data traffic will rise 11-fold.
Given the dramatic growth in the amount of data being generated, together with ever-expanding applications across industries, it is reasonable to expect that individual data will soon be worth more than $100 per Internet user.
Within ten years, the data-capture industry can be expected to generate more than $500 billion annually.
Based on these projections, one might wonder what kind of compensation the creators of this multibillion-dollar data can expect.
As it stands, the answer is none at all.
Individual users are at the bottom of a broken economy.
The value that their data generate is being collected by third parties, and sold to whatever cash-rich organization is willing to purchase it.
This does not have to be the case.
The first step toward reclaiming some of the value of our own data is to view this information as an asset, rather than as a by-product.
At that point, Internet users can find ways to take control of their own creation.
Already, Facebook users can export all of their personal data as a zip file simply by clicking a link on their profiles.
Presumably, they could sell that information directly to the organizations that want it, instead of allowing Facebook to do so.
Of course, the data market does not yet exist on this scale.
But, as Facebook’s data-export facility demonstrates, a new model that turns data into an asset and consumers into producers is not a distant prospect.
Such a model would empower billions of Internet users by making them beneficiaries of a transactional exchange – one that adds value in every direction.
Beyond enabling individual Internet users to monetize their data, this model would benefit data buyers by connecting them more closely to consumers – not least by diminishing the mistrust that can arise when users are not complicit in the sharing and use of their data.
Indeed, firms that acknowledge that personal data are personal property will be in a better position to build relationships with individual consumers, thereby gaining deeper insights into their specific needs and desires.
If passive data are worth hundreds of billions of dollars when sold by third parties, the data that individuals choose to share – reliable, honest insights into their motivations as consumers – should be worth much more.
By recognizing the individuals behind the data, companies can access and share in that value, within a fully inclusive data economy.
Personal data is exactly that – personal.
People should choose whether to share it, and they should be able to share it on their own terms.
Send in the Clowns
New York &#45;&#45; Beppo Grillo is one of Italy’s most famous comics. He is also one of Italy’s most influential political commentators.
His blog attracts 160,000 hits daily, and if he could run for prime minister (he can’t, because of a criminal record), more than half of Italy’s voters, according to a poll last year, would have considered voting for him.
Grillo is yet another reminder of a modern phenomenon: the important role of comedians in contemporary politics.
Until a few years ago, the one TV program most Mexicans turned to for political information was called The Morning Quickie, broadcast from 6-10 a.m.
The host, interviewer, and main commentator was Victor Trujillo, better known as Brozo the Clown, adorned with a green wig and a red rubber nose.
It was Brozo the Clown who exposed a major corruption scandal in the office of a former Mexico City mayor.
While staid TV pundits ask the usually vapid questions during presidential debates in the United States, candidates know that the really important thing is to get laughs on the comedy shows of David Letterman or Jay Leno.
And, for several years, American liberals have looked to Jon Stewart, another comic talent, for critical political commentary.
Of course, comic entertainment in politics is not just a modern phenomenon.
Nero was a murderer who understood that he had to amuse the masses to gain popular support.
Then there is the long tradition of the court jester with license to criticize the despot by sweetening his barbs with jokes.
The annual Gridiron Club Dinner in Washington, where the president is lampooned by the press, is a relic of this custom.
In the US, especially, the borderlines between showbiz and politics (or indeed religion) have always been porous.
The similarities between the variety show, the evangelical meeting, and the party convention are striking.
Europeans like to sneer at American political jamborees as being hopelessly vulgar.
In fact, democracy demands a degree of showmanship and pizzazz; politicians need to appeal to the mass of voters, and not just to an elite, which can afford to ignore hoi polloi.
To be utterly boring, holding forth for hours on end, regardless of entertainment value, is the privilege of autocrats.
Only communist rulers could force millions of people to buy their complete works, filled with wooden ideas written in turgid prose.
The problem with many democratic politicians today is that they have become almost as dull as the old communist autocrats.
Most, especially in Europe, are professional politicians with no experience apart from working the levers of party machines.
Gone, for the most part, are the colorful rogues and public-spirited idealists who used to liven up parliamentary politics.
Like bureaucrats, professional politicians have mastered the art of saying nothing interesting in public.
They are managed by equally professional press handlers, masters of spin and the television sound bite.
In these dying days of serious newspaper journalism, slick television shows, packaged by highly-paid anchormen – who never utter an original thought themselves, and would never expect a politician to do so – are the only venues where professional politicians feel secure enough to “face” the public. As a result, the public is turned off.
Not since the 1930’s has popular disgust with politicians in Europe, the US, as well as Japan, run so high.
This is dangerous, because such sentiments can end in disgust with liberal democracy itself.
Does the future belong, then, to the clowns, the anarchic blogosphere, the anti-politicians, and the populist showmen who entertain the masses with jokes, slurs, and indiscretions on TV channels, which some of them actually own?
If the success of a TV pundit with a red rubber nose is a rebuke to the dull and fawning anchormen, the political success in recent years of entertainers, demagogues, and public figures who make a virtue of their indiscretion is a slap in the face of the professional political class which they profess to despise.
The recent re-election of the great showman Silvio Berlusconi illustrates this perfectly.
Although none of the aspiring candidates for the US presidency can match him in terms of zaniness, similar trends are plain to see.
John McCain managed to defeat his more conventional Republican rivals by seeming to be totally different from them: a maverick who says what he damned well wants, a tough guy with the knowing wink of the old ladies’ man.
Barack Obama, at least when he began his campaign, had all the charisma of the holy roller, turning on the crowds with the rhetorical spark of a great evangelist.
That is why he barged his way past Hillary Clinton, the consummate operator of the party machine.
In some ways, Obama’s candidacy illustrates the problems facing our democracies today.
People don’t trust the professionals.
But electing a clown is not the answer either.
Obama happily combines showmanship and seriousness in a way that could inject new life into the democratic system.
But he has been maneuvered into a peculiar dilemma.
Attacked for being shallow, indiscreet, and flashy by the Hillary camp, he has toned down his revivalist rhetoric, and adopted a more sober, more cautious, more professional air.
Yet, by doing so, he may have made himself less popular, and is being accused of elitism to boot.
Here is one case where a bit more vulgar showbiz may be exactly what democracy requires.
East Asia’s Patriots and Populists
TOKYO – When faced with domestic worries, politicians often resort to foreign diversions – a simple axiom that is highly useful in assessing the increasingly tense sovereignty disputes in the East and South China Seas.
Although China is involved in the most wide-ranging and intense disputes, the most tragic is that between South Korea and Japan, given that both countries are democracies with almost identical strategic interests.
On August 10, South Korean President Lee Myung-bak visited the island of Takeshima (called Dokdo in Korean), which has been the subject of a territorial dispute between Japan and South Korea for 60 years.
During a lecture at the Korea National University of Education four days later, he stoked tensions further, saying of the Emperor of Japan’s proposed visit: “If he wants to come, he should apologize first for the past.”
Despite his numerous achievements as president, Lee is trumpeting his nationalist/anti-Japanese credentials in the waning days of his term, which ends in February 2013.
Indeed, so strident has he become that he refused to accept a message from Japan’s prime minister about his island visit.
Lee’s hyper-patriotism is new.
Less than two months ago, he reached an agreement to share military intelligence with Japan – a deal that was subsequently abandoned, owing to fears that the opposition would attack his party’s presidential candidate as subservient to Japan.
Lee’s recent behavior may also reflect his fear that he could suffer a fate similar to that of past South Korean presidents.
Some have been assassinated, one committed suicide, and others were arrested and condemned to death after stepping down.
Lee may have interpreted his brother’s arrest in July for accepting bribes as a prelude to such a fate.
Attempting to mitigate future domestic political damage by undermining the dynamics of the relationship between South Korea and Japan – and both countries’ relationship with the United States – is unwise.
Given North Korea’s continued potential for military mischief and the fluid state of security in Asia in the wake of China’s rise, such tactics could have serious, if unintended, consequences.
The origins of the dispute over Takeshima lie in the period immediately before the Treaty of San Francisco was signed in 1951, formally ending World War II in the Pacific.
The treaty demarcated territory, including Takeshima.
But South Korea’s then-president, Syngman Rhee, in violation of the treaty and international law, instituted the “Syngman Rhee Line” to demarcate an expansive area, including Takeshima, within which South Korea unilaterally claimed fishery jurisdiction.
Since then, South Korea has used the issue as a means of boosting national prestige – and, aware that its sovereignty claims are legally dubious, has refused to allow the International Court of Justice to adjudicate.
More ominous, however, is the sovereignty dispute between Japan and China.
Here, history has a story to tell as well.
Japan’s government officially incorporated the Senkaku Islands into Japanese national territory in 1895.
Since then, the islands have consistently been held to be Japanese.
Indeed, at one point there was a dried bonito factory in operation, and more than 200 residents on Uotsuri, the largest of the islands (roughly the size of New York City’s Central Park).
At WWII’s end, in accordance with Article 3 of the Treaty of San Francisco, the islands were placed under United States control, but reverted to Japan in 1972, as part of the agreement that returned to it administration of Okinawa.
Until this point, neither China nor Taiwan expressed objections.
In the Chinese World Atlas published under Mao Zedong in 1960, the Senkaku Islands were treated as part of Okinawa.
And, although circumstances changed in 1968, when a survey by the United Nations Economic and Social Commission for Asia and Pacific revealed the seas around the islands to contain an abundance of resources, the periodic tensions that arose were manageable.
That dynamic was altered when the Democratic Party of Japan came to power three years ago.
The DPJ’s feckless dithering over whether to renew the US Marines’ lease on a base on Okinawa signaled to the world – and to China, in particular – that the party did not value the US alliance and America’s security guarantee as highly as previous governments did.
As a result, China has since been testing Japan’s resolve and America’s assurances, though US Secretary of State Hillary Clinton’s recent and resolute affirmation of her country’s commitment to Japan’s security should put an end to any suspicion in that regard.
Meanwhile, domestic tension in China – particularly the scandal surrounding the purge of former Chongqing Communist Party boss Bo Xilai and the country’s economic slowdown – has probably prompted the government to play the nationalist card more forcefully than usual.
The Party’s upcoming Congress to anoint the country’s new leadership for the next decade adds to its desire to manipulate public emotion.
But the nationalist genie, once released, is not easily controlled.
Some anti-Japanese demonstrations – which featured rioting, looting, and the destruction of Japanese businesses – mutated into anti-government protests.
By allowing social tensions to mount to such a degree, the Chinese government may also be partly responsible for the recent rampage among thousands of workers at the Foxconn plant (where components for Apple iPods and iPads are made) in Taiyuan.
This December, South Korea will elect a new president. Japan is likely to hold fresh elections soon as well.
The governments that emerge should use their popular mandates to forge a new form of cooperation that can transcend an embittered past.
What France and Germany achieved in the 1950’s can serve as an example.
By forging shared sovereignty over issues vital to national security – namely, coal and steel – visionary leaders in both countries laid the foundation for European peace and security, while overcoming a long history of antagonism.
In the face of China’s rise and maritime ambitions, East Asia’s two great democracies must seek to do no less.
If they succeed, South Korea and Japan would establish a precedent that offers the best path to resolving the great sovereignty questions that are now destabilizing Asia.
Sense and Nonsense about Disproportionate Force
As the war in Lebanon continues, the term “disproportionate force” is being bandied about as if some crystal clear principle of international law lay behind it, telling us when force is disproportionate and why it is illegal.
But civilian deaths as a result of military combat are not enough to say that “disproportionate force” has been used.
Nor has that standard, whatever it is, been met if more children die on one side than the other.
So what, then, does “disproportionate force” mean, and what is its place in the law of war?
Let’s go back to basics.
In the domestic law of self‑defense, the use of force must always be both necessary and proportionate to the interest being protected.
A good example is whether a storeowner may shoot looters who are escaping with his goods.
If there is no other way to stop the thieves, the use of force is necessary.
But is it proportional?
That depends on whether the cost to the looters of being shot so clearly outweighs the value of the stolen goods that the storeowner should do nothing, at least at the moment.
He always has recourse to the police and the possibility that they might recover the goods.
In other words, force becomes disproportionate when the costs of using it are too high.
However, this does not mean that force becomes “disproportionate” simply when the costs outweigh the benefits.
After all, a woman may use deadly force to avoid being raped, even though the life of the aggressor, one would think, is worth more than the sexual integrity of the potential victim.
Anytime physical harm is threatened, it seems that the use of all necessary force is permissible.
Suppose a terrorist is pulling out his victim’s teeth one by one, and that the only way to stop him is to kill him.
Most people would say that doing so is permissible, even though the harm to the aggressor is much greater than the value of the victim’s teeth.
How, then, do we know when force is disproportionate?
This is a matter of constant debate.
Many legal systems now take the position that, with regard to property offenses at least, the victim must be willing to surrender his property when the only available option is to kill the thief.
Let us try to apply these principles to international conflict.
Two distinct realms of self‑defense exist within the law of war.
One is the justification for going to war in the first place; the second is the use of force in combat in the course of war, such as killing civilians who are attacking soldiers.
In the justification of armed conflict, everyone agrees that the defending state may use all necessary force to repel an aggressor.
When Argentina invaded and occupied the Falkland Islands, the United Kingdom could take whatever measures were necessary to remove them.
But let us suppose that the British bombed Buenos Aires.
For the use of force to be necessary, it must yield a direct military advantage that contributes to thwarting the aggression.
It would not be acceptable to argue that to bomb a city on the mainland was necessary in order to compel the Argentine population to put pressure on the military junta to withdraw from the islands.
So bombing Buenos Aires in that context would have been unnecessary and therefore could not possibly qualify as proportionate.
International lawyers commonly confuse the concepts of necessity and proportionality, claiming that both apply in wars of self‑defense.
But this is not as clear as it is in domestic criminal law.
I know of no case in the international version of shooting escaping looters where a court has affirmed that the use of force was necessary but not proportionate.
There are two reasons why international law is tolerant of the use of all, or nearly all, necessary force.
First, in international conflicts, the defending army must always protect the lives of its citizens, not just property interests.
Second, with rare exceptions, there is no international police force that can assist a defending nation.
The problem of disproportionate force has different contours on the battlefield, when soldiers are already at war.
Among all the dozens of war crimes in international law, none mentions the element of disproportionate force.
The closest adjective used in the Rome Statute is “clearly excessive” force.
Intentional targeting of civilians is prohibited, as is an attack on military targets in the knowledge that there will be “clearly excessive” harm relative to the value of the military target.
As one would expect, there is no criterion, not even a theory, for determining when the use of force carries an excessive cost to civilian bystanders.
The specific problem in Lebanon, particularly Israel’s bombing of southern Beirut, is deciding when the targets are protected civilians or part of a militant terrorist organization and thus subject to legitimate attack.
We have too little information about many of these attacks to know one way or the other.
This is an area where the concepts are clear but the facts remain murky.
Journalists on the ground would do well to inquire about the relevant military objectives rather than pass judgment on the basis of the numbers of civilians killed, which may have minimal relevance to legal analysis.
Separatism, Italian-Style
ROME – Many separatist movements in Europe have resorted to various violent terrorist acts since the second half of the twentieth century.
From the 1960’s onwards, bombs and death were the order of the day in regions like Northern Ireland, Corsica (France), South Tyrol (Italy), and the Basque country (Spain).
Indeed, the specter of violent separatism has reared its head again in Spain.
The Basque terrorist organization ETA has ended its truce with the Spanish government, and, on the occasion of the 50th anniversary of its founding, placed bombs in the town of Burgos and on the island of Majorca.
Fortunately, elsewhere in Europe, reason seems to prevail nowadays and the resort to violence has been curtailed.
But that does not mean the end of separatism.
Italy, for example, is under constant threat of cultural and economic separatism, albeit in a peaceful way.
Silvio Berlusconi's ally in government, the Lega Nord (Northern League), is continuously conjuring up schemes to embarrass the national government with threats to the concept of national unity.
The Lega Nord, led by the charismatic Umberto Bossi, holds the decisive votes in Parliament to keep the Berlusconi government afloat.
It uses this power to blackmail the government into introducing measures which discriminate between citizens of Italy’s north and south.
Exponents of the Northern League, such as MEP Mario Borghezio, MP Roberto Cota, and Senator Federico Bricolo, are well known for their xenophobic statements in Parliament, particularly against those from non-EU countries.
But their chauvinism does not stop there: they constantly propose measures to discriminate between northern Italian citizens from Veneto or Lombardy and southerners from Naples, Calabria, or Sicily.
During this summer of “separatist” folly, the first proposal concerned the appointment of headmasters of schools in the Veneto region: the local councilors in the province of Vicenza approved a measure to reserve all headmaster posts in the province for northern Italian teachers.
At the end of July, another Lega Nord MP, Paola Goisis, proposed in Parliament that teachers from Italy’s south should not be allowed to teach in northern schools unless they are well versed in the history, traditions, and dialects of the area where the school is located.
The Italian Minister for Education, Mariastella Gelmini, agreed to discuss the proposal.
The third attack on Italian unity launched at the beginning of August by the President of the Lega Nord group in the Senate, Federico Bricolo, who proposed adding a proviso to Article 12 of the Italian Constitution whereby the flags and anthems of the different regions would be officially recognized on an equal footing with the national anthem and flag.
The Lega Nord’s latest provocation is its proposal that state employees receive different salaries for the same job, depending on whether they live in the north or the south.
Agriculture Minister Luca Zaia has gone so far as to argue that relating salaries to the cost of living in different regions will force the south to be self-sufficient and stop relying on help from the north.
This proposal seems to have found a certain degree of support from Berlusconi, though he would likely be the only one in the country to agree with it.
The trade unions, employers’ associations, all opposition political parties, and even many of Berlusconi’s own MPs have united against what they consider to be an aberration.
Zaia’s vivid imagination does not stop there: he has now proposed that popular Italian TV series be dubbed or subtitled in the local dialect!
And Berlusconi, consumed as he is with defending his difficult position as the main protagonist in a seemingly endless soap opera of sex scandals, is not capable of reining in such blatant provocations on the part of the Lega Nord.
The result is that Berlusconi’s supposed allies such as former Minister Gianfranco Micciche and current Governor of Sicily Raffaele Lombardo, are seriously thinking of setting up a “Party of the South” to ensure that Italy’s southern regions have enough clout to withstand the Lega Nord’s onslaught.
Whether such plans materialize remains to be seen. What seems certain is that, unlike in Spain, Italy’s separatist movements are gaining ground through a bloodless revolution.
Mao was wrong: political blackmail seems to be a more effective tool than the barrel of a gun.
Serious Harm By Research
The media, legislators, and other organizations often raise concerns about human-rights violations and ethical breaches in clinical research.
Such cases are legion.
Human subjects are allowed to be bitten by 100 mosquitoes that could carry malaria in a study in Brazil.
A French doctor performs a face transplant without the benefit of the usual research that would accompany such a high-risk procedure.
An 18-year-old subject in a gene therapy trial dies, with numerous lapses in the clinical trial noted after his death.
Just months ago, a clinical trial in London that saw the first use of a drug in humans resulted in six healthy subjects becoming violently ill.
Two of the volunteers nearly died.
The volunteers were paid several thousands of dollars to participate in the trial.
The general public is aware of clinical research trials, but there is little awareness that the number of subjects enrolled in research is much larger than the numbers enrolled in clinical trials alone.
While some research-related deaths reach the popular media, the actual number of deaths in research is higher.
Even regulators and industry experts are not aware of the true numbers of deaths and adverse events, due to the lack of proper reporting.
The large numbers of human subjects in research are an outgrowth of the legitimate need for more and better health care throughout our ever-lengthening lifespans.
No single entity keeps track of the real extent of the abuse of all human subjects engaged in research either in the United States or worldwide.
I estimate that in the US alone, there are more than twenty million subjects, half of whom represent drug trials.
Worldwide, this number could exceed fifty million, with about half that number in drug studies.
These staggering numbers present an awesome responsibility for our citizens and our governments alike.
The immediate ethical issue facing us is the real harm inflicted on unsuspecting subjects through a vast array of indignities, adverse events, injuries, and death.
Many of those who are harmed are poor, uneducated, and politically powerless.
Some are impaired in their ability to give informed consent due to mental or cognitive disabilities, or are exposed to coercion, improper monitoring, and pervasive conflicts of interest.
Moreover, the justice of the distribution of risks and benefits is questionable when research subjects are concentrated on a vulnerable segment of our society.
Bloomberg Market magazine recently published an extensive article on the unethical practices of the largest contract research organization conducting some clinical trials in Florida.
Trial subjects drawn from illegal Latin American immigrants were threatened with reporting of their illegal status to the US Department of Homeland Security if they complained about the risks of the drug that they received.
That is only one example.
The system of protections for human subjects worldwide is either non-existent or broken.
In Europe since May 2004, all clinical trials in the 25 nations of the EU must conform to the European Directive (ED) issued in 2001 at the behest of pharmaceutical industry.
Each country’s regulatory body will have to issue their own regulations that are within the bound the Directive.
The Directive brings the protections for human subjects in European clinical trials in line with those in the US.
But in both the US and Europe, the system has serious shortcomings and gaps.
For example, there are highly de-centralized ethics committees with varied qualities to protect human subjects.
The EU directive does not regulate social and behavioral research while the US. does not regulate research that is privately funded and not related to drug licensing.
Further down the chain of supervision, research institutions are designated to “manage” their own conflicts of interests as well as those of their investigators.
Yet many institutions and investigators have a financial stake in the clinical trials they conduct.
Most investigators have little or no training in ethics and regulatory compliance, for which there is no mandatory education.
Advocates in the US, such as the ten-year-old human rights organization, Citizens for Responsible Care and Research (www.circare.org), which I co-founded, have proposed a universal National Human Subjects Protection Act.
However, there is no likelihood of passage anytime soon.
Opposition to serious reforms to protect human subjects in research comes from two groups: the pharmaceutical industry and universities.
The objections of industry rest on the narrow issue of added financial cost.
The objections coming from universities are more puzzling, as it is in their long-term interest to conduct research ethically.
In both cases, the effective protection of human subjects should receive much higher priority, thereby justifying the modest added expense, which probably would amount to no more than 1-2% of the overall cost of clinical studies.
If such research is truly to serve the public good, the safety, health, and dignity of human subjects should not be compromised.
Serious Negotiations or Hot Confrontation with Iran?
Berlin – For two weeks, it looked like the regime in Iran had finally gotten the message that, if it continues to pursue its nuclear program, serious military confrontation is likely to result.
Indeed, there were interesting – and previously unheard of – statements and signals from Teheran that suggested an increased willingness to start negotiating about Iran’s nuclear program and regional security issues.
And America’s decision to send Undersecretary of State William Burns to a meeting with Iran’s top nuclear negotiator suggests that those signals are being taken seriously.
But the recent military muscle-flexing with rocket tests and the rejection of a compromise by Iranian President Mahmoud Ahmedinejad and his foreign minister show that the country’s leadership is seriously divided over the strategic line that Iran should pursue.
Iran’s leadership still harbors the misconception that Israeli threats against its nuclear facilities are an expression of the domestic difficulties of Prime Minister Ehud Olmert’s government. This is plainly wrong.
Olmert’s government has serious problems, but they are not the reason that the situation between Israel and Iran is coming to a head.
On the contrary, a cross-party consensus exists in Israel concerning Iran’s possible nuclear armament and regional hegemony.
All sides agree that, unless there is a diplomatic solution, Iran’s possession of nuclear weapons must be prevented in good time and by any means necessary.
Moreover, Saudi Arabia and some other Arab countries share this view, albeit behind closed doors.
If Iran adopts a more realistic approach, there is real hope for a diplomatic solution.
The most recent offer of the 5+1 group (the United Nations Security Council’s five permanent members and Germany) was well received in Teheran.
In addition to far-reaching political and economic cooperation, the offer promises cooperation on nuclear matters, including the construction and supply of the newest light-water reactors in Iran, as well as Iranian access to nuclear research and development – provided that there is a negotiated settlement.
But what was really new is that Iran also responded positively to the procedure proposed by the 5+1 group.
In the pre-negotiation phase, this means that Iran would agree not to install any new centrifuges, necessary for increasing the volume of uranium being enriched, while the 5+1 group would refrain from calling for new sanctions in the Security Council.
Once negotiations start, Iran would, under the supervision of the International Atomic Energy Agency, suspend its uranium enrichment and all related activities for six months – something that the Iranian government refused even to discuss in the last four years.
For its part, the Security Council would suspend all its deliberations related to the Iranian nuclear program.
The aim of these negotiations is a comprehensive agreement between Iran and the 5+1 group that would resolve both the nuclear conflict and address regional security issues (Iraq, the Israeli-Palestinian conflict, Lebanon, Persian Gulf, Afghanistan) while opening up extensive international and regional cooperation.
There were also signals emanating from Iran that, having , having mastered uranium enrichment technology, the authorities could envisage continuing enrichment in a third country in a joint consortium with the West.
A similar proposal made by Russia was unceremoniously dismissed only a short time ago.
Moreover, while there is no willingness to accept Israeli hegemony, the tone vis-à-vis Israel is beginning to change.
Ahmadinejad’s vile anti-Semitism has of late been indirectly, but fairly openly, criticized by one of the closest confidants of Iran’s supreme religious leader, former Foreign Minister Ali Akbar Velayati.
Iranian spokesmen also indicate that there is an official awareness of Israel’s importance for a comprehensive regional solution, and that doing business with Israel is no longer inconceivable.
One hasn’t heard this sort of language in all the years since Ahmadinejad came to power.
But is Iran really serious?
Or are these simply the old stonewalling tactics?
Does the government just want to buy time once more, now to make it through the America’s presidential elections?
Given the public contradictions in the Iranian leadership’s behavior, is there a reliable Iranian position, and, if so, who represents it?
These crucial questions can only be answered by the practical test of negotiations, this time with direct American participation.
If Iran is serious, the result may be nothing less than the long awaited “Grand Bargain” – a regional reconciliation of interests between Iran, on one side, and America, Europe, and the region’s US allies on the other.
But if Iran is playing for time, its behavior is shortsighted and foolish.
The conflict, and thus the danger of a military confrontation, will not disappear under a new US administration.
On the contrary, should the negotiations fail, the confrontation will resurface in short order, and be far more dangerous.
Neither John McCain nor Barack Obama will take a softer stance than the current American administration on the question of Iran’s regional hegemony and its nuclear program.
Indeed, if a diplomatic solution fails, they are more likely to react more forcefully.
It is therefore imperative that diplomacy, which looks possible at the moment, be given a chance.
If the top leadership in Teheran has realized that it makes a great deal more sense, and is more in keeping with Iran’s interests, to consolidate the successes of its foreign policy in the last few years, and to strengthen the regime, rather than risk everything in a military showdown with unforeseeable consequences, then there is a real chance for a diplomatic solution.
If not, the region will be plunged into a hot confrontation.
There’s wisdom, which Iran’s leaders should heed, in the old adage that a bird in the hand is worth two in the bush.
Services without Tears
NEW YORK – A famous claim in economics is that the cost of services (such as health care and education) tends to increase relative to the cost of goods (such as food, oil, and machinery).
This seems right: people around the world can barely afford the rising health-care and school-tuition costs they currently face – costs that seem to increase each year faster than overall inflation.
But a sharp decline in the costs of health care, education, and other services is now possible, thanks to the ongoing information and communications technology (ICT) revolution.
The cost of services compared to the cost of goods depends on productivity.
If farmers become much better at growing food while teachers become little better at teaching kids, the cost of food will tend to fall relative to the cost of education.
Moreover, the proportion of the population engaged in farming will tend to fall, since fewer farmers are needed to feed the entire country.
This is the long-term pattern that we’ve seen: the share of the workforce in goods production has declined over time, while the cost of goods has fallen relative to that of services.
In the United States, around 4% of the population in 1950 was employed in agriculture, 38% in industry (including mining, construction, and manufacturing), and 58% in services.
By 2010, the proportions were roughly 2%, 17%, and 81%, respectively.
In the meantime, health-care and tuition costs have soared, along with the costs of many other services.
But a productivity revolution in service-sector delivery is now possible.
As a professor, I feel it in my own classroom.
Ever since I began teaching 30 years ago, it had seemed that the technology was rather fixed.
I would stand before a class and give a one-hour lecture.
Sure, the blackboard gave way to an overhead projector, and then to PowerPoint; but, otherwise, the basic classroom “production system” seemed to change little.
In the past two years, everything has changed – for the better.
At eight on Tuesday mornings, we turn on a computer at Columbia University and join in a “global classroom” with 20 other campuses around the world.
A professor or a development expert somewhere gives a talk, and many hundreds of students listen in through videoconferencing.
Information technology is revolutionizing the classroom and driving down the costs of producing first-rate educational materials.
Such monopoly pricing should be ended.
Yet there are other reasons for high health-care costs.
Many people suffer from chronic ailments, such as heart disease, diabetes, obesity, and depression and other mental disorders.
These diseases can be expensive to address if they are poorly managed and treated.
Far too many people end up in the emergency room and the hospital because they lacked the advice and help to keep their conditions under control without institutional care, or even to prevent their disorders entirely.
Now information technology is coming to the rescue.
Innovative companies like CareMore in California are using ICT to keep their clientele healthy and out of the hospital.
For example, when CareMore’s patients step on the scale at home each day, their weight is automatically transmitted to the health-care unit. If there is a dangerous weight swing, which could be caused by congestive heart failure, the clinic brings the patient in for a quick examination, thereby heading off a potentially devastating crisis.
These innovative companies’ approaches combine three ideas.
The first is to use ICT to help individuals monitor their health conditions, and to connect individuals with expert advice.
The second is to empower outreach workers (sometimes called “community health workers”) to provide home-based care in order to prevent more serious illnesses and to cut down on the high costs of doctors and hospitals.
The third idea is to recognize that many illnesses arise or become worse because of individuals’ social circumstances.
Perhaps the patient is isolated, lonely, suffering from depression, out of work, or facing some other personal or family calamity.
If these social conditions go unaddressed, they may give rise to an expensive, even deadly, medical condition.
Smart healthcare is therefore holistic, helping people not only as patients arriving in the emergency room, but also as individuals and family members in their own homes and communities.
Holistic health care is more humane, effective, and cost-efficient.
The ICT revolution provides the means to achieve holistic health care in new and powerful ways.
In economic terms, information and communications technologies are “disruptive,” meaning that they will outcompete the existing, more expensive ways of doing things.
Implementing disruptive technologies is never easy.
Existing high-cost producers, especially entrenched monopolists, resist.
National budgets may continue to favor the old ways.
Nevertheless, the promise of great cost savings and major advances in service delivery is at hand.
The world’s economies, rich and poor alike, have much to gain from accelerated innovation in the information age.
Learning from Rwanda
GENEVA – How is it that Rwanda, among the world’s poorest countries – and still recovering from a brutal civil war – is able to protect its teenage girls against cancer more effectively than the G-8 countries?
After just one year, Rwanda reported vaccinating more than 93% of its adolescent girls against the human papillomavirus (HPV) – by far the largest cause of cervical cancer.
Vaccine coverage in the world’s richest countries varies, but in some places it is less than 30%.
In fact, poor coverage in the world’s richest countries should come as no great surprise, especially when one considers the demographics of those missing out.
Where available, evidence suggests that they are mainly girls at the lower end of the socioeconomic spectrum – often members of ethnic minorities with no health-care coverage.
This implies that those who are at greatest risk are not being protected.