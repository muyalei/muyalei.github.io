How do you convince your passionate followers if other, equally credentialed, economists take the opposite view?
All too often, the path to easy influence is to impugn the other side’s motives and methods, rather than recognizing and challenging an opposing argument’s points.
Instead of fostering public dialogue and educating the public, the public is often left in the dark.
And it discourages younger, less credentialed economists from entering the public discourse.
In their monumental research on centuries of public and sovereign debt, the normally very careful Reinhart and Rogoff made an error in one of their working papers.
The error is in neither their prize-winning 2009 book nor in a subsequent widely read paper responding to the academic debate about their work.
Reinhart and Rogoff’s research broadly shows that GDP growth is slower at high levels of public debt.
While there is a legitimate debate about whether this implies that high debt causes slow growth, Krugman turned to questioning their motives.
He accused Reinhart and Rogoff of deliberately keeping their data out of the public domain.
Reinhart and Rogoff, shocked by this charge – tantamount to an accusation of academic dishonesty – released a careful rebuttal, including online evidence that they had not been reticent about sharing their data.
In fairness, given Krugman’s strong and public positions, he has been subject to immense personal criticism by many on the right.
Perhaps the paranoid style in public debate, focusing on motives rather than substance, is a useful defensive tactic against rabid critics.
Unfortunately, it spills over into countering more reasoned differences of opinion as well.
Perhaps respectful debate in economics is possible only in academia.
The public discourse is poorer for this.
The Delayed Promise of Health-Care IT
WASHINGTON, DC – Because information technology (IT) has so quickly transformed people’s daily lives, we tend to forget how much things have changed from the not-so-distant past.
Today, millions of people around the world regularly shop online; download entire movies, books, and other media onto wireless devices; bank at ATMs wherever they choose; and self-book entire trips and check themselves in at airports electronically.
But there is one sector of our lives where adoption of information technology has lagged conspicuously: health care.
Some parts of the world are doing better than others in this respect.
Researchers from the Commonwealth Fund recently reported that some high-income countries, including the United Kingdom, Australia, and New Zealand, have made great strides in encouraging the use of electronic medical records (EMR) among primary-care physicians.
Indeed, in those countries, the practice is now nearly universal.
Yet some other high-income countries, such as the United States and Canada, are not keeping up.
EMR usage in America, the home of Apple and Google, stands at only 69%.
The situation in the US is particularly glaring, given that health care accounts for a bigger share of GDP than manufacturing, retail, finance, or insurance.&#160;Moreover, most health IT systems in use in America today are designed primarily to facilitate efficient billing, rather than efficient care, putting the business interests of hospitals and clinics ahead of the needs of doctors and patients.
That is why many Americans can easily go online and check the health of their bank account, but cannot check the results of their most recent lab work.
Another difference between IT in US health care and in other industries is the former’s lack of interoperability.
In other words, a hospital’s IT system often cannot “talk” to others.
Even hospitals that are part of the same system sometimes struggle to share patient information.
As a result, today’s health IT systems act more like a “frequent flyer card” designed to enforce customer loyalty to a particular hospital, rather than an “ATM card” that enables you and your doctor to access your health information whenever and wherever needed.
Ordinarily, lack of interoperability is an irritating inconvenience.
In a medical emergency, it can impose life-threatening delays in care.
A third way that health IT in America differs from consumer IT is usability.
The design of most consumer Web sites is so obvious that one needs no instructions to use them.
Within minutes, a seven year old can teach herself to play a complex game on an iPad.
But a newly hired neurosurgeon with 27 years of education may have to read a thick user manual, attend tedious classes, and accept periodic tutoring from a “change champion” to master the various steps required to use his hospital’s IT system.
Not surprisingly, despite its theoretical benefits, health IT has few fans among health-care providers.
In fact, many complain that it slows them down.
Does this mean that health IT is a waste of time and money?
Absolutely not.
In 2005, colleagues of ours at the RAND Corporation projected that America could save more than $80 billion a year if health care could replicate the IT-driven productivity gains observed in other industries.
The fact that the US has not gotten there yet is not a problem of vision, but of less-than-ideal implementation.
Other industries, including banking and retail trade, struggled with IT until they got it right.
The gap between what IT promised and what it delivered in the early days was so stark that experts called it the “IT productivity paradox.”
Once these industries figured out how to make their IT systems more efficient, interoperable, and user-friendly, and then realigned their processes to leverage technology’s capabilities, productivity soared.
In America, as in much of the world, health care is late to the IT game, and is experiencing these growing pains only now.&#160;But health-care providers can shorten the process of transformation by learning from other industries.
The US government is trying to help.
In 2009, Congress passed the Health Information Technology for Economic and Clinical Health (HITECH) Act.
HITECH has undeniably accelerated IT adoption among health-care providers.
Yet the problems of usability and interoperability persist.
Globally, the health IT industry should not wait to be forced by government regulators into doing a better job.
Developers can boost the pace of adoption by creating more standardized systems that are easier to use, truly interoperable, and afford patients greater access to and control over their personal health data.
Health-care providers and hospital systems can dramatically boost the impact of health IT by reengineering traditional practices to take full advantage of its capabilities.
If America is any indicator, the sky is the limit when it comes to potential gains from health IT.
According to the Institute of Medicine, the US currently wastes more than $750 billion per year on unnecessary or inefficient health-care services, excessive administrative costs, high prices, medical fraud, and missed opportunities for prevention.
Properly applied, health IT can improve health care in all of these dimensions.
The payoff will be worth it.
Indeed, as with the adoption of IT elsewhere, we may soon wonder how health care could have been delivered any other way.
The Democratic Hopes of Iraqis
The escalating violence in Iraq gives a bleak impression of that country’s prospects.
Sectarian conflict seems to be increasing on a daily basis, with militias massacring hundreds of Sunnis and Shiites solely on the basis of their religious identities.
Yet it would be a mistake to think that this bloodlust represents widespread sentiment among Iraqis.
While neither American nor Iraqi security officials have yet found a way to tame the militias, the Iraqi public is increasingly drawn toward a vision of a democratic, non-sectarian government for the country.
In 2004 and 2006, I was involved in conducting two nationwide public opinion surveys in Iraq.
Contrasting the findings of these surveys demonstrates that over the two years when sectarian violence has increased, Iraqis increasingly view their fate in a national, rather than communal, context.
Over this period, the number of Iraqis who said that it was “very important” for Iraq to have a democracy increased from 59% to 65%.
These same Iraqis saw a link between an effective democracy and the separation of religion and politics, as under a western system.
Overall, those who responded that they “strongly agree” that “Iraq would be a better place if religion and politics were separated” increased from 27% in 2004 to 41% in 2006.
Particularly significant were increases from 24% to 63% during this period among Sunnis and from 41% to 65% among Kurds.
Opinion on this question within the majority Shiite community remained stable, with 23% strongly agreeing in both 2004 and 2006.
Similarly, the survey found declining support for an Islamic state.
Between 2004 and 2006, the number of Iraqis who said it was “very good to have an Islamic government where religious leaders have absolute power” fell from 30% to 22%.
Declines occurred in all three leading ethnic communities: from 39% to 35% among Shiites, from 20% to 6% among Sunnis, and from 11% to 5% among Kurds.
There was some increase in the number of Shiites who thought that there should be a strong religious element in national laws, the majority still opposed this.
Nationalist sentiment is also increasing.
Asked whether they considered themselves “Iraqis, above all” or “Muslims, above all,” the 2006 survey found that 28% of Iraqis identified themselves as “Iraqis, above all,” up from 23% in 2004.
In the capital, Baghdad, the center of so much sectarian violence, the numbers were even more impressive, with the share of the population who saw themselves as “Iraqis, above all” doubling, from 30% to 60%.
By contrast, similar surveys in other Arab capitals find a decided tilt toward a Muslim identity.
In Amman, Jordan, the most recent figure is 12% who put their national identity ahead of their Muslim identity.
The figure is 11% in Cairo, Egypt, and 17% in Riyadh, Saudi Arabia.
Curiously, in Tehran, Iran, the choice is markedly in favor of Iranian, rather than Muslim, identity.
Among residents of the Iranian capital, the share of “nationalists” soared from 38% in 2000 to 59% in 2005.
At the same time, Iraqi attitudes show a strong reaction to daily violence.
Between 2004 and 2006, the proportion who strongly agreed that life in Iraq is “unpredictable and dangerous” increased from 46% to 59%.
The change was felt in all communities, rising from 41% to 48% among Shiites, from 77% to 84% among Sunnis, and from 16% to 50% among Kurds.
At the same time, the surveys found little support among any of these three major groups for sectarian conflict.
The violence has had a major effect on Iraqi attitudes toward foreigners.
By 2006, distrust of Americans, British, and French had reached 90%, and attitudes toward Iraq’s neighbors were also tense.
More than half of Iraqis surveyed said that they would not welcome Turks, Jordanians, Iranians, or Kuwaitis as neighbors.
These feelings, it appears, are directly related to the violence gripping Iraq, and the role of foreigners in the country’s perilous security situation.
So it appears that Iraqis are showing greater attachment to their national identity and are supportive of a non-sectarian approach to government.
These are the basic traits of a modern political order.
Among Sunnis, the decline in support for an Islamic state is most dramatic, and may have significant ramifications for the ability of religious extremists to recruit among them.
Although Iraqis remain angry about the violence in their country, this anger has not undermined their sense of national identity.
At the same time, they appear to be holding onto important democratic values.
The key question, of course, remains whether these values can be translated into a peaceful reality.
Why India Slowed
NEW DELHI – For a country as poor as India, growth should be what Americans call a “no-brainer.”
It is largely a matter of providing public goods: decent governance, security of life and property, and basic infrastructure like roads, bridges, ports, and power plants, as well as access to education and basic health care.
Unlike many equally poor countries, India already has a strong entrepreneurial class, a reasonably large and well-educated middle class, and a number of world-class corporations that can be enlisted in the effort to provide these public goods.
Why, then, has India’s GDP growth slowed so much, from nearly 10% year on year in 2010-11 to 5% today?
Was annual growth of almost 8% in the decade from 2002 to 2012 an aberration?
I believe that it was not, and that two important factors have come into play in the last two years.
First, India probably was not fully prepared for its rapid growth in the years before the global financial crisis.
For example, new factories and mines require land.
But land is often held by small farmers or inhabited by tribal groups, who have neither clear and clean title nor the information and capability to deal on equal terms with a developer or corporate acquirer.
Not surprisingly, farmers and tribal groups often felt exploited as savvy buyers purchased their land for a pittance and resold it for a fortune.
And the compensation that poor farmers did receive did not go very far; having sold their primary means of earning income, they then faced a steep rise in the local cost of living, owing to development.
In short, strong growth tests economic institutions’ capacity to cope, and India’s were found lacking.
Its land titling was fragmented, the laws governing land acquisition were archaic, and the process of rezoning land for industrial use was non-transparent.
India is a vibrant democracy, and, as the economic system failed the poor and the weak, the political system tried to compensate.
Unlike in some other developing economies, where the rights of farmers or tribals have never stood in the way of development, in India politicians and NGOs took up their cause.
Land acquisition became progressively more difficult.
A similar story played out elsewhere.
For example, the government’s inability to allocate resources such as mining rights or wireless spectrum in a transparent way led the courts to intervene and demand change.
And, as the bureaucracy got hauled before the courts, it saw limited upside from taking decisions, despite the significant downside from not acting.
As the bureaucracy retreated from helping businesses navigate India’s plethora of rules, the required permissions and clearances were no longer granted.
In sum, because India’s existing economic institutions could not cope with strong growth, its political checks and balances started kicking in to prevent further damage, and growth slowed.
The second reason for India’s slowdown stems from the global financial crisis.
Many emerging markets that were growing strongly before the crisis responded by injecting substantial amounts of monetary and fiscal stimulus.
For a while, as industrial countries recovered in 2010, this seemed like the right medicine.
Emerging markets around the world enjoyed a spectacular recovery.
But, as industrial countries, beset by fiscal, sovereign-debt, and banking problems, slowed once again, the fix for emerging markets turned out to be only temporary.
To offset the collapse in demand from industrial countries, they had stimulated domestic demand.
But domestic demand did not call for the same goods, and the goods that were locally demanded were already in short supply before the crisis.
The net result was overheating – asset-price booms and inflation across the emerging world.
In India, matters were aggravated by the investment slowdown that began as political opposition to unbridled development emerged.
The resulting supply constraints exacerbated inflation.
So, even as growth slowed, the central bank raised interest rates in order to rebalance demand and the available supply, causing the economy to slow further.
To revive growth in the short run, India must improve supply, which means shifting from consumption to investment.
And it must do so by creating new, transparent institutions and processes, which would limit adverse political reaction.
Over the medium term, it must take an axe to the thicket of unwieldy regulations that make businesses so dependent on an agile and cooperative bureaucracy.
One example of a new institution is the Cabinet Committee on Investment, which has been created to facilitate the completion of large projects.
By bringing together the key ministers, the committee has coordinated and accelerated decision-making, and has already approved tens of billions of dollars in spending in its first few meetings.
In addition to more investment, India needs less consumption and higher savings.
The government has taken a first step by tightening its own budget and spending less, especially on distortionary subsidies.
Households also need stronger incentives to increase financial savings.
New fixed-income instruments, such as inflation-indexed bonds, will help.
So will lower inflation, which raises real returns on bank deposits.
Lower government spending, together with tight monetary policy, are contributing to greater price stability.
If all goes well, India’s economy should recover and return to its recent 8% average in the next couple of years.
Enormous new projects are in the works to sustain this growth.
For example, the planned Delhi-Mumbai Industrial Corridor, a project with Japanese collaboration entailing more than $90 billion in investment, will link Delhi to Mumbai’s ports, covering an overall length of 1,483 kilometers (921 miles) and passing through six states.
The project includes nine large industrial zones, high-speed freight lines, three ports, six airports, a six-lane expressway, and a 4,000-megawatt power plant.
We have already seen a significant boost to economic activity from India’s construction of its highway system.
The boost to jobs and growth from the Delhi-Mumbai Industrial Corridor, linking the country’s political and financial capitals, could be significantly greater.
To the extent that democratic responses to institutional incapacity will contribute to stronger and more sustainable growth, India’s economic clouds have a silver lining.
But if India’s politicians engage in point-scoring rather than institution-building, the current slowdown may portend stormy weather ahead.
The Democratization of Aid
The outpouring of aid in response to the Indian Ocean tsunami brought hope to a troubled world.
In the face of an immense tragedy, working class families around the world opened their wallets to the disaster’s victims.
Former US President Bill Clinton called this response a “democratization of development assistance,” in which individuals lend their help not only through their governments but also through their own efforts.
But, while more than 200,000 people perished in the tsunami disaster, an equivalent number of children die each month of malaria in Africa, a disaster I call a “silent tsunami.”
Africa’s silent tsunami of malaria, however, is actually largely avoidable and controllable.
Malaria can be prevented to a significant extent, and can be treated with nearly 100% success, through available low-cost technologies.
Yet malaria’s African victims, as well as those in other parts of the world, are typically too poor to have access to these life-saving technologies.
A global effort, similar to the response to the Asian tsunami, could change this disastrous situation, saving more than one million lives per year.
Herein lies the main message of the new report of the UN Millennium Project, which was delivered in mid-January to UN Secretary General Kofi Annan.
The Project, which I direct on behalf of the Secretary General, represents an effort by more than 250 scientists and development experts to identify practical means to achieve the Millennium Development Goals to cut extreme poverty, disease, and hunger by the year 2015.
Our new report, entitled Investing in Development: A Practical Plan to Achieve the Millennium Development Goals (available for download at www.unmillenniumproject.org ), shows that these goals can be achieved.
The key to meeting the Millennium Development Goals in poor countries is an increase in investment in people (health, education, nutrition, and family planning), the environment (water and sanitation, soils, forests, and biodiversity), and infrastructure (roads, power, and ports).
Poor countries cannot afford these investments on their own, so rich countries must help.
If more financial aid is combined with good governance in poor countries, then the Millennium Development Goals can be achieved on time.
In short, our new Report is a call to action.
Rich countries and poor countries need to join forces to cut poverty, disease, and hunger.
The reason that the Millennium Development Goals are feasible is that powerful existing technologies give us the tools to make rapid advances in the quality of life and economic productivity of the world’s poor.
Illness and deaths from malaria can be reduced sharply by using insecticide-treated bed nets to stop the mosquitoes that transmit malaria, and by effective medicines when the illness strikes.
The total cost of battling malaria in Africa would be around $2 to $3 billion per year.
With around one billion people living in high-income countries, it would thus cost just $2 to $3 per person per year in the developed world to fund an effort that could save more than one million children annually.
When child mortality is reduced, poor families choose to have fewer children, because they are more confident that their children will survive to adulthood.
Thus, paradoxically, saving children’s lives is part of the solution to rapid population growth in poor countries.
Malaria is an important example where specific investments can solve the problems of disease, hunger, and extreme poverty.
Our report makes dozens of such practical recommendations.
Investments in soil nutrients and water harvesting could help African farmers double or triple their food yields.
Anti-retroviral medicines can help save millions from death due to AIDS.
Rural roads, truck transport, and electricity could bring new economic opportunities to remote villages in Latin America, Africa, and Asia.
School meal programs using locally produced food could boost attendance by poor children, especially girls, and improve their ability to learn, while also providing an expanded market for local farmers.
It’s up to the US and Japan to follow through on their promises as well.
Moreover, with the “democratization” of aid now underway, we can look forward to increased private efforts alongside official development assistance.
Of course, not all developing countries are sufficiently well governed to use an increase in aid in an honest, effective way.
The world should therefore start this bold effort by focusing on the poor countries that are relatively well governed and that are prepared to carry out needed investments in an efficient and fair manner.
Ghana, Senegal, Tanzania, Kenya, and Ethiopia are on that list.
It is urgent that we get started in these and similarly well governed poor countries this year.
America’s Blinders
SINGAPORE – The time has come to think the unthinkable: the era of American dominance in international affairs may well be coming to an end.
As that moment approaches, the main question will be how well the United States is prepared for it.
Asia’s rise over the last few decades is more than a story of rapid economic growth.
It is the story of a region undergoing a renaissance in which people’s minds are re-opened and their outlook refreshed.
Asia’s movement toward resuming its former central role in the global economy has so much momentum that it is virtually unstoppable.
While the transformation may not always be seamless, there is no longer room to doubt that an Asian century is on the horizon, and that the world’s chemistry will change fundamentally.
Global leaders – whether policymakers or intellectuals – bear a responsibility to prepare their societies for impending global shifts.
But too many American leaders are shirking this responsibility.
Last year, at the World Economic Forum in Davos, two US senators, one member of the US House of Representatives, and a deputy national security adviser participated in a forum on the future of American power (I was the chair).
When asked what future they anticipated for American power, they predictably declared that the US would remain the world’s most powerful country.
When asked whether America was prepared to become the world’s second-largest economy, they were reticent.
Their reaction was understandable: even entertaining the possibility of the US becoming “number two” amounts to career suicide for an American politician.
Elected officials everywhere must adjust, to varying degrees, to fulfill the expectations of those who put them in office.
Intellectuals, on the other hand, have a special obligation to think the unthinkable and speak the unspeakable.
They are supposed to consider all possibilities, even disagreeable ones, and prepare the population for prospective developments.
Honest discussion of unpopular ideas is a key feature of an open society.
But, in the US, many intellectuals are not fulfilling this obligation.
Richard Haass, the president of the Council on Foreign Relations, suggested recently that the US “could already be in the second decade of another American century.”
Likewise, Clyde Prestowitz, the president of the Economic Strategy Institute, has said that “this century may well wind up being another American century.”
To be sure, such predictions may well prove accurate; if they do, the rest of the world will benefit.
A strong and dynamic US economy, reinvigorated by cheap shale gas and accelerating innovation, would rejuvenate the global economy as a whole.
But Americans are more than ready for this outcome; no preparation is needed.
If the world’s center of gravity shifts to Asia, however, Americans will be woefully unprepared.
Many Americans remain shockingly unaware of how much the rest of the world, especially Asia, has progressed.
Americans need to be told a simple, mathematical truth.
With 3% of the world’s population, the US can no longer dominate the rest of the world, because Asians, with 60% of the world’s population, are no longer underperforming.
But the belief that America is the only virtuous country, the sole beacon of light in a dark and unstable world, continues to shape many Americans’ worldview.
American intellectuals’ failure to challenge these ideas – and to help the US population shed complacent attitudes based on ignorance – perpetuates a culture of coddling the public.
But, while Americans tend to receive only good news, Asia’s rise is not really bad news.
The US should recognize that Asian countries are seeking not to dominate the West, but to emulate it.
They seek to build strong and dynamic middle classes and to achieve the kind of peace, stability, and prosperity that the West has long enjoyed.
This deep social and intellectual transformation underway in Asia promises to catapult it from economic power to global leadership.
China, which remains a closed society in many ways, has an open mind, whereas the US is an open society with a closed mind.
With Asia’s middle class set to skyrocket from roughly 500 million people today to 1.75 billion by 2020, the US will not be able to avoid the global economy’s new realities for much longer.
The world is poised to undergo one of the most dramatic power shifts in human history.
In order to be prepared for the transformation, Americans must abandon ingrained ideas and old assumptions, and liberate unthinkable thoughts.
That is the challenge facing American public intellectuals today.
The Emerging Economies’ Eurozone Crisis
WASHINGTON, DC – Most of today’s economic institutions, from money to banking, evolved over many years – the unintended consequences of decisions by millions of individuals.
By contrast, the eurozone stands out for being a deliberate creation.
It is arguably the world’s second-largest, deliberately-planned economic structure, after Communism.
The eurozone is a remarkable experiment, a genuine vanguard of global progress.
As 2012 comes to a close, it is in trouble, and every effort must be made to nurture and strengthen it.
By the second half of 2011, it was evident that emerging economies, which had weathered the financial crisis that began in 2008 moderately well, were taking on water as the eurozone crisis deepened.
Growth slowed sharply in Brazil, India, China, and other countries.
Central banks acted as lenders of last resort, thereby averting a major crisis. In December 2011 and February 2012, the European Central Bank announced the long-term refinancing operation (LTRO), whereby European banks were lent around €1 trillion ($1.3 trillion) in two tranches.
Then, in July, came ECB President Mario Draghi’s famous assurance to do “whatever it takes” to save the euro.
The United States Federal Reserve injected liquidity, as did other advanced countries’ central banks.
There was a collective sigh of relief, financial markets stabilized, and industrial-production rebounded.
The question on everyone’s mind now is whether this post-storm calm will last, allowing the global economy to pick up.
Nowhere does this question loom larger than in developing and emerging economies, which are outside the main theater of the crisis, but are more precariously positioned than the advanced countries.
Many had only recently begun to grow rapidly, and, with vast reservoirs of poor people, economic growth has a moral urgency that it does not have in rich countries.
So, will the global economy stage a sustained recovery?
Examining the past as carefully as I can, and aware of the risks of augury, my answer has to be no.
Until 2015, the outlook is gloomy for Europe and, by extension, for the emerging and developing economies.
The injection of liquidity that occurred over the last year was the right policy.
But it only bought time; it did not solve the problem.
And time is running out.
Unfortunately, most people have an instinctive propensity to look away from approaching problems until they are very close.
America’s “fiscal cliff,” for example, was long in coming, but we are scrambling to avoid it only now.
So we should take early stock of the fact that there is another problem coming our way, which may be called (to give it the resonance of a coming storm) Edward –&#160;the “European Debt Wall and Repayment Deadline.”
The LTRO money that banks received on such easy terms, we must recall, took the form of three-year loans, which implies a wall of debt repayment in December 2014 and February 2015.
If Europe succeeds in making major fiscal and banking reforms and gets its economy in order, Edward will lose steam.
If not, the crisis will persist, and Europe will be rocked as Edward makes landfall by the end of 2014.
Where does that leave developing countries?
The US and Europe are the world’s two largest economic powerhouses.
Their slowdown will have an adverse impact on all emerging economies.
Moreover, the US and Europe have already used large doses of fiscal stimulus, which shares an uncanny similarity to antibiotics.
Administered over a short duration, it can be a powerful antidote; but, used repeatedly over too long a period, the side effects can outstrip the benefits.
Consider the case of India.
Since 2009, India has been expanding its deficit as a deliberate measure to counter its economic slowdown.
Because fiscal expansion followed several years of restraint, it was very effective in spurring demand and output growth.
But now the scope for further expansion is limited.
Unlike advanced countries, most emerging economies are exhibiting inflationary pressures, which could be exacerbated by another round of stimulus spending.
So the short-run situation remains precarious.
Nevertheless, for emerging economies, the medium- to long-term prospects are bright.
Countries that are saving a substantial amount, investing in human capital, and providing a modicum of good governance should resume their previous rapid growth.
India, for example, is saving and investing well over 30% of its GDP, devoting a significant share of these resources to infrastructure.
Its entrepreneurial capacity is expanding.
In several recent years, India’s outward direct investment in Britain has exceeded inward direct investment from Britain.
So, once the crisis is over, annual growth should rebound to its earlier rate of more than 8%.
Investors seem to be taking this view to heart.
They have been tightfisted when it comes to short-term equity investments.
But, when it comes to long-term direct investment, they committed a record-high $43.8 billion to India in 2011-2012.
Beyond the current crisis, the prospect appears to be similar in other major emerging economies, including Brazil, China, and Indonesia.
Easing short-term jitters and paving the way for further developing-country growth will require a clear and credible program for returning high-income economies, especially those in Europe, to a sustainable fiscal path.
It will be a bumpy road ahead, requiring careful navigation and bold policy implementation.
The Diabetes Watch
WELLINGTON – The world is currently in the grip of a diabetes epidemic.
This is not surprising – an increase in body fat and a decrease in physical activity are the direct causes of type 2 (as opposed to type 1) diabetes.
In fact, much of the health effects of obesity and physical inactivity are mediated through diabetes.
These health effects are serious.
Diabetes already is the major cause of kidney failure, blindness, and lower-limb amputation in many countries, and a major cause of heart attacks and strokes.
Despite this, surveillance of diabetes remains relatively undeveloped throughout the world, even in high-income countries.
Public-health surveillance is “the ongoing systematic collection, analysis, interpretation, and dissemination of health data for the purpose of preventing and controlling disease” – in short, information for action.
There is nothing in this definition that restricts surveillance to communicable diseases, yet in practice this has generally been the case. The reasons are not hard to find.
Communicable disease outbreaks occur over days to weeks (or at the most, months); the danger is “clear and present”; and prevention and control generally requires intervention by the state – the quarantine of victims, tracing and immunization of contacts, or elimination of environmental sources of the infectious agent.
The situation regarding chronic diseases like diabetes is very different.
The epidemic happens silently over years or decades; the danger is either not recognized or not considered avoidable; and action is often seen as the responsibility of the individual (lifestyle modification) or health-care system (pharmaceutical prescription), rather than the state.
Yet effective chronic disease surveillance can save lives.
If disease trends are monitored, along with patients’ responses to treatment and the population’s exposure to risk factors, the success or failure of policies designed to prevent or control chronic diseases can be evaluated, resource allocation can be rationally prioritized, and the public can be kept fully informed of the risks that they face.
Recognizing this, in December 2005 the New York City Board of Health mandated the laboratory reporting of test results for glycosylated haemoglobin (HbA1c) – a biomarker for diabetes and a key indicator of blood glucose control – thereby creating the world’s first population-based diabetes registry.
Mandatory laboratory reporting of HbA1c results (along with basic demographic data) for a defined population (New York City residents) allowed New York’s Department of Health to monitor trends in diabetes prevalence, assess testing coverage, and examine health-care use and glycemic control of residents living with diabetes.
Beyond these population-based surveillance functions, the registry was able to support patient care by ensuring that individual health-care providers and patients were made aware of elevated or rising HbA1c levels.
Both the patient-support function and the surveillance function required use of a unique patient identifier, so that letters could be mailed to patients and tests from the same patient could be linked over time.
In 2009, Thomas Frieden and colleagues from the New York City Board of Health reviewed the registry’s first four years of operation and concluded that it was performing well.
Getting all laboratories to report regularly and completely, however, proved challenging and not all health-care providers and patients proved willing to participate.
The Board of Health’s initiative has been widely praised as exemplifying the application of classical communicable-disease surveillance-and-control tools to a chronic disease.
Other commentators, however, have criticized the registry for potentially compromising patient confidentiality and privacy, and even for disrupting the relationship between patients and their doctors.
While these criticisms may or may not be justified, it is probably true to say that the New York City diabetes registry, though highly innovative, is at best an interim solution.
Rather than relying on laboratory reporting of a single biomarker, an ideal chronic-disease surveillance system would extract all necessary data directly from the patient record.
Any diagnosis of diabetes, or subsequent monitoring of disease progression, requires a medical consultation and hence an entry into the patient record – and so into the practice’s patient-management information system.
Logically, the surveillance system should operate by extracting the entire subset of data required for surveillance purposes from each health-care provider’s patient management information system (“front-end capture”).
This data would then be securely transferred (electronically) to a suitable data warehouse.
After appropriate cleaning (checking for missing data, correcting coding errors), and anonymizing, the data would be available for access and querying.
Given appropriate statistical analysis and careful interpretation, useful reports could be generated for surveillance purposes and, if desired, for patient-care support as well (using encrypted unique patient identifiers to preserve confidentiality of personal information).
In view of the rising burden of diabetes and other chronic diseases throughout the world, urgent attention must be devoted to strengthening surveillance systems for noncommunicable diseases at all levels – from local practices to global institutions.
The Diet Debacle
SAN FRANCISCO – Two seemingly benign nutritional maxims are at the root of all dietary evil: A calorie is a calorie, and You are what you eat.&#160;Both ideas are now so entrenched in public consciousness that they have become virtually unassailable.
As a result, the food industry, aided and abetted by ostensibly well-meaning scientists and politicians, has afflicted humankind with the plague of chronic metabolic disease, which threatens to bankrupt health care worldwide.
The United States currently spends $147 billion on obesity-related health care annually.
Previously, one could have argued that these were affluent countries’ diseases, but the United Nations announced last year that chronic metabolic disease (including diabetes, heart disease, cancer, and dementia) is a bigger threat to the developing world than is infectious disease, including HIV.
These two nutritional maxims give credence to the food industry’s self-serving corollaries: If a calorie is a calorie, then any food can be part of a balanced diet; and, if we are what we eat, then everyone chooses what they eat.
Again, both are misleading.
If one’s weight really is a matter of personal responsibility, how can we explain toddler obesity?
Indeed, the US has an obesity epidemic in six-month-olds.
They don’t diet or exercise.
Conversely, up to 40% of normal-weight people have chronic metabolic disease.
Something else is going on.
Consider the following diets: Atkins (all fat and no carbohydrates); traditional Japanese (all carbohydrates and little fat); and Ornish (even less fat and carbohydrates with lots of fiber).
All three help to maintain, and in some cases even improve, metabolic health, because the liver has to deal with only one energy source at a time.
That is how human bodies are designed to metabolize food.
Our hunter ancestors ate fat, which was transported to the liver and broken down by the lipolytic pathway to deliver fatty acids to the mitochondria (the subcellular structures that burn food to create energy).
On the occasion of a big kill, any excess dietary fatty acids were packaged into low-density lipoproteins and transported out of the liver to be stored in peripheral fat tissue.
As a result, our forebears’ livers stayed healthy.
Meanwhile, our gatherer ancestors ate carbohydrates (polymers of glucose), which was also transported to the liver, via the glycolytic pathway, and broken down for energy.
Any excess glucose stimulated the pancreas to release insulin, which transported glucose into peripheral fat tissue, and which also caused the liver to store glucose as glycogen (liver starch).
So their livers also stayed healthy.
And nature did its part by supplying all naturally occurring foodstuffs with either fat or carbohydrate as the energy source, not both.
Even fatty fruits – coconut, olives, avocados – are low in carbohydrate.
Our metabolisms started to malfunction when humans began consuming fat and carbohydrates at the same meal.
The liver mitochondria could not keep up with the energy onslaught, and had no choice but to employ a little-used escape valve called “de novo lipogenesis” (new fat-making) to turn excess energy substrate into liver fat.
Liver fat mucks up the workings of the liver.
It is the root cause of the phenomenon known as “insulin resistance” and the primary process that drives chronic metabolic disease.
In other words, neither fat nor carbohydrates are problematic – until they are combined.
The food industry does precisely that, mixing more of both into the Western diet for palatability and shelf life, thereby intensifying insulin resistance and chronic metabolic disease.
But there is one exception to this formulation: sugar.
Sucrose and high-fructose corn syrup are comprised of one molecule of glucose (not especially sweet) and one molecule of fructose (very sweet).
While glucose is metabolized by the glycolytic pathway, fructose is metabolized by the lipolytic pathway, and is not insulin-regulated.
Thus, when sugar is ingested in excess, the liver mitochondria are so overwhelmed that they have no choice but to build liver fat.
Today, 33% of Americans have a fatty liver, which causes chronic metabolic disease.
Prior to 1900, Americans consumed less than 30 grams of sugar per day, or about 6% of total calories.
In 1977, it was 75 grams/day, and in 1994, up to 110 grams/day.
Currently, adolescents average 150 grams/day (roughly 30% of total calories) – a five-fold increase in one century, and a two-fold increase in a generation.
In the past 50 years, consumption of sugar has also doubled worldwide.
Worse yet, other than the ephemeral pleasure that it provides, there is not a single biochemical process that requires dietary fructose; it is a vestigial nutrient, left over from the evolutionary differentiation between plants and animals.
It is therefore clear that a calorie is not a calorie.
Fats, carbohydrates, fructose, and glucose are all metabolized differently in the body.
Furthermore, you are what you do with what you eat.Combining fat and carbohydrate places high demands on the metabolic process.
And adding sugar is particularly&#160;egregious.
Indeed, while food companies would have you believe that sugar can be part of a balanced diet, the bottom line is that they have created an unbalanced one.
Of the 600,000 food items available in the US, 80% are laced with added sugar.
People cannot be held responsible for what they put in their mouths when their choices have been co-opted.
And this brings us back to those obese toddlers.
The fructose content of a soft drink is 5.3%.
Of course, many parents might refuse to give soft drinks to their children, but the fructose content of soy formula is 5.1%, and 6% for juice.
We have a long way to go to debunk dangerous nutritional dogmas.
Until we do, we will make little headway in reversing an imminent medical and economic disaster.
The Digital War on Poverty
NEW YORK – The digital divide is beginning to close.
The flow of digital information – through mobile phones, text messaging, and the Internet – is now reaching the world’s masses, even in the poorest countries, bringing with it a revolution in economics, politics, and society. 
Extreme poverty is almost synonymous with extreme isolation, especially rural isolation.
But mobile phones and wireless Internet end isolation, and will therefore prove to be the most transformative technology of economic development of our time.
The digital divide is ending not through a burst of civic responsibility, but mainly through market forces.
Mobile phone technology is so powerful, and costs so little per unit of data transmission, that it has proved possible to sell mobile phone access to the poor.
There are now more than 3.3 billion subscribers in the world, roughly one for every two people on the planet. 
Moreover, market penetration in poor countries is rising sharply.
India has around 300 million subscribers, with subscriptions rising by a stunning eight million or more per month.
Brazil now has more than 130 million subscribers, and Indonesia has roughly 120 million.
In Africa, which contains the world’s poorest countries, the market is soaring, with more than 280 million subscribers.
Mobile phones are now ubiquitous in villages as well as cities.
If an individual does not have a cell phone, they almost surely know someone who does.
Probably a significant majority of Africans have at least emergency access to a cell phone, either their own, a neighbor’s, or one at a commercial kiosk.
Even more remarkable is the continuing “convergence” of digital information: wireless systems increasingly link mobile phones with the Internet, personal computers, and information services of all kinds. The array of benefits is stunning.
The rural poor in more and more of the world now have access to wireless banking and payments systems, such as Kenya’s famous M-PESA system, which allows money transfers through the phone.
The information carried on the new networks spans public health, medical care, education, banking, commerce, and entertainment, in addition to communications among family and friends.
India, home to world-leading software engineers, high-tech companies, and a vast and densely populated rural economy of some 700 million poor people in need of connectivity of all kinds, has naturally been a pioneer of digital-led economic development.
Government and business have increasingly teamed up in public-private partnerships to provide crucial services on the digital network.
In the Indian states of Andhra Pradesh and Gujarat, for example, emergency ambulance services are now within reach of tens of millions of people, supported by cell phones, sophisticated computer systems, and increased public investments in rural health.
Several large-scale telemedicine systems are now providing primary health and even cardiac care to rural populations.
Moreover, India’s new rural employment guarantee scheme, just two years old, is not only employing millions of the poorest through public financing, but also is bringing tens of millions of them into the formal banking system, building on India’s digital networks.
On the fully commercial side, the mobile revolution is creating a logistics revolution in farm-to-retail marketing.
Farmers and food retailers can connect directly through mobile phones and distribution hubs, enabling farmers to sell their crops at higher “farm-gate” prices and without delay, while buyers can move those crops to markets with minimum spoilage and lower prices for final consumers.
The strengthening of the value chain not only raises farmers’ incomes, but also empowers crop diversification and farm upgrading more generally.
Similarly, world-leading software firms are bringing information technology jobs, including business process outsourcing, right into the villages through digital networks.
Education will be similarly transformed.
Throughout the world, schools at all levels will go global, joining together in worldwide digital education networks.
Children in the United States will learn about Africa, China, and India not only from books and videos, but also through direct links across classrooms in different parts of the world.
Students will share ideas through live chats, shared curricula, joint projects, and videos, photos, and text sent over the digital network.
Universities, too, will have global classes, with students joining lectures, discussion groups, and research teams from a dozen or more universities at a time.
This past year, my own university – Columbia University in New York City – teamed up with universities in Ecuador, Nigeria, the United Kingdom, France, Ethiopia, Malaysia, India, Canada, Singapore, and China in a “Global Classroom” that simultaneously connected hundreds of students on more than a dozen campuses in an exciting course on global sustainable development. 
In my book The End of Poverty , I wrote that extreme poverty can be ended by the year 2025.
A rash predication, perhaps, given global violence, climate change, and threats to food, energy, and water supplies.
But digital information technologies, if deployed cooperatively and globally, will be our most important new tools, because they will enable us to join together globally in markets, social networks, and cooperative efforts to solve our common problems.
The Dilemma of Curiosity and Its Use
Albert Einstein once said, “I have no special gift, but I am passionately curious.”
Certainly, Einstein was being tremendously modest.
But, just as certainly, curiosity is a powerful driving force in scientific discovery.
Indeed, along with talent and interest, as well as mathematical or other quantitative abilities, curiosity is a necessary characteristic of any successful scientist.
Curiosity betrays emotional passion.
It is a state of being involuntarily gripped by something that is difficult to ward off and for which, since one cannot act otherwise, one is accountable only in a limited sense.
We all come into the world curious, equipped with the psychological drive to explore the world and to expand the terrain that we think we master.
It is no coincidence that a well-known book on developmental psychology bears the title The Scientist in the Crib, a work that traces the parallels between small children’s behavior and the processes and research strategies that are usual in science.
But the urge for knowledge that drives inborn curiosity to transcend given horizons does not remain uncurbed.
Parents can tell many a tale about how, with the beginning of school, their children’s playful approach suddenly changes, as they must now focus on objects dictated by the curriculum.
Likewise, however desirable its ability to produce the unexpected and unforeseeable, science today cannot claim that it is not accountable to society.
Curiosity is insatiable and, in research, it is inextricably tied to the unforeseeability of results.
Research is an endless process, with a destination that no one can predict precisely.
The more that unexpected results, brought forth by research in the laboratory, are a precondition for further innovations, the more pressure there is to bring the production of knowledge under control, to direct research in specific directions, and to tame scientific curiosity.
But curiosity must not be limited too severely, lest science’s ability to produce new knowledge be lost.
This dilemma is at the center of many policy debates surrounding scientific research.
To be sure, not everything that arouses scientific curiosity is controversial; in fact, most scientific research is not.
Still, the dilemma is obvious in pioneering fields like biomedicine, nanotechnology, and neurosciences.
Research in these areas sometimes meets with vehement rejection, for example, on religious grounds with respect to stem-cell research, or owing to fear with respect to the possibility of altering human identity.
Curiosity implies a certain immoderation, a certain necessary excess.
That is precisely what makes it a passion: it is amoral and follows its own laws, which is why society insists on taming it in various ways.
Private investment in research directs curiosity onto paths where new scientific breakthroughs promise high economic potential.
Politicians expect research to function as a motor of economic growth.
Ethics commissions want to establish limits on research, even if these require frequent re-negotiation.
The demand for more democratic input, finally, also seeks to influence research priorities.
These considerations must be borne in all efforts to support basic research.
In Europe, the establishment of the European Research Council (ERC) is entering a decisive phase, with crucial implications concerning the role we are prepared to concede to scientific curiosity.
For the first time, support for basic research is being made possible on the EU level.
Individual teams are to enter a pan-European competition to determine the best of the best, opening a free space for scientific curiosity and enabling the unforeseeable outcomes that are characteristic of cutting-edge research.
The dilemma – and it is a decisive one – is that today we cherish the passionate curiosity of an Albert Einstein.
But we still want to control the unforeseeable consequences to which curiosity leads.
The dilemma must be overcome by allowing curiosity to be protected and supported, while trying to capture those of its fruits that will benefit society.
How we accomplish this must be continuously negotiated in the public sphere.
Irreducible contradictions will remain, and therein lie the ambivalence that characterizes modern societies’ stance toward science.
The Dilemma of Multiculturalism
Many people have suddenly become very hesitant about using the term “multicultural society.”
Or they hesitate to use it approvingly, as a desirable ideal that social reality should at least approximate.
July’s terrorist attacks in London demonstrated both the strength and the weakness of the concept.
London is certainly a multicultural metropolis.
An indiscriminate attack such as a bomb in the Underground will necessarily hit people of many cultural backgrounds and beliefs.
Sitting, or more likely standing, in the “tube” (as London’s Underground is affectionately known), one never ceases to be amazed at the ease with which Jewish mothers and Muslim men, West Indian youngsters and South Asian businessmen, and many others endure the same stressful conditions and try to lighten its impact by being civil to one another.
The terror attacks demonstrated not only how particular people helped each other, but also how the whole city, with all the ingredients of its human mixture, displayed a common spirit of resilience.
This is the positive side of a multicultural society.
Careful observers have always noted that it is strictly confined to the public sphere, to life in those parts of the city that are shared by all.
It does not extend in quite the same way to people’s homes, let alone to their ways of life in the private sphere.
This is one reason why London has experienced the other, darker side of the multicultural society: the veneer of multiculturalism is thin.
It does not take much to turn people of one group against those of others with whom they had apparently lived in peace.
We know this because it lies at the core of the murderous environment that gripped the Balkans in the 1990’s.
For decades (and in some cases much longer), Serbs and Croats, – indeed, Orthodox, Catholic, and Muslim “Yugoslavs” – had lived together as neighbors.
Few thought it possible that they would turn against each other in a bloodletting of such brutal enormity that it is very unlikely that Bosnia-Herzegovina can ever become a successful multicultural society.
Yet it happened, and in a different way it is happening now in Britain.
It is important to recognize that we are not talking about the return of age-old hostilities.
Ethnic and cultural conflicts today, often in the form of terrorism, are not the eruption of a supposedly extinct volcano.
They are, on the contrary, a specifically modern phenomenon.
For the terrorists themselves, such conflicts are one consequence of the unsettling effects of modernization.
Beneath the veneer of integration into a multicultural environment, many people – especially young men with an immigrant background – are lost in the world of contradictions around them.
Their seamless, all-embracing world of tradition is gone, but they are not yet confident citizens of the modern, individualistic world.
The question is not primarily one of employment, or even poverty, but of marginalization and alienation, of the lack of a sense of belonging.
It is in such circumstances that the key feature of terrorism comes into play: the preaching of hate by often self-appointed leaders.
They are not necessarily religious leaders; in the Balkans and elsewhere, they are nationalists who preach the superiority of one nationality over others.
But to call these hate-mongers “preachers” is fitting nonetheless, because they invariably appeal to higher values to sanctify criminal acts.
The mobilization of criminal energies by such preachers of hate is itself a modern phenomenon.
It is a far cry even from such doubtful claims as the self-determination of peoples defined as ethnic communities.
Hate preachers use highly modern methods to enhance their personal power and to create havoc around them.
But countering them does not involve warfare, or even a rhetorically looser “war on terror.”
Of course, part of the answer is to identify the relatively small number of people who are prepared to use their lives to destroy the lives of others without distinction or purpose.
But the more important issue is to identify the preachers of hate and stop their murderous incitement.
This is why it is so important to capture and prosecute Radovan Karadzic, who spurred on the homicidal rage of so many Bosnian Serbs.
And this is why militant Islamist preachers must be stopped.
Beyond this carefully targeted – and, in principle, limited – task, there remains the need to strengthen the sphere of common values and cooperation in societies that will, after all, remain multicultural.
This will be difficult, and it must not be approached naively.
Differences will not – and need not – go away; but ensuring that all citizens can rely on each other requires us to find a way to extend and bolster the civic trust that we see in the public sphere.
The Diplomacy of the Blind
PARIS – Why do revolutions so often take professional diplomats by surprise?
Is there something in their DNA that makes them prefer the status quo so much that, more often than not, they are taken aback by rapid changes, neither foreseeing them nor knowing how to respond once they begin?
What is happening today in the Arab world is a revolution that may turn out to be for the Middle East the equivalent of what the French Revolution was for Europe in 1789: a profound and radical change that alters completely the situation that prevailed before.
How many Bastilles will ultimately fall in the region, and at what pace, no one can say.
The only recent analogy is the collapse of the Soviet bloc, followed by the demise of the Soviet Union itself, in 1989-1991.
Who saw that sudden and rapid transformation coming?
As the German Democratic Republic was about to disappear, some top French diplomats in Germany were still assuring their government in Paris that the Soviet Union would never accept German reunification, so there was nothing to worry about: life would go on nearly as usual.
The specter of a united Germany was not to become a reality soon.
We saw the same conservative instinct at work with the first reactions to the events in Tunisia, and then in Egypt.
“President Ben Ali is in control of the situation,” some said.
Or “President Mubarak has our complete confidence.”
The United States managed to get it right, albeit very slowly, whereas many European countries erred on the side of the status quo for a much longer time, if not systematically, as they refused to see that the region could be evolving in a direction contrary to what they deemed to be in their strategic interest.
Historical and geographic proximity, together with energy dependency and fear of massive immigration, paralyzed European diplomats.
But there is something more fundamental underlying diplomats’ natural diffidence.
They are very often right in their readings of a given situation – the US diplomatic cables released by WikiLeaks, for example, include a slew of masterful and penetrating analyses.
But it is as if, owing to an excess of prudence, they cannot bring themselves to pursue their own arguments to their logical conclusions.
Revolutionary ruptures upset diplomats’ familiar habits, both in terms of their personal contacts and, more importantly, in terms of their thinking.
A fast-forward thrust into the unknown can be exhilarating, but it is also deeply frightening.
In the name of “realism,” diplomats and foreign-policy strategists are naturally conservative.
Indeed, it is no accident that Henry Kissinger’s masterpiece, A World Restored, was devoted to the study of the recreation of the world order by the Vienna Congress after the rupture of the French Revolution, followed by the Napoleonic adventures.
Is it more difficult to predict, and adjust to, the coming of a fundamental change, than to defend the present order, under the motto of “the devil you know is always preferable to the devil you don’t know!”
But, beyond these mental habits lie more structural reasons for the conservatism of foreign policymakers and diplomats.
By emphasizing the relations between states and governments over contacts with the opposition or civil societies (when they exist in an identifiable form), traditional diplomacy has created for itself a handicap that is difficult to overcome.
By requiring their diplomats to limit their contacts with “alternative” sources of information in a country, in order to avoid antagonizing despotic regimes, governments irremediably limit diplomats’ ability to see change coming, even when it is so close that nothing can be done.
When regimes lose legitimacy in the eyes of their citizens, it is not reasonable to derive one’s information mainly from that regime’s servants and sycophants.
In such cases, diplomats will too often merely report the regime’s reassuring yet biased analysis.
Diplomats, instead, should be judged by their ability to enter into a dialogue with all social actors: government representatives and business leaders, of course, but also representatives of civil society (even if it exists only in embryonic form).
With proper training and incentives, diplomats would be better equipped to anticipate change.
Of course, not all Western foreign ministries are the same; some do understand the need to nurture relationships with people outside the government – if not in opposition to it.
But one thing is clear: the more traditional foreign ministries tend to be, the more difficult it is for them and their diplomats to grasp change.
Needless to say, the ability to comprehend change has become indispensable at a time when the world is experiencing tectonic geopolitical shifts.
The new Middle East that is emerging before us is probably both “post-Western,” given the rise of new powers, and “post – Islamist,” with the revolt led by young, technologically savvy people with no ties to political Islam whatsoever.
By being late in perceiving change that they do not want to see coming, Western diplomats run the risk of losing on both levels: the regime and the people.
Diplomats require openness and imagination in order to carry out their responsibilities.
They should not abdicate these qualities when they are needed most.
The Diplomacy Option
DENVER – A senior Russian diplomat, in contrasting North Korea and Iran, once said to me: “The North Koreans are like neighborhood children with matches.
The Iranians are who we really need to worry about.”