And is fear in the rest of the world a response to the West’s strength, or to its new weakness?
Either way, the West has now spread its negative emotions, after having once imposed its mostly materialist values on the rest of the world.
It is, of course, too early to say whether this is a sign of deep change, or merely a passing trend, and reality is, no doubt, much less simple.
But, to distill the essence of today’s mood, one could say that fear is the direct result of the process of globalization: the world is not necessarily flat, but it definitely feels smaller –&#160;and “others” appear more menacing than ever.
In the aftermath of World War II, a group of idealistic Frenchmen bent on reconciliation with their former enemy declared that France would have “the Germany she deserved.”
That is, German behavior would be a function of how France behaved toward its defeated neighbor.
In the same vein, we will have the “other” we deserve.
If our behavior is based on fear, we will look with suspicion on all those who are different from us, deepening the alienation of the millions inside and outside our countries who believe that they cannot integrate into even the most open societies.
Their response could, in turn, call into question that very openness.
Of course, in today’s interdependent and transparent world, no society can protect itself fully.
There is no isolation from globalized markets, your neighbors’ identity crises, or the humiliation felt by those you have tried with so much (at times misguided) energy to integrate.
The simultaneity of unmanageable uncertainties – the crux of globalization itself – may lure some into seeking to reverse a process that has become inescapable and over which no one has control.
Given that all alternatives to globalization are unrealistic, frightening, or both, how can we sublimate, transcend, or at least channel our fears?
Can Western societies remain what they are, or at least should be – open, tolerant, and respectful of difference – while responding to demands for greater protection against the multifaceted threats, whether imagined or real, that we face?
How we answer these questions will in large part determine whether, in a relentlessly globalizing world, fear has the last word.
The Trapdoors at the Fed’s Exit
MUMBAI – The ongoing weakness of America’s economy – where deleveraging in the private and public sectors continues apace – has led to stubbornly high unemployment and sub-par growth.
The effects of fiscal austerity – a sharp rise in taxes and a sharp fall in government spending since the beginning of the year – are undermining economic performance even more.
Indeed, recent data have effectively silenced hints by some Federal Reserve officials that the Fed should begin exiting from its current third (and indefinite) round of quantitative easing (QE3).
Given slow growth, high unemployment (which has fallen only because discouraged workers are leaving the labor force), and inflation well below the Fed’s target, this is no time to start constraining liquidity.
The problem is that the Fed’s liquidity injections are not creating credit for the real economy, but rather boosting leverage and risk-taking in financial markets.
The issuance of risky junk bonds under loose covenants and with excessively low interest rates is increasing; the stock market is reaching new highs, despite the growth slowdown; and money is flowing to high-yielding emerging markets.
Even the periphery of the eurozone is benefiting from the wall of liquidity unleashed by the Fed, the Bank of Japan, and other major central banks.
With interest rates on government bonds in the US, Japan, the United Kingdom, Germany, and Switzerland at ridiculously low levels, investors are on a global quest for yield.
It may be too soon to say that many risky assets have reached bubble levels, and that leverage and risk-taking in financial markets is becoming excessive.
But the reality is that credit and asset/equity bubbles are likely to form in the next two years, owing to loose US monetary policy.
The Fed has signaled that QE3 will continue until the labor market has improved sufficiently (likely in early 2014), with the interest rate at 0% until unemployment has fallen at least to 6.5% (most likely no earlier than the beginning of 2015).
Even when the Fed starts to raise interest rates (some time in 2015), it will proceed slowly.
In the previous tightening cycle, which began in 2004, it took the Fed two years to normalize the policy rate.
This time, the unemployment rate and household and government debt are much higher.
Rapid normalization – like that undertaken in the space of a year in 1994 – would crash asset markets and risk leading to a hard economic landing.
But if financial markets are already frothy now, consider how frothy they will be in 2015, when the Fed starts tightening, and in 2017 (if not later), when the Fed finishes tightening?
Last time, interest rates were too low for too long (2001-2004), and the subsequent rate normalization was too slow, inflating huge bubbles in credit, housing, and equity markets.
We know how that movie ended, and we may be poised for a sequel.
The weak real economy and job market, together with high debt ratios, suggest the need to exit monetary stimulus slowly.
But a slow exit risks creating a credit and asset bubble as large as the previous one, if not larger.
Pursuing real economic stability, it seems, may lead again to financial instability.
Some at the Fed – Chairman Ben Bernanke and Vice Chair Janet Yellen – argue that policymakers can pursue both goals: the Fed will raise interest rates slowly to provide economic stability (strong income and employment growth and low inflation) while preventing financial instability (credit and asset bubbles stemming from high liquidity and low interest rates) by using macro-prudential supervision and regulation of the financial system.
In other words, the Fed will use regulatory instruments to control credit growth, risk-taking, and leverage.
But another Fed faction – led by Governors Jeremy Stein and Daniel Tarullo – argues that macro-prudential tools are untested, and that limiting leverage in one part of the financial market simply drives liquidity elsewhere.
Indeed, the Fed regulates only banks, so liquidity and leverage will migrate to the shadow banking system if banks are regulated more tightly.
As a result, only the Fed’s interest-rate instrument, Stein and Tarullo argue, can get into all of the financial system’s cracks.
But if the Fed has only one effective instrument – interest rates – its two goals of economic and financial stability cannot be pursued simultaneously.
Either the Fed pursues the first goal by keeping rates low for longer and normalizing them very slowly, in which case a huge credit and asset bubble would emerge in due course; or the Fed focuses on preventing financial instability and increases the policy rate much faster than weak growth and high unemployment would otherwise warrant, thereby halting an already-sluggish recovery.
The exit from the Fed’s QE and zero-interest-rate policies will be treacherous: Exiting too fast will crash the real economy, while exiting too slowly will first create a huge bubble and then crash the financial system.
If the exit cannot be navigated successfully, a dovish Fed is more likely to blow bubbles.
The Economist’s Stone
CAMBRIDGE – This year marks the 100th anniversaries of two distinct institutional innovations in American economic policy: the introduction of the federal income tax and the establishment of the Federal Reserve.
They are worth commemorating, if only because we are at risk of forgetting what we have learned since then.
Initially, neither the income tax nor the Fed was associated with the explicit concepts of fiscal and monetary policy.
Indeed, it wasn’t until after the experience of the 1930’s that they came to be viewed as potential instruments for macroeconomic management.
John Maynard Keynes pointed out the advantages of fiscal stimulus in circumstances like the Great Depression.
Milton Friedman blamed the Depression on the Fed for allowing the money supply to fall.
Keynes is associated with a belief in activist economic policy aimed at ensuring counter-cyclical responses to economic fluctuations – expansionary policies during recessions and policy tightening during upswings.
Friedman, by contrast, opposed discretionary policymaking, believing that government institutions lacked the ability to get the timing right.
But both opposed pro-cyclical policy, such as the misguided US fiscal and monetary tightening of 1937: before the economy had fully recovered, President Roosevelt raised taxes and cut spending, while the Federal Reserve raised reserve requirements, prolonging and worsening the Great Depression.
After World War II, students and policymakers internalized the lessons of the 1930’s.
But episodes in recent decades – for example, high inflation in the 1970’s – overwhelmed much of what was learned.
As a result, many advanced countries today are repeating the mistake of 1937, despite facing similar macroeconomic conditions: high unemployment, low inflation, and near-zero interest rates.
The pros and cons of austerity nowadays have been thoroughly debated.
Austerity’s proponents correctly point out that permanently expansionary macroeconomic policies lead to unsustainable deficits, debts, and inflation.
Advocates of stimulus are right to note that in the aftermath of a recession, when unemployment is high and inflation is low, the immediate consequences of policy contraction are continued unemployment, slow growth, and rising debt/GDP ratios.
And pro-cyclicalists, both in the US and Europe, represent the worst of all worlds by pursuing expansionary policies during booms, such as in 2003-07, and contractionary policies during recessions, such as in 2008-2012.
But, if counter-cyclicalists are right to favor moderating, rather than exacerbating, upswings and downswings in the economy, we still need to know what works best.
Given recent conditions, is monetary or fiscal stimulus the more effective instrument?
John Hicks addressed this question clearly in a once-famous 1937 article called “Mr. Keynes and the Classics.”
Under the conditions that prevailed then, and that prevail again now (high unemployment, low inflation, and near-zero interest rates), monetary expansion is relatively less effective, because it cannot push interest rates below zero.
Moreover, firms are less likely to respond to easy money by investing in new physical capital and labor if they cannot sell what they already produce in the factories they already have with the workers they already employ.
Fiscal stimulus is relatively more effective in these conditions, because it creates demand for goods without driving up those rock-bottom interest rates and crowding out private-sector demand (as it would in normal times).
None of this should be controversial.
Introductory economics used to emphasize the Keynesian multiplier effect: recipients of government spending (or of consumer spending stimulated by tax cuts or transfers) respond to the increase in their incomes by spending more, as do the recipients of that spending, and so on.
Again, the multiplier is much more relevant under current conditions, because it does not fuel higher inflation and interest rates (and thus crowd out private spending).
Unfortunately, many economists and politicians have forgotten much of what they knew (or have been blinded by new theories of policy ineffectiveness).
Indeed, by the time the 2008-2009 global recession hit, even advocates of fiscal stimulus had lowered their estimates of the multiplier.
But the continuing severity of recessions in the United Kingdom and other countries pursuing fiscal contraction has suggested that multipliers are not just positive, but greater than one – just as the old wisdom had it.
The International Monetary Fund has responded by forthrightly confessing that official forecasts, including its own, had underestimated the multiplier’s size.
Of course, the effects of fiscal policy are uncertain.
One never knows, for example, when rising debt levels might alarm international investors, who then start demanding sharply higher interest rates, as happened to countries on the European periphery in 2010.
We are also uncertain about the magnitude of the negative long-term effects of high tax rates on growth.
And monetary policy is much better understood than it was in the past.
Indeed, a much-admired recent paper characterized monetary policy as science and fiscal policy as alchemy.
To be sure, the state of knowledge and practice at central banks is close to the best that modern society has to offer, whereas fiscal policy is set in a highly political process that is poorly informed by economic knowledge and largely motivated by officials’ desire to be re-elected.
But the problem with the ancient alchemists and their search for the philosopher’s stone was not that they were stupid or selfish people.
Nor was their problem that political leaders refused to listen to them.
Rather, the state of knowledge at the time simply fell far short of the modern science of chemistry.
In this sense, the term alchemy could be applied to pre-Keynesians like US Treasury Secretary Andrew Mellon, whose prescription at the start of the Great Depression was to “liquidate labor, liquidate stocks, liquidate farmers, [and] liquidate real estate” in order to “purge the rottenness out of the system.”
It could also be applied to those today who favor returning monetary policy to the pre-1914 gold standard.
This does not mean that either fiscal policy or monetary policy has graduated to the status of a science like chemistry, underpinned by natural laws that generate precisely foreseeable outcomes.
But surely we have learned since 1913 that fiscal expansion is appropriate under some conditions, even if it is inappropriate under others.
The Fed vs. the Financiers
In his August 31 address to the world’s most influential annual monetary policy conference in Jackson Hole, Wyoming, United States Federal Reserve Chairman Ben Bernanke coolly explained why the Fed is determined to resist pressure to stabilize swooning equity and housing prices.
Bernanke’s principled position – echoed by European Central Bank head Jean Claude Trichet and Bank of England head Mervyn King – has set off a storm in markets, accustomed to the attentive pampering lavished on them by Bernanke’s predecessor, Alan Greenspan.
This is certainly high-stakes poker, with huge sums hanging in the balance in the $170 trillion global financial market.
Investors, who viewed Greenspan as a warm security blanket, now lavish him with fat six-figure speaking fees.
But who is right, Bernanke or Greenspan? Central bankers or markets?
A bit of intellectual history is helpful in putting today’s debate in context.
Bernanke, who took over at the Fed in 2006, launched his policy career in 1999 with a brilliant paper presented to the same Jackson Hole conference.
As an academic, Bernanke argued that central banks should be wary of second-guessing massive global securities markets.
They should ignore fluctuations in equity and housing prices, unless there is clear and compelling evidence of dangerous feedback into output and inflation.
Greenspan listened patiently and quietly to Bernanke’s logic.
But Greenspan’s memoirs, to be published later this month, will no doubt strongly defend his famous decisions to bail out financial markets with sharp interest rate cuts in 1987, 1998, and 2001, arguing that the world might have fallen apart otherwise.
On the surface, Bernanke’s view seems intellectually unassailable.
Central bankers cannot explain equity or housing prices and their movements any better than investors can.
And Bernanke knows as well as anyone that none of the vast academic literature suggests a large role for asset prices in setting monetary policy, except in the face of extraordinary shocks that influence output and inflation, such as the Great Depression of the 1930’s.
In short, no central banker can be the Oracle of Delphi.
Indeed, many academic economists believe that central bankers could perfectly well be replaced with a computer programmed to implement a simple rule that adjusts interest rates mechanically in response to output and inflation.
But, while Bernanke’s view is theoretically rigorous, reality is not.
One problem is that academic models assume that central banks actually know what output and inflation are in real time.
In fact, central banks typically only have very fuzzy measures.
Just a month ago, for example, the US statistical authorities significantly downgraded their estimate of national output for 2004!
The problem is worse in most other countries.
Brazil, for example, uses visits to doctors to measure health-sector output, regardless of what happens to the patient.
China’s statistical agency is still mired in communist input-output accounting.
Even inflation can be very hard to measure precisely.
What can price stability possibly mean in an era when new goods and services are constantly being introduced, and at a faster rate than ever before?
US statisticians have tried to “fix” the consumer price index to account for new products, but many experts believe that measured US inflation is still at least one percentage point too high, and the margin of error can be more volatile than conventional CPI inflation itself.
So, while monetary policy can in theory be automated, as computer programmers say, “garbage in, garbage out.”
Stock and housing prices may be volatile, but the data are much cleaner and timelier than anything available for output and inflation.
This is why central bankers must think about the information embedded in asset prices.
In fact, this summer’s asset price correction reinforced a view many of us already had that the US economy was slowing, led by sagging productivity and a deteriorating housing market.
I foresee a series of interest rate cuts by the Fed, which should not be viewed as a concession to asset markets, but rather as recognition that the real economy needs help.
In a sense, a central bank’s relationship with asset markets is like that of a man who claims he is going to the ballet to make himself happy, not to make his wife happy.
But then he sheepishly adds that if his wife is not happy, he cannot be happy.
Perhaps Bernanke will soon come to feel the same way, now that his honeymoon as Fed chairman is over.
The Fight for Pakistan’s Soul
CAMBRIDGE – As its army confronts, ever more bloodily, the Taliban in the Swat Valley, Pakistan is fighting for its very soul.
The army appears to be winning this time around, in marked contrast to its recent half-hearted confrontations with Taliban forces in neighboring tribal areas.
For now, the Taliban are on the run, some with shaved beards and some in burqas , to avoid being recognized and thrashed.
The reason is simple: increasingly, people across Pakistan support the army’s action.
This support persists despite the terrible humanitarian cost: more than 1.5 million internal refugees.
This round of fighting was preceded by a negotiated calm, as the government sought to quell militants in Pakistan’s tribal areas by striking a deal with the Taliban leader, Sufi Mohammad.
The deal, which instituted a version of Sharia law in the region in exchange for a commitment that militants would lay down their weapons, was blessed by the comparatively liberal Awami National Party (ANP), which governs the North-West Frontier Province (NWFP), where Swat is located.
But the Taliban’s assurances of a lower profile were upended by two incidents that exposed its real face. First, private news channels broadcast across the country a video clip recorded on a cell phone of the public flogging of a 17-year-old Swat girl.
This gave the public a stark sense of what Taliban justice really meant.
Then, Mohammad was interviewed on GEO TV, where he explained his political views.
According to Mohammad, democracy is un-Islamic, as are Pakistan’s constitution and judiciary, and Islam bars women from getting an education or leaving their homes except to perform the Hajj in Mecca.
In the long-term, however, what really matters is whether the Muslims of South Asia will be able to roll back the spread of Talibanization altogether.
The answer to that question lies within the various Muslim communities of the region, not just in Pakistan.
Afghanistan faces an election later this year.
A clear and transparent vote will make a real difference in establishing the credibility of the Afghan government.
In Pakistan, the democratic transition, after years of military rule, is still not complete. There is much hope, though, in the vibrancy of the Pakistani media, as well as in the energy that the legal community generated last March in restoring deposed Chief Justice Iftikhar Mohammad Chaudhry to his seat on the Supreme Court.
Then there is the Pakistani army, the country’s “super political party.” To a large degree, Pakistan’s relations with India, Afghanistan, and the United States depend on the military.
Army commander Ashfaq Kiyani has shown no interest in taking over the state, as his predecessor, General Pervez Musharraf, did. But the army must accept its subservience to Pakistan’s political leadership.
The army command must finally recognize that repeated military interventions have not served the country well.
Most significantly, in the face of martial law and political assassination, Pakistanis have not given up their dream of democracy.
A living example of this is Afzal Lala, a Pashtun politician associated with the Awami National Party who, despite all the threats from the bloodthirsty Tehrik-e-Taliban Pakistan, remained in Swat through the recent fighting.
Democracy will be decisive because it generates investments in education, health, and economic empowerment that reward ordinary voters.
Talibanization gains ground when people lose faith in the capacity of the modern state to improve their lives.
While poor law enforcement needs urgent attention, counter-terrorism is never solely a military affair.
Financial pledges from the US and the “Friends of Pakistan” consortium (the European Union, China, and Japan) are important, but when it comes to investing wisely in development projects, Pakistan’s track record is nothing to be proud of.
Effective oversight from donors and Pakistan’s private sector will be critical.
Only one condition should be imposed on aid for Pakistan: the first money should be spent on rebuilding all the bombed-out girls’ schools in Swat.
If need be, the army should guard these schools around the clock.
The Fiscal Crisis Down Below
PALO ALTO – Sub-national governments – states, countries, cities, provinces, towns, and special districts – play different roles from country to country, but usually deliver important public services such as police and fire protection, transportation, education, health care, and welfare.
In many countries, their fiscal position has collapsed under the combined weight of mismanagement and the global economic and financial crisis.
The relationship between sub-national and central governments includes the general division of responsibilities for providing and financing public services; national subsidies that at least partly pay for various services delivered locally; and tax collection.
In the United States, the federal government has primary responsibility for defense, public old-age pensions (Social Security), and health care for the elderly (Medicare); sub-national governments are responsible for education and law enforcement.
Health care for the poor (Medicaid) is a shared responsibility.
Matching funds flow from the federal government to state and local governments by formulae delineating the shared responsibilities. Some of these formulae allow sub-national governments wide discretion; others do not.
The global economic and financial crisis has precipitated an immense expansion of central government spending, borrowing (and hence future taxing), lending, regulation, and mandates, some in “aid” to sub-national governments (about $200 billion in the US stimulus bill).
A key question is whether central governments’ power worldwide will expand permanently – over not only the private economy, but also over sub-national governments.
There is remarkable variability in the role of sub-national governments relative to central governments.
Prior to the current crisis, US government revenue was roughly 60% federal and 40% state and local.
France was the most centralized of the major economies, with roughly a 80%-20% split between the national and sub-national governments, while the United Kingdom falls in between, at 75%-25%.
China was the least centralized, at 30%-70%.   Argentina was the most balanced, at about 50%-50%.
Debates about government centralization run deep in the history and constitutions of most countries.
The US Constitution knitted together the thirteen original colonies, and its Tenth Amendment reserves to the states all powers not expressly delegated to the federal government.
Even today, tensions over further centralization (e.g., to supranational authority, as in Europe) and devolution (e.g., for Scotland, Québec, or Kurdistan) are intense.
There are several reasons to favor a healthy dose of decentralization. US Supreme Court Judge Louis Brandeis famously argued for “the states as laboratories.”
A recent example in the US was welfare reform.
When states got waivers for time limits and work or training requirements for welfare recipients, they were so successful that federal welfare reform followed.
Competition among localities – for example, between states for businesses and workers, and between school districts for students – can lead to more efficient and effective allocation of public resources.
If people are able to migrate, they will move to jurisdictions with the mix of taxes and services (e.g., quality schools) that they prefer.
As with competition in private markets, competition in government services leads to better outcomes.
Local authorities are closer to problems than national officials tucked away in a country’s capital.
There are important differences among jurisdictions.
Hospitals are more important for some areas, while schools are more important for others.
Geographic cost-of-living differences are difficult to reflect in one-size-fits-all national programs.
Some functions are better suited for central-government financing.
National defense is an obvious example, as are functions for which economies of scale are important.
Where localities try to offload support for the poor onto other localities, the central government must step in and either fund such programs directly or set minimum standards.
In the US until the past three decades, local school districts were responsible for education.
Various court rulings have determined that this process allowed richer districts to spend more than poorer ones and ordered states to equalize spending.
States from California to Texas now routinely collect previously local property taxes to fund schools, and then redistribute them to local school districts.
Many believe that the reduction of local control over the schools as a result of the elimination of local financing has been a major contributor to the poor performance of some American schools.
California is an example of long-run fiscal folly meeting short-run national and global economic crisis, resulting in chaos.
For many decades, Californians had rapidly rising living standards, great public K-12 and higher education systems, and unprecedented upward mobility.
But California’s unemployment rate, 12.3% in November 2009, was tied for the nation’s third highest.
People and jobs are seeking better opportunities elsewhere. The state’s bond rating is dead last.
Excessive state spending, heavy regulation, and dangerously high taxes have helped create the state’s economic woes.
The top personal income-tax rate (also levied on capital gains), the sales tax rate, the corporate tax rate, and the gas tax are all at or near the highest of any state.
The top 1% of the state’s income earners pay almost half the income taxes.
Thus, the state’s coffers (and hence spending) overflow during booms, but then collapse, forcing emergency retrenchment, during busts.
Ironically, California’s progressive tax and spending policies create such volatility that they destroy the state’s ability to fund everything, even basic services from education to health for its most vulnerable citizens.
Now facing another $20 billion deficit despite temporary tax hikes and spending cuts, California’s fiscal woes foreshadow those of many sub-national – or, indeed, national –governments globally.
While all must first deal with the current emergency, sensible local fiscal, tax, and political reforms are vital to restore a balance of centralized and decentralized government.
The Fiscal Cliff and US Foreign Policy
PRINCETON – The world should be worried.
The possibility that US President Barack Obama and the Republicans in Congress will fail to reach a compromise before mandatory deep spending cuts and tax increases take effect on January 1 is very real.
Global markets are well aware of the danger of the United States falling over the “fiscal cliff,” and are watching nervously.
They know that this outcome could well throw the US – and the world – back into recession.
Foreign ministries around the world should be equally nervous.
Unless the US can get its fiscal house in order, it will be forced to abdicate leadership on a wide range of critical global issues.
In the short term, Syria and its neighbors are already paying the price of America’s inability to focus on anything other than domestic politics since Obama’s re-election.
In my view, the Syrian crisis is at a tipping point: while it is now apparent that the opposition will eventually win and President Bashar al-Assad will fall, the endgame’s duration will be a key element determining who actually comes into power and on what terms.
Syria’s implosion, and the chaos and extremism that are likely to breed there, will threaten the entire Middle East: the stability of Lebanon, Jordan, Turkey, Iraq, Gaza, the West Bank, Israel, Iran, and Saudi Arabia hangs in the balance.
But we do not even know who will succeed Hillary Clinton as US Secretary of State when Obama’s second term formally begins in January, or who will be on the White House security team.
In the medium term, the world abounds with tensions and potential crises that US leadership is likely to be indispensable to resolving.
As events over the past two weeks in Egypt have demonstrated all too vividly, the Arab awakening is still only in its first act in many countries.
Indeed, democracy is fragile, at best, across North Africa; and, in the Middle East, Jordan, the Palestinian territories, Kuwait, and Saudi Arabia have only begun to feel the ripples of the tidal wave sweeping the region.
Bahrain remains a flashpoint; Iraq is deeply unstable; and the simmering conflict between Iran and Israel could flare up at any time.
Even when the US is not on the front lines, it has played a vital role in behind-the-scenes diplomacy, nudging wary rivals closer to one another to create a united opposition, and working with regional leaders like Turkey, Egypt, and Saudi Arabia to broker deals.
In Asia, the US has been playing a similar role in pushing for multilateral resolution of dangerous bilateral disputes between China and its many neighbors over territories in the East and South China Seas, while at the same time restraining US allies who might otherwise provoke crises.
And, on big global issues like climate change, organized crime, trade, and prevention of atrocities, the absence of the US as a policy catalyst and active negotiator will be quickly and keenly felt.
Avoiding this fate requires the US to “rebuild itself at home,” as the Obama administration’s 2010 National Security Strategy promises.
But, if US politicians spend the next two years the way they have spent the last two – patching together temporary policy fixes while avoiding the hard issues that voters and markets expect them to face – America’s voice will grow fainter, and weaker, in international institutions and affairs.
Equally worrisome is the prospect of deep, across-the-board cuts in the US defense budget at a time when many rising powers are increasing their defense spending.
As much as many countries may dislike the US military, the availability and extraordinary capabilities of America’s soldiers, ships, aircraft, and intelligence assets often function as a global insurance policy.
In the long term, the challenge is more vague, but deeper.
The longer the US obsesses over its own political dysfunction and attendant economic stagnation, the less likely it is to bear the mantle of global responsibility and leadership.
Openly isolationist political forces, such as the Tea Party and libertarians like Ron Paul, will grow stronger.
A retreating US will, in turn, guarantee the emergence of what foreign-policy analyst Ian Bremmer describes as a “G-Zero world,” in which no country will take the lead and marshal the necessary economic and political coalitions to solve collective problems.
Individual presidents and secretaries of state will certainly try.
But, without Congressional support, they will bring fewer and fewer resources to the table and will suffer from an increasing credibility gap when they seek to negotiate with other countries.
Global leaders can do more than stand by and watch.
Why not remind US politicians of their global responsibilities?
The G-7 or G-8 leaders could issue a statement, for instance, urging the US to get its fiscal house in order.
NATO allies could make a similar statement.
Indeed, other regional organizations, such as the African Union or the Arab League, could weigh in.
Even G-20 members, were they so moved, could make a statement.
Of course, when we think about the G-20, we immediately wonder who, other than the US, could organize the issuance of such a statement.
That is precisely the problem, and it could get much worse.
The Fish that Shrank
A thousand years ago, the Norse settlers of my home city of York ate cod that weighed as much as eight kilograms.
We know this from archaeologists and the fascination they have for medieval waste heaps.
Moreover, the fish will not have had a chance to reproduce, undermining the fish stock altogether.
For example, the avowed intention of the EU Common Fisheries Policy is "to protect fish resources by regulating the amount of fish taken from the sea, 
 by allowing young fish to reproduce
 , and by ensuring that measures are respected."
As it stands, however, we have "fished down" our stocks, until few large, old individuals remain.
To see this, consider North Sea cod.
Suppose you start with ten thousand individuals aged one year.
If there is only natural mortality, about one thousand of these individuals survive to reach age eight years.
A moderate level of fishing mortality brings the number of survivors down to about one hundred individuals.
But the levels of fishing mortality we have been applying over the last twenty years bring the number of survivors down to about three.
Mortality acts cumulatively, so, given the way we manage fisheries, few individuals survive to become big and old.
In fact, first attempts at spawning can be relatively unsuccessful, so a fisheries policy that depends for the most part just on first-time spawners (which is what we are approaching now), could be especially flawed. 
The policy is also suspect because the success of spawning is notoriously variable from year to year.
A bad year for recruitment in a fished-down stock means losing both the new cohort and most of the spawners, because the latter are caught before they can spawn again.
By contrast, a stock with lower mortality for large individuals at least maintains an adequate reserve of large individuals available as spawners next year.
The long-term concern is that fishing is intended to be selective; fishermen aim to harvest the most profitable species and the most profitable sizes within species.
Like all other forms of life, fish stocks change genetically when directional selection is applied.
The effect of fishing-down is to give an advantage to genes causing slow growth but early adulthood.
Indeed, there is evidence from many fisheries of earlier maturation; such changes take place fast enough to be detected over the decades.
For example, early in the twentieth century, you would not find a mature cod in the North Sea less than about 50 centimeters in length, whereas by the 1980's mature cod were as small as 15 centimeters.
So there is a lot of under-age sex out there in the sea.
By cherishing our large, old fish we would reverse the selection for early maturation while selecting for faster growth through the size classes most vulnerable to fishing. 
The genetic changes that we are bringing about in our fish stocks are going largely unrecognized.
Yet the precautionary principle places some responsibility on us to hand down the living resources of the marine realm in a form that can be used as much by future generations as by us.
Radical new thinking is needed in fisheries management to overcome the ecological and evolutionary problems generated by current practice.
The recent credit and liquidity crisis has revealed a similar condition in financial markets, because no one, not even central banks, seems to understand its nature or magnitude.
It is often suggested that what is called the “subprime” crisis was the result of lax monetary policies that led to excess liquidity in financial markets.
But there is an obvious paradox here, because how can an 
 excess
 of liquidity result ultimately in a 
 shortage
 of liquidity that has to be made up by central banks?
In fact, monetary laxity can be a symptom not of excess liquidity, but of excess saving.
This is reflected in increasing income inequality in much of the developed world, and the vertiginous surpluses of oil-producing countries and Asian nations.
The emergence of sovereign wealth funds like those of China and the Gulf states to invest the savings of these countries’ budget surpluses is but the tip of the global excess savings iceberg.
Whereas excess liquidity is inflationary and calls for higher interest rates, excess saving is deflationary and calls for lower rates.
This brings us to the dilemma faced by central banks.
When central bankers cannot avoid a crisis – or when they contribute to create one – their fundamental mission is to do 
 everything
 in their power to contain its impact. 
Their actions, their every word or wink, suddenly assumes immense importance.
The central banker is not only a lender of last resort, but also a speaker of last resort.
In both respects, the United States Federal Reserve has fared much better than the European Central Bank in this most recent crisis.
If the problem is excess saving, and if uncertainty erodes confidence, a massive injection of liquidity such as the ECB’s in response to the subprime crisis may be necessary but not sufficient.
A rate cut also may be required to restore confidence.
The Fed has understood that message.
Europe needs to know why the ECB has not. 
Moreover, the Fed’s efforts were ultimately stabilizing, because it stated its intentions clearly prior to taking action.
By contrast, in late August, the ECB’s governor, Jean-Claude Trichet, referred to the monetary-policy decision taken before the crisis to justify the possibility of a rate hike, then left rates on hold.
On September 6, Trichet said that the ECB had maintained the 
 status quo
 , leaving it unclear what he meant.
Indeed, whereas the ECB seemed to have implicitly accepted an objective of economic growth, it maintained its hard line against inflation on October 4.
But the euro is a European affair.
Why should a situation in which Europe is bearing the burden of weak currencies in the US and Asia be remedied in a global forum?
In the middle of a financial crisis, the only thing worse than doubt is false certainty.
The paradox is that the ECB could have secured its independence by cutting interest rates on September 6, and again on October 4.
Now it will be forced to do so.
A victim of its backward-looking bias, the ECB chose to wait for the worst instead of preventing it.
As a result, the inevitable European rate cut, when it comes, will probably not have the stabilizing effect that it should.
The Folklore of Buried Memories
How victims remember trauma is the most controversial issue facing psychology and psychiatry today.
Many clinical trauma theorists believe that combat, rape, and other terrifying experiences are seemingly engraved on the mind, never to be forgotten.
Others disagree, arguing that the mind can protect itself by banishing memories of trauma from awareness, making it difficult for victims to remember their most horrific experiences until it is safe to do so many years later.
While acknowledging that trauma is often all too memorable, these certain clinical trauma theorists assert that a condition known as “traumatic dissociative amnesia” leaves a large minority of victims unable to recall their trauma, precisely because it was so overwhelmingly terrifying.
However, these clinical trauma theorists do not argue that “repressed” or “dissociated” memories of horrific events are either inert or benign.
On the contrary, these buried memories silently poison the lives of victims, giving rise to seemingly inexplicable psychiatric symptoms, and therefore must be exhumed for healing to occur.
This is no ordinary academic debate.
The controversy has spilled out of the psychology laboratories and psychiatric clinics, capturing headlines, motivating legislative changes, and affecting outcomes in civil lawsuits and criminal trials.
Whether individuals can repress and recover memories of traumatic sexual abuse has been especially contentious.
During the 1990’s, many adult psychotherapy patients began to recall having been sexually abused during childhood.
Some took legal action against the alleged perpetrators, often their elderly parents.
While complaints against parents, based on allegedly repressed and recovered memories of abuse, have declined, those against large institutions, such as the Catholic Church, have increased.
Strikingly, both advocates and skeptics of the concept of traumatic dissociative amnesia adduce the same studies when defending their diametrically opposed views.
But it is the advocates who misinterpret the data when attempting to show that victims are often unable to recall their traumatic experiences.
Consider the following.
After exposure to extreme stress, some victims report difficulties remembering things in everyday life.
Advocates of traumatic amnesia misconstrue these reports as showing that victims are unable to remember the horrific event itself.
In reality, this memory problem concerns ordinary absentmindedness that emerges in the wake of trauma; it does not refer to an inability to remember the trauma itself.
Ordinary forgetfulness that emerges after a trauma must not be confused with amnesia for the trauma.
Consider, too, that one symptom of posttraumatic stress disorder is an “inability to recall an important aspect of the trauma.”
This symptom, however, does not mean that victims are unaware of having been traumatized.
Indeed, the mind does not operate like a video recorder, and thus not every aspect of a traumatic experience gets encoded into memory in the first place.
High levels of emotional arousal often result in the victim’s attention being drawn to the central features of the event at the expense of other features.
Incomplete encoding of a trauma must not be confused with amnesia – an inability to recall something did get into memory.
Moreover, a rare syndrome called “psychogenic amnesia” is sometimes confused with traumatic amnesia.
Victims of psychogenic amnesia suddenly lose all memory of their previous lives, including their sense of personal identity.
Occasionally, this sudden, complete memory loss occurs after severe stress, but not invariably.
After a few days or weeks, memory abruptly returns.
In contrast, the phenomenon of dissociative amnesia supposedly entails victims’ inability to remember their traumatic experiences, not an inability to remember their entire lives or who they are.
Several surveys show that adults reporting childhood sexual abuse often say that there was a period of time when they “could not remember” their abuse.
Claims of prior inability to remember imply that they had attempted unsuccessfully to recall their abuse, only to remember it much later.
Yet if these individuals were unable to remember their abuse, on what basis would they attempt to recall it in the first place?
Most likely, they meant that there was a period of time when they did not think about their abuse.
But not thinking about something is not the same thing as being unable to remember it.
It is inability to remember that constitutes amnesia.
Research conducted in my laboratory on adults reporting histories of childhood sexual abuse provides a solution to this bitter controversy.
Some of our participants reported having forgotten episodes of nonviolent sexual abuse perpetrated by a trusted adult.
They described it as having been upsetting, confusing, and disturbing, but not traumatic in the sense of being overwhelmingly terrifying.
Failing to understand what had happened to them, they simply did not think about it for many years.
When reminders prompted recollection many years later, they experienced intense distress, finally understanding their abuse from the perspective of an adult.
These cases count as recovered memories of sexual abuse, but not as instances of traumatic dissociative amnesia.
That is, the events were not experienced as traumatic when they occurred, and there is no evidence that they were inaccessible during the years when they never came to mind.
Sexual abuse is not invariably traumatic in the sense of being overwhelmingly terrifying.
Of course, it is always morally reprehensible, even when it fails to produce lasting psychiatric symptoms.
The Forgotten Genocide
NEW DELHI – It is exactly 40 years since the Pakistani military regime of Yahya Khan initiated “Operation Searchlight” in March 1971.
That military expedition was but the latest in a series of pogroms carried out to intimidate the restive population of what was then called East Pakistan – today’s independent Bangladesh.
What followed was one of the worst massacres in human history, now all but forgotten by the international community.
Pakistan was created by the partition of British India in 1947, but its territory was divided into two enclaves separated by hundreds of miles.
While they shared a religion, Islam, there were major cultural and linguistic differences between East and West Pakistan.
In the east, there was a strong sense of being Bengali, and a sizeable Hindu minority continued to live in the province.
There was, moreover, strong resentment that political power lay in the hands of western-based politicians and generals who were blatantly insensitive to Bengali demands.
It seemed to many that, with the creation of Pakistan, East Pakistan had merely exchanged one form of colonialism for another.
And, as Bengali demands for autonomy gained momentum, the response became more repressive.
In November 1970, tropical cyclone “Bhola” struck East Pakistan, killing between 300,000 and 500,000 people.
Bhola is still considered one of the worst natural disasters on record, and the military dictatorship’s lukewarm relief efforts incensed the Bengali population.
So, when Pakistan’s military leaders finally allowed elections in late December 1970, East Pakistan voted overwhelmingly for the Bengali-nationalist Awami League, which won 167 of 169 seats in the province.
Since East Pakistan was more populous than West Pakistan, the election’s outcome raised the prospect that the Bengalis would now rule the country as whole.
This was not palatable to the Punjabi-dominated military brass or to Zulfikar Ali Bhutto, the leader of West Pakistan’s largest political party.
The elections were “canceled,” and East Pakistan erupted in open revolt.
Yahya Khan responded by sending in the troops.
The result was a genocide in which as many as three million people, particularly minorities and intellectuals, were killed.
Dhaka University’s residential halls were particularly targeted.
Up to 700 students were killed in a single attack on Jagannath Hall.
Several well-known professors, both Hindu and Muslim, were murdered.
Hundreds of thousands of women were systematically raped in the countryside.
By September 1971, ten million refugees had poured into eastern India.
The world knew what was happening.
Time magazine’s August 2, 1971, issue quoted a United States official saying, “This is the most incredible, calculated thing since the days of the Nazis in Poland.”
The article goes on to describe the streams of refugees:
“Over the rivers and down the highways and along countless jungle paths, the population of East Pakistan continues to hemorrhage into India: an endless unorganized flow of refugees with a few tin kettles, cardboard boxes, and ragged clothes piled on their heads, carrying their sick children and their old.
They pad along barefooted, with the mud sucking at their heels in the wet parts.
They are silent, except for a child whimpering now and then, but their faces tell the story.
Many are sick and covered with sores.
Others have cholera, and when they die by the roadside there is no one to bury them.”
The international community’s response to the massacres was shameful.
We now have copies of desperate cables sent by diplomat Archer Blood and his colleagues at the US consulate in Dacca (now Dhaka) pleading with the US government to stop supporting a military regime that was carrying out genocide.
Instead, President Richard Nixon concentrated on intimidating Indian Prime Minister Indira Gandhi into staying out.
He would even send the US Seventh Fleet to cow her.
Fortunately, Gandhi held her nerve and began to prepare for war.
Strengthened by promises of support from the US and China, Pakistan’s military commanders ordered pre-emptive air strikes against India on December 3, 1971.
The Indian response was swift and sharp.
With support from the civilian population, as well as from the Mukti Bahini, an irregular army of Bengali rebels, the Indian army swept into East Pakistan.
Nixon was too bogged down in Vietnam to do more than issue threats.
On December 16, the Pakistanis signed the instrument of surrender in Dacca.
Bangladesh was born.
Having acquiesced in the genocide, the international community has conveniently forgotten it, and no Pakistani official has ever been brought to justice.
On the contrary, many of the perpetrators later held senior government positions.
It is as if the Nuremberg trials never happened after WWII.
As the world watches Libya’s Muammar el-Qaddafi slaughter his own people, we should remember the human cost of international indifference.
The Forgotten Sick
LIVERPOOL – The developed world is familiar with the global threats of viral infections that incite fear in both rich populations and poor.
The pandemics of SARS, avian, and swine influenza have cost the global economy an estimated $200 billion.
These threats emerge frequently and unpredictably from human contact with animals.
Rapid response is required of governments, United Nations agencies, regulatory authorities, and the pharmaceutical industry for coordination, surveillance, and vaccine production.
But the poorest people – those who live on less than $2 per day – are often not considered important when a pandemic threat emerges.
They do not contribute significantly to the global economy, and their countries’ health systems function on a tiny fraction of what advanced economies devote to their populations’ health.
Conversely, the developed countries’ view of the diseases of the developing world is that only three are important: AIDS, tuberculosis, and malaria.
This stems from the power of advocacy constituencies and the recognition that these diseases might threaten the developed world.
As a result, these diseases receive a disproportionate amount of funding for research and control, while other infections kill, blind, deform, and disable many more – the “bottom billion” – who have little access to health care.
These infections are known as the Neglected Tropical Diseases (NTDs).
They are unfamiliar to the developed world, and their names are often difficult to pronounce: filariasis (elephantiasis), onchocerciasis (river blindness), schistosomiasis (bilharzia), and others, particularly intestinal worms.
These are not familiar diseases to people lucky enough to live in the world’s richest countries, but they are household names to hundreds of millions of poor people, who are often infected with more than one of them.
They are long-lasting conditions, often contracted at an early age, and both the illnesses and their symptoms are progressive.
Indeed, whereas the misery that worm diseases cause is extensive and the burden excessive – as much as tuberculosis or malaria – they do not kill immediately. Instead, they gradually erode children’s development prospects.
And symptoms accumulate: sight is gradually lost, genital lesions appear around puberty (sometimes increasing risk of HIV), and skin condition declines as millions of microscopic worms become intolerably itchy.
Blood loss, causing anemia, is the result of thousands of worms chewing at the wall of the gut.
Other diseases, such as sleeping sickness, transmitted by tsetse flies are fatal if untreated, as is leishmaniasis, if the parasites that cause it – transmitted by tiny sandflies – invade the liver and spleen.
Again, the misery caused by these infections exceeds the burden of tuberculosis or malaria.
The good news is that NTDs can be treated, as quality drugs – donated by the major pharmaceutical companies – are made available.
These drug donations for river blindness, trachoma (another blinding disease), leprosy, elephantiasis, worms, and bilharzia, as well as for sleeping sickness, give hope to millions.
Moreover, the cost of the annual treatment recommended by the World Health Organization is often less than $0.50, and much less in Asia, with delivery carried out by communities or through schools.
The increase in treatment has been spectacular – more than 500 million people in 51 countries treated for elephantiasis in 2007, and 60 million in 19 countries have been treated for river blindness.
Guinea disease is now endemic in only four countries, and leprosy is a problem in only six.
These are impressive figures, and the expense is trivial compared to the anti-retroviral drugs needed to treat AIDS, which cost more than $200 annually and must be taken every day, not every year.
Given that roughly one billion people are infected with NTDs, compared to 40 million with HIV, and that the drugs targeting them are donated and actually prevent disease and stop transmission, treating NTDs is a major opportunity to lift populations out of poverty.
The main challenge is to convince policymakers that there is more to reducing poverty than focusing on just three diseases.
In fact, NTDs are “low-hanging fruit.”
If the international community is serious about alleviating poverty and achieving development goals, tackling the diseases so directly associated with economic misery should be a fundamental objective.
We can easily meet that objective, because we have drugs that are effective, free (or very cheap), that have low delivery costs, and that provide add-on benefits.
Now is the time to rethink our public-health investments and messaging, and evaluate whether we are getting the best value for our donor dollars, or whether we should do much more to tackle diseases that we have so far largely ignored.
The Forgotten Side of the War on Terrorism
During the past decade – particularly since the September 11, 2001, attacks on the United States – Westerners have generally considered international terrorism to be the most urgent threat to human security.
Accordingly, vast resources have been mobilized and expended to counter its many forms.
Unfortunately, however, the US-led invasion of Afghanistan and the subsequent invasion – without UN authority – of Iraq underscore the primacy of military solutions in the strategic thinking of affluent nations.
At the same time, developing countries have continued to grapple with the persistence of mass poverty, endemic disease, malnutrition, environmental degradation, and gross income inequity, all of which have caused a degree of human suffering that far exceeds what has been caused by terrorist attacks.
We need, therefore, to revisit today’s global challenges from a Third-World perspective.
Indeed, a fundamental lesson of terrorist attacks and insurgencies, we now know, is that no nation, however self-sufficient, can afford to remain heedless of whether others sink or swim.
For much of the developing world, the basic instability of international relations – owing to terrorist strikes, guerrilla warfare, and the preemptive wars that America threatens on its enemies – is aggravating socioeconomic anxieties and fueling doubts about the benefits of globalization.
Certainly, we are all beginning to realize how precarious that process is – how easily market mechanisms can be rolled back by cultural resentments stemming from economic exploitation, political oppression, and social injustice.
People in the industrialized countries are already an estimated 74 times richer per capita than those in the poorest countries.
Today, one-quarter of the world’s population still lives on the equivalent of less than one US dollar a day, and the World Bank says that the daily spending power of 1.2 billion people is roughly equal to the price of a hamburger, two soft drinks, or three candy bars in the West.
According to the UN Food and Agriculture Organization, 815 million people, including 200 million children under the age of five, go to bed hungry each night.
The Philippine government has proposed that half of all scheduled debt payments be withheld for a specified period, to be invested in reforestation, clean water, housing, food production, primary healthcare, sanitation, basic education, farm-to-market roads, ecologically sound tourism, micro-finance, and related MDG projects.
For lenders, debt could be converted, wherever possible, into equity in MDG projects with earning potential, while building up poor countries’ capacity for self-reliance.
But self-reliance will be impossible to achieve as long as the rich countries favor free markets and free trade only when it suits them.
Even UN Secretary-General Kofi Annan has warned that the unrestrained tide of globalization might not raise all boats, but only the yachts – while overturning a lot of canoes.
The most glaring injustice in this respect has been the failure of the US and the European Union to deliver substantially on their promises of market access to agricultural exports from poor countries.
Bimal Ghosh, a former director of the UN Development Program, famously calculated that the daily subsidy for every cow in the EU – currently amounting to €2.50 – exceeds the daily income of millions of poor people around the world.
The poor countries argue that broader liberalization in the EU, the US, and Japan alone would yield benefits worth up to $142 billion by 2015.
The G-8 nations and the global alliance that America leads must aim not merely to defeat terrorism.
They must address all aspects of human security, including people’s well-being and safety in their homes, neighborhoods, and workplaces.
And they must win people’s allegiance by the power of their values and their ideals – not only by isolating terrorists and extremists, but also by helping, in meaningful ways, poor countries to prosper.
Above all, those who lead us today must create a genuinely new global order in which all peoples take part – with dignity and an assurance of fairness.
The Forgotten Twentieth-Century
BERLIN – It has been 20 years since the dissolution of the Soviet Union, which for many historians marked the real end of the “short twentieth century” – a century that, beginning in 1914, was characterized by protracted ideological conflicts among communism, fascism, and liberal democracy, until the latter seemed to have emerged fully victorious.
But something strange happened on the way to the End of History: we seem desperate to learn from the recent past, but are very unsure about what the lessons are.
Clearly, all history is contemporary history, and what Europeans, in particular, need to learn today from the twentieth century concerns the power of ideological extremes in dark times – and the peculiar nature of European democracy as it was constructed after World War II.
In some ways, the great ideological struggles of the twentieth century now seem about as close and relevant as the scholastic debates of the Middle Ages – especially, but not only, for younger generations.
Who now remotely understands – let alone takes the trouble to try to understand – the great political dramas of intellectuals like Arthur Koestler and Victor Serge, people who risked their lives for and then against communism?
Nevertheless, much more than most of us would care to admit, we remain enmeshed in the concepts and categories of the twentieth century’s ideological wars.
This was most obvious with the intellectual responses to Islamist terror: terms like “Islamo-fascism” or “third totalitarianism” were coined not just to characterize a new enemy of the West, but also to evoke the experience of the anti-totalitarian struggles that preceded and followed World War II.
Such terms seek to borrow legitimacy from the past and to explain the present – in a way that most serious scholars of either Islam or terrorism never found very helpful.
Analogizing in this way seemed more to reflect a desire to re-fight the old battles, rather than to sharpen political judgment about contemporary events.
So how should we think about the ideological legacy of the twentieth century?
For one thing, we need to stop viewing the twentieth century as a historical parenthesis filled with pathological experiments conducted by crazed thinkers and politicians, as if liberal democracy had been there before those experiments and merely needed to be revived after they failed.
It is not a pleasant thought – and perhaps even a dangerous one – but the fact remains that many people, not just ideologues, put their hopes in the twentieth century’s authoritarian and totalitarian experiments, viewing politicians like Mussolini and even Stalin as problem-solvers, while liberal democrats were written off as dithering failures.
This is not to make any excuses – it is not true that to comprehend is to forgive.
On the contrary, any proper understanding of ideologies must reckon with their power to seduce and even genuinely convince people who care little about their emotional appeal – whether to pride or to hate – but who think they actually offer rational policy solutions.
We must remember that Mussolini and Hitler were ultimately brought to power by a king and a retired general, respectively – in other words, traditional elites, not street-fighting fanatics.
Second, we need to appreciate the special and innovative nature of the democracy created by Western European elites after 1945.
In light of the totalitarian experience, they stopped identifying democracy with parliamentary sovereignty – the classic interpretation of modern representative democracy everywhere but in the United States.
Never again should a parliamentary assembly just cede power to a Hitler or a Pétain.
Instead, the architects of post-war European democracy opted for as many checks and balances as possible – and, paradoxically, for empowering unelected institutions to strengthen liberal democracy as a whole.
The most important example is constitutional courts – a different animal from the US Supreme Court, and one specifically tasked with ensuring respect for individual rights.
Eventually, even countries traditionally suspicious of “government by judges” – France being the classic case – accepted this model of constrained democracy.
And virtually all Central and Eastern European countries adopted it after 1989.
Importantly, European institutions – especially the European Court of Justice and the European Court of Human Rights – also fit this understanding of democracy through prima facie undemocratic mechanisms.
Today, many Europeans are clearly dissatisfied with this conception of democracy.
Many have the impression that the continent is entering what the political scientist Colin Crouch has called a “post-democratic” era.
Citizens increasingly claim that political elites do not properly represent them, and that directly elected institutions – national parliaments in particular – are forced to bow to unelected bodies like central banks. Passionate grassroots protest and surging populist parties across the continent are the result.
It will not do simply to reaffirm the post-war European model of democracy, as if the only alternative were totalitarianism of one sort or another.
But we should be clear about where we are coming from, and why – and that there was no golden age of European liberal democracy, whether before World War II, in the 1950’s, or at some other mythical point.
Ordinary Europeans long trusted elites with the business of democracy – and often even seemed to prefer unelected elites.
If they now want to modify the social contract (and assuming that direct democracy remains impossible), change ought to be based on a clear, historically grounded sense of which innovations European democracy might really need – and of whom Europeans really trust to hold power.
That discussion has barely begun.
The Forgotten Virtues of Free Trade
LONDON – “Laissez-faire,” French President Nicolas Sarkozy recently declared, “is finished.”  Perhaps, but should we really be satisfied if he is right?
If laissez-faire has run its course, what will possibly replace it as the foundation of an open, global society?
Now more than ever, it is worth remembering that the last great financial crash not only inspired the New Deal in the United States, but also plunged the world into a new dark age of economic nationalism and imperialism.
Free trade is far from perfect, but the alternatives are worse.
Protectionism is bad for wealth, bad for democracy, and bad for peace.
Yet a new wave of protectionism is a genuine danger.
Barack Obama, appealing to swelling protectionist sentiment among Americans, threatened during his presidential campaign to rewrite the North American Free Trade Agreement unilaterally.
This July, the World Trade Organization’s Doha trade round fell to pieces, partly because the US refused to lower its agricultural subsidies.
The world is on a slippery slope toward nationalism and exclusion.
If a government can step in to bail out failing banks, why not to protect its failing businesses or farmers as well?
We need a new deal for trade.
There is now widespread talk of a “Bretton Woods II” that would restructure global finance, promote sustainability, and offer developing countries “aid for trade.”
But, to be effective, any new deal to promote trade must involve more than a new set of international institutions.
It requires democratic reform from the bottom up.
In fact, this requirement is rooted in history.
We have become so accustomed to thinking of free trade as a specialist matter for liberal economists and trade negotiators in dark suits that we forget how a century ago, free trade was a core belief for many democrats, radicals, women activists, and, indeed, organized labor.
Back then, Britain was in a position not unlike that of the US today: a superpower in relative decline, facing new competitors and a backlash against globalization.
In the late nineteenth century, all powers raised their trade barriers – except Britain.
Britain’s stance holds lessons for today.
Most economists stress the superiority of the free-trade model and point to the power of lobbies and interest groups to explain its unpopularity in practice.
As US Federal Reserve Chairman Ben Bernanke has argued, trade expansion inevitably creates some losers, whose protests distract attention from the benefits of globalization.
This is true, but it is only half the story, for it ignores how, at crucial moments in history, free trade has mustered the support of the many winners.
A century ago, during an earlier crisis of globalization, the demand for free trade in Britain inspired a genuine mass movement.
It was not just a cause dear to bankers, merchants, or the young John Maynard Keynes.
It mobilized millions of people.
For women, who remained disenfranchised, free trade was a kind of substitute citizenship: parliament safeguarded their interests as consumers by keeping the door open for cheap imports.