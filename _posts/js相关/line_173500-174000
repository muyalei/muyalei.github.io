The likeliest outcome of the current flurry of diplomacy will be a fudge, which both sides will present as the “best result possible.”
There will be another roadmap to final two-state status, with some timetable for Palestinian statehood, conditional on Hamas’s renunciation of violence.
The Israelis will claim to have conceded nothing essential; Abbas’s Fatah will claim to have secured a road paved with gold.
And the peace process will creep forward –&#160;more process than peace – until it encounters the next roadblock.
Syria’s Hijacked Struggle
LONDON – As Syria’s civil war has progressed, the West’s views on arming the opposition have become increasingly confused, which reflects the growing muddle on the ground.
While President Bashar al-Assad’s regime remains vicious and tyrannical, and some of its opponents’ motives remain altruistic, the conflict can no longer be defined simply as one of good versus evil.
No unified, patriotic Syrian opposition has existed since extremists hijacked the peaceful protests in 2011.
Indeed, some opposition tactics are as brutal as those of the Syrian regime.
The United Nations estimates that security forces have suffered 15,000 fatalities, and the opposition 10,000, and that 45,000 civilians have died in the last two years of fighting.
And the UN has condemned militant groups – which now form the majority of fighters in Syria – for murder, kidnap, torture, assault, corruption, and reliance on child soldiers.
With Syria engulfed by chaos, the world does not know what to think.
Martin Dempsey, Chairman of the US Joint Chiefs of Staff, has admitted that the United States lacks a clear picture of the situation in Syria.
Moreover, Lakhdar Brahimi, the UN and Arab League special envoy for Syria, reports that opposition forces include 38 nationalities.
Jihadi fighters connected to Al Qaeda are now in the majority.
The United Kingdom’s Home Office has warned that hundreds of British and other European Muslims fighting in Syria alongside Al Qaeda-linked rebel groups could return home to carry out terrorist attacks.
In February, Syria’s state news agency accused jihadi rebels of firing a rocket containing chemical materials in Khan al-Assal – an allegation that the British television outlet Channel Four backs.
The rebels, however, blame the government for the attack (which neither side has been able to prove involved chemical weapons).
The Free Syrian Army (FSA), which the US, the UK, and France support, has denied involvement in the alleged chemical attacks, but has not condemned the rebels’ brutal campaigns.
Puzzlingly, US Secretary of State John Kerry, justifying plans to provide the FSA’s Supreme Military Council with additional assistance, has cited the danger of “letting this country, the heart of the Middle East … [become] hijacked by extremists.”
But to oppose extremism and support the FSA is a blatant contradiction.
Even the US Department of Justice has stated categorically that most of the FSA adhere to Al Qaeda’s ideology.
The Supreme Military Council is overwhelmingly Islamist, with rebel-controlled areas of Syria already practicing sharia (Islamic law).
Salim Idris, the council’s chief of staff, has expressed a willingness to fight alongside extremist groups that refuse to accept the unified command.
He has labeled only the Al Qaeda-linked Jabhat al-Nusra as extremist (though he rejects America’s branding of the group as a “foreign terrorist organization.”)
In this context, the West’s belief that it can channel assistance to certain elements of the opposition is absurd, as is the concept of “non-lethal” aid, such as night-vision goggles, medical equipment, and armor, given the conventionally “lethal” aid that is already pouring in from the Gulf states.
The Guardianrecently reported that Jordan is accepting Saudi money to supply arms directly to Syrian rebels.
Western proponents of such aid, such as Kerry and UK Foreign Minister William Hague, must recognize that these supply channels do not discriminate between opposition groups.
More important, Western leaders must comprehend the danger of picking sides.
And that means that they must stop ignoring the lessons of the campaign in Afghanistan in the 1980’s, which facilitated the Taliban’s rise to power, and the intervention in Libya in 2011, which produced a weak and divided government that remains locked in a power struggle with Islamist militias.
France, the US, the UK, and Turkey have recognized the Syrian National Coalition as Syria’s future interim government, despite reports that two-thirds of the SNC’s 263 founders are linked to the Muslim Brotherhood.
The SNC’s outgoing leader, Moaz Al-Khatib, has publicly criticized America’s decision to blacklist Jabhat al-Nusra – as has the SNC’s key backer, Turkey.
It is ironic, to say the least, that the West’s democratically elected governments happily engage with members of the Muslim Brotherhood, but refuse to work with more liberal groups.
Our views are much too liberal for the SNC, and its key backer, Turkey, whose foreign minister has said that jihad in Syria is not related to terrorism.
They disregard the warnings of people like Alain Chouet, the former chief officer of the French General Directorate for External Security in Damascus, who recently denounced France’s provision of weapons to the rebels as “completely illegal” and described how “the jihadi militias have taken precedence over the others.”
The repercussions of mishandling Syria will extend far beyond the country’s borders.
After all, Syria is situated on a national, sectarian, regional, and geopolitical fault line, where the strategic interests of Russia, China, Iran, Iraq, and Lebanon confront those of NATO, Turkey, Saudi Arabia, Qatar, Kuwait, and Jordan.
And regional tensions are running high.
Improving Syria’s prospects for peace and democracy must begin with diplomacy.
Democracy is about representation, and that should cover the full mosaic of Syria’s people.
Every group with an interest in the country’s future – including the government, domestic opposition groups, and those in exile – should be invited to participate in official talks.
And, in order to create a level playing field, it is essential to recognize that the Arab League (which has already given Syria’s seat to the Muslim Brotherhood-controlled SNC) represents the autocratic states that are encouraging sectarianism and extremism in Syria.
During this process, Western governments must stop providing money or other kinds of aid to opposition groups in Syria, and put pressure on Turkey and the Gulf countries to stop supporting extremists.
Any financial aid should be channeled through organizations like the Red Cross and UNICEF to supply medical care to civilians, or allocated to a fund for post-war reconstruction.
Some leaders may be starting to recognize the need for a new narrative.
The UN Security Council has condemned the opposition’s recent attack on a mosque in Damascus, which killed a senior pro-government Muslim cleric.
US President Barack Obama recently expressed concern about Syria “becoming an enclave for extremism, because extremists thrive in chaos.”
But, in reality, Syria has already become an enclave for extremism.
Responding with military aid will simply turn a catastrophe into an apocalypse.
The New-Model Nomad
MOGADISHU – One of my earliest childhood memories is of swimming in a small gully near my grandmother’s home in Yaaq Bari Wayne, a dusty collection of tin-roofed adobe buildings huddled together in the plains of southern Somalia’s Bay region.
After the Gu rains, the gully became a deep triangular cleft cut into the ground, filled with muddy water.
Children gathered like ants to spilled sugar, jumping off ledges and diving into its murky depths with screams and whoops of excitement.
Many were children of nomadic herders, who moved south in May and June in search of better pastures.
When they arrived, brown domes made of crisscrossed branches and layers of multicolored mats would appear like crowds of dappled beetles on the outskirts of town.
For me, the nomad was a romantic figure, akin to the American cowboy of the Wild West.
In the world’s harshest environment, they trekked hundreds of kilometers, surviving on camel milk and dried meat, with all of their possessions strapped to a camel’s back.
But, in the last two decades, the story of the Somali nomad has become far less romantic – and far more complex.
Nomads have been victims of famine, violent conflict, and circumstance, accounting for a significant share of the deaths – as many as one million – caused by 22 years of civil war, and of the 260,000 Somalis who died of starvation in 2010-2012.
At the same time, nomads have been perpetrators of violence and terrorism.
And yet there is another way to view nomads: as survivors, entrepreneurs, providers, and agents of transformation.
With global problems like food insecurity and climate change threatening livelihoods around the world, nomadic and pastoral lifestyles have the potential to protect and sustain millions of people.
In fact, it was cataclysmic climate change – which transformed a lush, green Sahara into a harsh desert – that triggered the emergence of such lifestyles in East Africa 5,000 years ago.
Cattle herding allowed nomads to create livelihoods in an increasingly unpredictable and challenging environment, characterized by an arid climate and scarce resources.
Today, there are some 50 million nomadic herders in Sub-Saharan Africa, and as many as 150 million agro-pastoralists, who combine traditional nomadic animal-rearing with some form of agriculture.
Policymakers often view nomadic pastoralism as an archaic and unproductive way of life, with little economic benefit.
But the opposite is true.
Pastoralist systems are 20% more productive than traditional ranching methods.
And pastoralists are more market-savvy than many believe; the Horn of Africa’s pastoral livestock and meat trade is estimated to be worth $1 billion.
Using land that cannot support conventional agriculture, pastoralists and agro-pastoralists produce meat, milk, and livestock products that sustain millions.
Indeed, according to an OECD study, they are responsible for 10% of the world’s meat production; in some regions, they supply as much as 60% of the beef and 70% of the milk consumed.
Effective trade networks and access to livestock markets would enable them to increase sales substantially.
Increased access to technology is also crucial to improving nomads’ economic prospects.
In Niger, pastoralists use mobile phones to keep abreast of commodity prices in regional markets, which enables them to sell their camels where prices are highest and purchase grains where prices are lowest.
In Kenya, veterinarians send text-message alerts to warn pastoralists of disease outbreaks and provide vaccination information.
Providing such support systems for pastoralists could prove to be an effective strategy for advancing economic development, enhancing food security, and reducing dependence on food aid in Somalia and elsewhere.
Indeed, for many countries, nomads – recast as intrepid entrepreneurs – could be the key to securing a prosperous future.
The Conundrum of Scientific Fraud
Science, and the behavior of scientists, has never been perfect.
Consider the Korean scientist Hwang Woo-suk, whose claim to have extracted stem cells from human embryos that he cloned turned out to be based on phony research.
Hwang has just been fired by Seoul National University, and six of his co-workers have been suspended or had their pay cut.
Hwang and his colleagues are hardly alone.
In response to the recurrence of well-publicized and highly damaging scandals in recent years, many universities and some entire national research funding agencies now convene “institutional review boards” to deal with breaches of what has come to be known as “research ethics.”
But are such boards necessary?
If so, in what spirit should they conduct their work?
From artists to scientists, all intellectual workers are preoccupied with the giving and taking of credit.
Hiring, promoting, and rewarding academic staff is increasingly based on “citation counts” – the number of times someone receives credit in peer-approved publications.
Even if someone’s work is criticized, it must be credited properly.
Credit is often given simply because someone has produced a creditable work.
But, increasingly, the fixation on credit reflects the work’s potential monetary value.
And, however one defines the creditworthiness of intellectual work, one thing is clear: reported cases of fraud are on the rise.
Sometimes fraud consists in plagiarism: the culprit takes credit for someone else’s work.
However, especially in the most competitive scientific fields, fraud often takes the form of forgery: the culprit fabricates data.
Plagiarism is the sin of the classroom; forgery is the sin of the laboratory.
But in neither case has a standard method emerged for ensuring the integrity of research – or of the scientists who carry it out.
A useful simplification in addressing the potential work of research ethics panels is to consider two models of review: “inquisitorial” and “accusatorial.”
Whereas the inquisitorial model presumes that fraud is rampant, but often undetected, the accusatorial model adopts a less paranoid stance, presuming that researchers are innocent until proven otherwise.
The natural context for an inquisitorial system is a field where scientific integrity is regularly under threat because research is entangled with political or financial interests.
Often, these entanglements are unavoidable, especially in the case of biomedical research.
Here the inquisitors are part cost accountant, part thought police.
They conduct spot-checks on labs to ensure that the various constituencies for scientific research are getting their money’s worth.
Inquisitors must be invested with the power to cut off funding to transgressors, perhaps even excommunicating them, by barring a scientist from practicing or publishing in the future.
Hwang, for example, has lost his research license and has been banned from assuming another public post for five years.
However, the credibility of such a system relies on the inquisitors’ ability to uphold standards of science that are genuinely independent from special interests both inside and outside the research community.
Otherwise, inquisitorial ethics reviews could come to be regarded as nothing more than intellectual vigilantism.
Consider the Danish Research Council’s ominously named “Committee on Scientific Dishonesty,” which was convened in 2002 following complaints raised against the political scientist Bjørn Lomborg, whose book The Skeptical Environmentalist purported to demonstrate that ecologists systematically biased their interpretation of data to support their political objectives.
The Committee initially found Lomborg guilty of much the same error that he had alleged of the ecologists.
However, the Committee refused to comment on whether his behavior was normal or deviant for the field in question.
This enabled Lomborg to appeal the verdict, successfully, claiming that the Committee had victimized him for political reasons, given his recent appointment as director of a major national institute for environmental research.
In the end, the Committee on Scientific Dishonesty was itself reorganized.
In retrospect, the Committee probably should have refused the case, given that Lomborg’s book was subject to intense public scrutiny, often by experts writing (favorably) in The Economist and (unfavorably) in Scientific American.
His was a genuine case in which intellectual work was given a fair trial in the proverbial “court of public opinion” and required no further oversight.
Indeed, Lomborg’s case would seem to support the accusatorial model of ethics review, which assumes that scientists adequately regulate their own affairs through normal peer-review procedures.
Here, scientific integrity is understood not as a duty to stakeholders—funders, companies, or, as with Hwang, politicians concerned about national prestige—but as a collective responsibility that is upheld by identifying and correcting errors before they cause substantial harm.
The accusatorial system is designed for those relatively rare cases when error slips through the peer-review net, resulting in some concrete damage to health or the environment, or causing the corruption of later research that assumes the validity of fraudulent work.
To lodge an accusation, the accuser must establish that some harm has been committed, which is then shown to have been the fault of the accused.
The countervailing assumptions of the inquisitorial and accusatorial systems reflect the ambiguity of the concept of scientific fraud.
Many so-called frauds are cases in which researchers claimed credit for work they have not really carried out, but that pointed in the right direction and was eventually completed by others.
Strictly speaking, they are guilty more of confusing the potential and the actual than the true and the false.
Indeed, by today’s standards, Galileo and Mendel committed fraud, since they likely massaged their data to fit a neat mathematical formula.
However, they remain scientific visionaries because others built so effectively on their “work.”
Of course, in their cases, no money changed hands and no one’s life was jeopardized.
Perhaps, from the point of view of research ethics, that makes all the difference.
The Cooperative Alternative
WASHINGTON, DC – In an era in which conventional models of finance, corporate governance, and corporate responsibility are increasingly debated, if not called into question, it may be time to revisit the alternative approach taken by economic cooperatives.
The foundational values of cooperatives embody not only a humane vision, but also a pragmatic approach to production that has enabled the successful ones to thrive – and to spur economic growth in countries that desperately need it.
Cooperative movements took shape in the Americas, Europe, Australia, and Japan in the 1800’s.
Many grew from the simple proposition that ordinary people could overcome adversity in the marketplace by banding together to buy and sell goods at reasonable prices, and quickly realized the added benefits of sharing knowledge among members, promoting inclusion, and building social capital.
Today, cooperatives cover a range of activities and come in a variety of shapes and sizes, from small-scale agricultural and consumer organizations in Africa to some of the leading agricultural brands and largest financial-service providers in North America and Europe. &nbsp;&nbsp;
According to the International Cooperative Alliance, a cooperative is a “jointly owned and democratically controlled enterprise.”
But, beneath this definition lie rich notions of voluntary association, accountability for strategic decisions, and concern for the communities that cooperatives serve.
Cooperatives have helped to bring information and services to far flung rural communities, empower workers, and expand financial services, healthcare, education, and housing.
In doing so, they have transformed the economic and social landscape in countless communities.
The International Cooperative Alliance reports that more than 800 million people are members of cooperatives worldwide.
Moreover, cooperatives account for a significant share of GDP in many countries, and an especially high share of the agricultural and consumer sectors.
Cooperatives are also one of the largest providers of financial intermediation to the poor, serving an estimated 78 million people globally who live on less than $2 per day.
Of course, cooperatives have sometimes struggled to live up to the ideal.
In the most egregious cases, some have fallen victim to bad politics, weak governance, or mismanagement.
Others are exposed to risks stemming from concentration in a single business sector, commodity, and/or geographic area.
Cooperatives have also wrestled with questions of members’ entry and exit, financial disclosure, and relationships with the non-cooperative sector.
And governments have often faced vexing questions with regard to financial regulation and taxation of cooperatives, including treatment of profits and consideration of exemptions.
And now, in a more mobile and urban world, one might ask: can cooperatives maintain their essential character, based on inclusion and knowledge sharing within a community?
In a world in which geography is a diminishing barrier to business, can cooperatives sufficiently distinguish themselves as a viable alternative model?
Or will they evolve to serve virtual communities, organized around new sets of challenges and opportunities?
The United Nations has declared 2012 the “Year of Cooperatives.”
This provides a good opportunity to examine the extraordinary history of cooperatives, assess their strengths and weaknesses, and rekindle a discussion about a development model that promises higher levels of inclusion, ownership, self-determination, and concern for community.
The World Bank is active in the development of producer and credit cooperatives around the world.
Some of the most notable programs include the Indian Dairy Cooperative, which has created an estimated 250,000 jobs, mostly in rural areas.
Similarly, Mexico’s National Savings and Financial Services Bank has helped to strengthen savings and credit institutions that serve millions of rural residents who would otherwise have been relegated to the margins of the formal financial sector.
The Bank’s policy work has re-affirmed the notion that rural producer organizations are fundamental building blocks of agricultural development.
And it has helped governments to supervise and regulate cooperative financial institutions.
As we search for innovative solutions to today’s development challenges, we should consider what the cooperative movement can offer.
That means not only greater economic inclusion, higher agricultural productivity, strengthened food security, and financial stability, but also lessons concerning responsible and sustainable business practices, corporate governance, and community relations.
And we should consider how to facilitate the spread of cooperatives’ best practices while avoiding common pitfalls.
The cooperative movement can prompt us to think in new, inspired ways.
Capitalizing on cooperatives’ successes and learning from their mistakes can help us to expand the menu of options as we search for more inclusive and sustainable models of development, and new ways of building and sharing knowledge.
The Copenhagen Panic
COPENHAGEN – A sense of panic is setting in among many campaigners for drastic cuts in global carbon emissions.
It is becoming obvious that the highly trumpeted meeting set for Copenhagen this December will not deliver a binding international treaty that will make a significant difference to global warming.
After lofty rhetoric and big promises, politicians are starting to play the blame game.
Developing countries blame rich countries for the lack of progress.
Many blame the United States, which will not have cap-and-trade legislation in place before Copenhagen.
The United Nations Secretary General says that “it may be difficult for President Obama to come with strong authority” to reach agreement in Copenhagen.
Others blame developing countries – particularly Brazil, China and India – for a reluctance to sign up to binding carbon cuts.
Wherever you turn, somebody is being blamed for Copenhagen’s apparent looming failure.
Yet, it has been clear for a considerable time that there is a more fundamental problem: immediate promises of carbon cuts do not work.
Seventeen years ago, industrialized nations promised with great fanfare in Rio de Janeiro to cut emissions to 1990 levels by 2000. Emissions overshot the target by 12%.
In Kyoto, leaders committed to a cut of 5.2% below 1990 levels by 2010.
The failure to meet that target will likely be even more spectacular, with emissions overshooting by about 25%.
The plan was to convene world leaders in Copenhagen and renew vows to cut carbon while committing to even more ambitious targets.
But it is obvious that even a last-minute scramble to salvage some form of agreement will fare no better in actually helping the planet.
With such a poor track record, there is a need for soul-searching and openness to other approaches.
A realistic “Plan B” does not mean plotting a second meeting after Copenhagen, as some have suggested.
It means re-thinking our strategy.
This year, the Copenhagen Consensus Center commissioned research from top climate economists examining feasible ways to respond to global warming.
Their research looked at how much we could help the planet by setting different levels of carbon taxes, planting more trees, cutting methane, reducing black soot emissions, adapting to global warming, or focusing on a technological solution to climate change.
The Center convened an expert panel of five of the world’s leading economists, including three Nobel Prize winners, to consider all of the new research and identify the best – and worst – options.
The panel found that expensive, global carbon taxes would be the worst option.
This finding was based on a groundbreaking research paper that showed that even a highly efficient global CO2 tax aimed at fulfilling the ambitious goal of keeping temperature increases below 2oC would reduce annual world GDP by a staggering 12.9%, or $40 trillion, in 2100.
The total cost would be 50 times that of the avoided climate damage.
And if politicians choose less-efficient, less-coordinated cap-and-trade policies, the costs could escalate a further 10 to 100 times.
Instead, the panel recommended focusing investment on research into climate engineering as a short-term response, and on non-carbon-based energy as a longer-term response.
Some proposed climate-engineering technologies – in particular, marine cloud-whitening technology – could be cheap, fast, and effective. (Boats would spray seawater droplets into clouds above the oceans to make them reflect more sunlight back into space, reducing warming).
Remarkably, the research suggests that a total of about $9 billion spent implementing marine cloud-whitening technology might be able to offset this entire century’s global warming. Even if one approaches this technology with concerns – as many of us do – we should aim to identify its limitations and risks sooner rather than later.
It appears that climate engineering could buy us some time, and it is time that we need if we are to make a sustainable and smooth shift away from reliance on fossil fuels.
Research shows that non-fossil fuel energy sources will – based on today’s availability – get us less than halfway toward a path of stable carbon emissions by 2050, and only a tiny fraction of the way towards stabilization by 2100.
If politicians change course and agree this December to invest significantly more in research and development, we would have a much greater chance of getting this technology to the level where it needs to be.
And, because it would be cheaper and easier than carbon cuts, there would be a much greater chance of reaching a genuine, broad-based – and thus successful – international agreement.
Carbon pricing could be used to finance research and development, and to send a price signal to promote the deployment of effective, affordable technology alternatives.
Investing about $100 billion annually would mean that we could essentially resolve the climate-change problem by the end of this century.
While the blame game will not solve global warming, the mounting panic could lead to a positive outcome if it means we re-consider our current approach.
If we want real action, we need to pick smarter solutions that will cost less and do more.
That would be a result for which every politician would be happy to accept responsibility.
The Corporate-Tax Conundrum
BERKELEY – The United States now has the highest statutory corporate-income tax rate among developed countries.
Even after various deductions, credits, and other tax breaks, the effective marginal rate – the rate that corporations pay on new US investments – remains one of the highest in the world.
In a world of mobile capital, corporate-tax rates matter, and business decisions about how and where to invest are increasingly sensitive to national differences.
America’s relatively high rate encourages US companies to locate their investment, production, and employment in foreign countries, and discourages foreign companies from locating in the US, which means slower growth, fewer jobs, smaller productivity gains, and lower real wages.
According to conventional wisdom, the corporate-tax burden is borne principally by the owners of capital in the form of lower returns.
But, as capital becomes more mobile, relatively immobile workers are bearing more of the burden in the form of lower wages and fewer job opportunities.
That is why countries around the world have been cutting their corporate-tax rates.
The resulting “race to the bottom” reflects intensifying global competition for capital and technological knowhow to support local jobs and wages.
Moreover, a high corporate-tax rate is an ineffective and costly tool for producing revenues, owing to innovative financial transactions and legal tax-avoidance mechanisms.
A company’s legal residence and geographic sources of income can be and are manipulated for such purposes, and the incentives and scope for such manipulation are especially large in sectors where competitive advantage depends on intangible capital and knowledge – sectors that play a major role in the US economy’s competitiveness.
In the absence of close and broad international cooperation, the US must join the race and lower its corporate-tax rate.
A lower rate would strengthen incentives for investment and job creation in the US, and weaken incentives for tax avoidance. It would also reduce numerous efficiency-reducing distortions in the US tax code, including substantial tax advantages for debt financing over equity financing and for non-corporate businesses over corporate businesses.
But each percentage-point reduction in the corporate-tax rate would reduce federal revenues by about $12 billion per year.
These revenue losses could be offset by curtailing so-called “corporate-tax expenditures” – deductions, credits, and other special tax provisions that subsidize some economic activities while penalizing others – and broadening the corporate-tax base.
Eliminating “special interest” loopholes, like tax breaks for oil and gas, or for corporate jets, would not yield enough revenue to pay for a meaningful rate cut.
And curtailing accelerated depreciation, the manufacturing production deduction, and the R&amp;D tax credit – which account for about 80% of corporate-tax expenditures – would involve significant tradeoffs.
Indeed, cutting these items to “pay for” a reduction in the corporate-tax rate could end up increasing the tax on corporate economic activity in the US.
Eliminating accelerated depreciation for equipment would raise the effective tax rate on new investments; repealing the domestic-production deduction would increase the effective tax rate on US manufacturing; and rescinding the R&amp;D tax credit would reduce investment in innovation.
Instead of cutting proven tax incentives for business investment, the US should offset at least some of the revenue losses from a lower corporate-tax rate by raising tax rates on corporate shareholders.
Most countries that reduced their corporate-tax rates have followed this path, while the US has done the opposite.
At 15%, America’s tax rates on dividends and capital gains are at historic lows, while the profit share of national income is at an all-time high.
Defenders of low rates for capital owners argue that it minimizes “double” taxation of corporate income – first of the corporation and then of its shareholders.
A lower corporate-tax rate would weaken this justification.
Moreover, pension funds, retirement plans, and non-profit organizations, which receive about 50% of all corporate dividends, do not pay tax on these earnings, and would benefit from a lower corporate-tax rate.
Although individual taxes on corporate income reduce the after-tax return to savings, they have less distorting effects on investment location than corporate taxes do, and they are more likely to fall on owners of capital than on workers.
Moreover, it is far easier to collect taxes from individual citizens and resident shareholders than from multinational corporations.
Apple can use sophisticated techniques to manipulate the location of its corporate income, but individual US citizens who own Apple stock have to report the dividends and capital gains that they earn from it in their worldwide income.&nbsp;
A recent study found that taxing capital gains and dividends as ordinary income, subject to a maximum 28% rate on long-term capital gains (the pre-1997 rate), could finance a cut in the corporate-tax rate from 35% to 26%.
Such a change would reduce corporations’ incentives to move investments abroad or shift profits to low-tax jurisdictions, while increasing the progressivity of tax outcomes by shifting more of the burden of corporate taxation from labor to capital owners.
An increase in the corporate-tax rate appeals to many US voters who believe that corporations are not paying their fair share of taxes and are worried about widening income inequality.
But, in a world of mobile capital, raising the corporate tax rate – or simply leaving it at its current level – would be a bad way to generate revenue, a bad way to increase the tax system’s progressivity, and a bad way to help American workers.
The Cost of Dick Cheney
LONDON – George W. Bush has started work on his memoirs.
Count to ten before you respond.
The autobiographies of political leaders are not a very elevated literary form.
First, few leaders write well, though there are exceptions, like Nehru, Churchill, and de Gaulle.
No wonder that most of them employ a “ghost,” like the one in Robert Harris’s excellent thriller of the same name, which is really a devastating critique of Britain’s former premier, Tony Blair.
Second, these memoirs are usually little more than slabs of self-justification interspersed with lists of famous people met in the course of life at the top.
To take one example, while Bill Clinton speaks with warmth, wit, and great eloquence in the flesh, his autobiography is not worth reading.
Third, these books are usually written largely for a big paycheck.
But it beats me how publishers ever recover the huge multi-million dollar advances they hand out.
When the great General George C. Marshall – whose memoirs of World War II and of his tenure as America’s Secretary of State would have been worth every penny – was offered $1 million by a publisher in the1950’s for his autobiography, the old man replied, “Why would I want $1 million?”
What a different world we now inhabit.
The good news about the Bush project, so far title-free, is that it is apparently going to be different from the usual reputation polishing.
Instead of starting at the beginning of his presidency, with all those dodgy Florida voting machines, and plodding on to the bitterly unpopular end, he intends to concentrate on the 20 most consequential decisions he made in the White House.
He will also focus on key moments in his life, such as his decision to give up alcohol and to choose Dick Cheney as his Vice President.
Shaking off his addiction to booze speaks extremely well of Bush, his strength of character and the support of his wife and family.
To turn your back on an addiction is never easy.
Those who do it, helped in Bush’s case by a growing religious faith, deserve sympathy and approval.
It was never Bush’s determination that was in doubt, nor his geniality – despite that slightly annoying rich-boy joshing.
Nor did I ever believe that the former president was stupid: a criticism leveled at him by many of his European peers who were themselves hardly philosopher kings.
The problem with Bush was not lack of intelligence but a complete absence of intellectual curiosity.
He was content to camp on his own shallow prejudices, and the rest of the world had to be fitted into this narrow terrain.
This is where Cheney came in.
It was certainly a key moment when Bush chose him.
Imagine, for example, how different the world and the opinions of Bush’s presidency might have been if he had chosen Colin Powell or John McCain as his running mate?
What Cheney did was to feed and nourish the Bush prejudices, and to move ruthlessly and energetically to occupy the policymaking ground left vacant by the President’s indolence and National Security Adviser Condoleezza Rice’s lack of political clout.
What did Cheney believe?
He thought that Ronald Reagan had shown that fiscal deficits don’t matter.
He believed in capitalism – or at least in supporting big companies and the rich – though whether he understood how free markets should work under the rule of law is more doubtful.
The law was never Cheney’s strong point.
He was an apologist for American power, even though during the Vietnam years he had wriggled and dodged to avoid being at the sharp, conscripted end of it.
He thought that the American president should operate beyond the checks and balances applied by the US Constitution, just as his country should not be constrained by any international rules.
Rules were for others, and at the very end of his term in office his one public disagreement with Bush concerned the President’s refusal to pardon Cheney’s former chief of staff, Scooter Libby, who had been convicted of perjury.
Cheney’s influence resulted in the bloody disaster of Iraq, the moral humiliation of Guantanamo, water-boarding and “extraordinary rendition,” the despair of friends and the contempt of critics, a full-dress parade of double standards around the globe.
Mean-spirited and partisan, Dick Cheney was one of America’s most powerful vice presidents.
I cannot think of one who did so much damage to America at home and to its reputation abroad.
No wonder Bush regards the choice of Cheney as such a key decision.
Ideas matter in politics and having only a few simplistic ones of his own, Bush found his agenda shaped and dominated by his clever surrogate and deputy.
That is what eventually did him in.
Bush’s presidency was discredited and sunk by the man whom he fatally selected to work for him.
The bigger tragedy was that so many others paid a much higher price for this than Bush did.
“The Cost of Dick Cheney” – perhaps that should be the title of Bush’s memoirs.
China’s Interest-Rate Challenge
NEW YORK – China’s successful transformation from a middle-income country to a modern, high-income country will depend largely on the reforms that the government undertakes over the next decade.
Financial reforms should top the agenda, beginning with interest-rate liberalization.
But liberalizing interest rates carries both risks and rewards, and will create both winners and losers, so policymakers must be prudent in their approach.
In 2012, the People’s Bank of China allowed commercial banks to float interest rates on deposits upward by 10% from the benchmark, and on bank loans downward by 20%.
So, if the PBOC sets the interest rate on one-year deposits at 3%, commercial banks can offer depositors a rate as high as 3.3%.
Many analysts viewed this policy, which introduced a small degree of previously non-existent competition among commercial banks, as a sign that China would soon liberalize interest rates further.
But any further move toward interest-rate liberalization must account for all potential costs and benefits.
Chinese policymakers should begin with a careful examination of the effects of current financial repression (the practice of keeping interest rates below the market equilibrium level).
The degree of financial repression in a country can be estimated by calculating the gap between the average nominal GDP growth rate and the average long-term interest rate, with a larger gap indicating more severe repression.
In the last 20 years, this gap has been eight percentage points for China, compared to roughly four percentage points on average for emerging economies and nearly zero for most developed economies, where interest rates are fully liberalized.
Developing-country central banks keep interest rates artificially low to ensure sufficient low-cost financing for the public sector, while avoiding large fiscal deficits and high inflation.
But, in the long run, such low interest rates may also discourage households from saving, lead to insufficient private-sector investment, and eventually result in economy-wide underinvestment, as occurred in many Latin American countries in the past.
In many ways, China is breaking the mold.
Despite severe financial repression, it has experienced extremely high savings and investment, owing mainly to Chinese households’ strong propensity to save and massive government-driven investment, particularly by local governments.
The adverse effects of financial repression in China are reflected primarily in its economic imbalances.
Low interest rates on deposits encourage savers, especially households, to invest in fixed assets, rather than keep their money in banks.
This leads to overcapacity in some sectors – reflected in China’s growing real-estate bubble, for example – and underinvestment in others.
More important, financial repression is contributing to a widening disparity between state-owned enterprises (SOEs) and small and medium-size enterprises (SMEs), with the former enjoying artificially low interest rates from commercial banks and the latter forced to pay extremely high interest rates in the shadow-banking system (or unable to access external financing at all).
Interest-rate liberalization – together with other financial reforms – would help to improve the efficiency of capital allocation and to optimize the economic structure.
It might also be a prerequisite for China to deepen its financial markets, particularly the bond market, laying a solid foundation for floating the renminbi’s exchange rate and opening China’s capital and financial accounts further – a precondition for the renminbi’s eventual adoption as an international reserve currency.
SMEs and households with net savings stand to gain the most from interest-rate liberalization.
But financial repression’s “winners” – commercial banks and SOEs – will face new challenges.
Under the current system, the fixed differentials between interest rates on deposits and those on loans translate into monopolistic profits for commercial banks.
(The three percentage-point differentials that Chinese banks have enjoyed are roughly on par with those of their developed-country counterparts.)
By creating more competition for interest income and reducing net interest-rate differentials, liberalized interest rates could reduce banks’ profitability, while SOEs will likely suffer the most, owing to much higher financing costs.
Another major risk of interest-rate liberalization in China stems from rising public debt, particularly local-government debt, which has grown significantly in the wake of the global financial crisis.
A key parameter for determining the long-run sustainability of public debt is the gap between interest rates and the nominal GDP growth rate.
In China, total public debt currently amounts to roughly 60-70% of GDP – a manageable burden.
But, after interest rates are liberalized, the public sector’s debt/GDP ratio is expected to increase substantially.
Given these challenges, China’s leaders must take a cautious approach to interest-rate liberalization.
Gradual implementation would enable the losers to adjust their behavior before it is too late, while sustaining momentum on pivotal reforms, which should be policymakers’ top priority.
After all, as Premier Li Keqiang has put it, “reforms will pay the biggest dividend for China.”
David Cameron’s Euro-Nemesis
LONDON – Unlike some in Britain’s Conservative Party, Prime Minister David Cameron has not previously given the impression of being obsessed with Europe.
He demonstrated no enthusiasm for the European Union, but he appeared clearly less exercised by its supposed iniquities than many Tories are.
This view of Cameron’s position is now difficult to sustain.
His long-gestating speech on Europe, although containing elements that many might share, also sows the seeds for a prolonged and acrimonious debate – and not just in Britain.
Conservatives in the House of Commons (and in the wider party) want to be reassured that their leader shares their antagonism for the entire European integration process.
They have not forgotten or pardoned his “treachery” in refusing to hold a referendum on the Lisbon Treaty, signed by his predecessor, Gordon Brown.
With his speech, that reassurance may now have been given.
Cameron, of course, faced a difficult task with his party, which required a statement from him of his European policy.
Cameron then had to find something appropriate to say.
He needed to placate Tories and his domestic critics while avoiding the economic and political havoc that would be caused by announcing an imminent referendum that might lead to the United Kingdom’s withdrawal from the EU.
The time that he took to decide what he would say attests to the difficulty of squaring that circle.
In fact, as Cameron’s speech made clear, his solution to his dilemma – to buy himself short-term peace from his critics at the expense of potentially making his (and Britain’s) problems more intractable in the long term – is hardly new.
It was already clear that Cameron wanted to push any possibility of a referendum into the most distant possible future.
The idea that he would seek to renegotiate the terms of Britain’s EU membership is also familiar from his earlier speeches and interviews.
Now that position has been bluntly and uncompromisingly expressed.
The demand for far-reaching change in the structure and functioning of the EU, including repatriation of powers to Britain, is a major new demarche at a difficult time for Europe.
Cameron has said on several occasions that he wishes to avoid a referendum revolving around the simple choice of continued EU membership on the basis of the current terms of membership.
Already some are claiming to discern in his European policy the makings of an heir to Harold Wilson, another famous “renegotiator” of Britain’s terms of membership in the then-European Community who went on to win a referendum on Europe.
Britain’s relationship with European integration has been a difficult one, regardless of which party has been in power (Wilson, after all, was a Labour prime minister).
This was inevitable from the outset, owing to Britain’s deep and irreconcilable disagreement with virtually all other EU member states on the fundamental issue of pooling sovereignty.
Essentially, the British point of view has been that a loose confederation of nation-states cooperating on trade is as much Europe as the UK needs.
But Britain joined the European Community, not just the free-trade area that Cameron now apparently wants.
Nonetheless, the undertow of Euro-skepticism in British politics has never diminished and was evident in Cameron’s speech.
Even the supremacy of European law in defined areas was accepted only reluctantly by Britain, and long after many others had done so.
Indeed, in his speech, Cameron could not resist a passing shot at the European Court of Justice.
Britain has made major positive contributions to Europe, particularly with respect to the single market.
But it is no exaggeration to state that whenever Britain has perceived an opportunity to wage a war of attrition against the European supranational project, it has done so, opposing any substantial increase in the EU’s competences or resources.
Given that this position reflects the British public’s attitude toward the EU, it is not surprising.
But it nonetheless distresses other member states, particularly those, like Germany, that recognize the great benefit of having a country with a strongly pro-free trade position and a deep commitment to the rule of law play an important role in the EU.
The prolonged period of renegotiation now proposed by Cameron implies high costs for both sides.
For starters, it creates a source of deep and prolonged uncertainty at a time when the eurozone crisis already has called into question the EU’s long-term health, if not its survival.
Moreover, Cameron’s strategy seems unlikely to lead to an outcome that satisfies anyone.
If it is intended to be a negotiation that takes place in the context of broader treaty talks, it may not happen in the foreseeable future.
European Council President Herman Van Rompuy, among others, seems to doubt the need for a new treaty, which would require the unanimous support of the member states – some of which are sharply opposed – to enter into force.
Indeed, Cameron recognized this explicitly in his speech, so the new treaty to embody a “new settlement” for Britain may have to be negotiated with all member states as a separate exercise.
Part of this negotiation apparently would entail a repatriation of powers, requiring the consent of all EU members – and making the conditions under which Cameron’s renegotiation is supposed to take place both legally and politically uncertain.
Many European politicians would view a member state’s repatriation of competences as a totally destructive precedent, and thus would oppose it resolutely.
The net result is that it seems highly probable that any attempted achievement of a “new settlement,” including repatriation of competences, will make it much more difficult for Britain to remain in the EU than would be the case if a straightforward “in/out” referendum were held now.
So, far from reassuring anyone (including Tory Euro-skeptics), Cameron’s stance heralds a new era of turbulence and uncertainty for Britain and its European partners.
Overcautious Obama
PRINCETON – In this election season in the United States, President Barack Obama is two men in one.
The Obama of the Cairo speech of 2009, when he called for a new beginning between the United States and Muslims around the world, has been increasingly eclipsed by Obama the terrorist-slayer, the commander-in-chief who has launched hundreds of drone strikes against Al Qaeda and its affiliates and who ordered the killing of Osama bin Laden.
Commander-in-chief Obama is doing what he thinks is necessary to keep Americans safe, but he is ignoring the deeper roots of US security that the Cairo Obama understood so well.
It may well be necessary for other Muslim countries to hold him to account.
Consider Syria.
Everything happening there was both predictable and predicted: a proxy war between Saudi Arabia and Iran, increasing sectarianism and ethnic segregation, the polarization of extremes and the silencing of moderates, de-stabilization of neighboring countries, infiltration by terrorist groups, and a bloodbath from which the country could take decades to recover. Syrian opposition groups beg for the kinds of weapons needed to fight President Bashar al-Assad’s planes, defend hard-won territory, provide safety for civilians, and signal to Assad that the world will not stand by as he does whatever it takes to subdue his own people.
Every morning, Obama receives a briefing from men who warn him of every plot and conspiracy to kill Americans.
He knows that any weapon capable of shooting down a Syrian warplane could also be used by a terrorist against a US airliner.
He believes that he is doing the right thing and following the prudent course in waiting for the Syrian conflict to burn itself out in some way while minimizing the long-term risk to American lives.
Other advisers warn him that any possible action – a safe zone on the Turkish border, for example, that could expand outward – requires taking out anti-aircraft defenses all over Syria.
That would mean bombing Damascus, which could harden Syrian support for Assad.
And perhaps it would do no good; it might save a few lives, they argue, but, with so many different countries fighting a proxy war via so many different groups of opposition fighters, it would not change the overall dynamic of the conflict.
Moreover, other countries in the region will not use offensive force without approval by the United Nations Security Council, which the Russians and Chinese continue to block.
And let’s not forget that US voters have no appetite for more military action in the Middle East, even if the price is years of civil war and the implosion and fragmentation of a country bordering Israel, Jordan, Iraq, Turkey, and Lebanon.
These arguments are advanced in good faith and deserve serious consideration. But the art of leading rests on calculating costs and benefits and exercising judgment when the balance is unclear.
No one speaks for the Syrian people in the Oval Office every morning.
No one adds up the costs of betraying, yet again, what America claims to stand for, even while witnessing people willing to march – in the face of bullets – for precisely those universal values: dignity, freedom, democracy, and equality.
The cost is yet another generation of Middle Eastern youth who will believe the worst about the US, no matter how far-fetched the rumor or extreme the claim.
No one points out the huge opportunity cost of what could have been and what could still be – albeit barely – if the US took decisive action to save tens of thousands of Syrian lives and possibly tip the balance of the conflict.
Consider the contrast with Libya, where one of the outcomes of US intervention was tens of thousands of Libyans marching in the streets with placards declaring their support of the US and their outrage and sorrow at the murder of the US ambassador.
And, on the world stage, Russia and China have been encouraged to believe that America will never push past a veto, effectively giving them the final word.
It may well be impossible to get the US to act before its presidential election in November.
The only chance, in my view, is if countries in the region – Turkey, Saudi Arabia, Jordan, Qatar, and the United Arab Emirates – call openly for US leadership.
They should remind Obama of what foreign-policy experts Nina Hachigian and David Shorr have recently called “the responsibility doctrine”: great powers have an active responsibility to uphold global norms and solve global problems.
The Arab League should publicly charge the Security Council with abdicating its responsibility for preserving international peace and security and call on countries with important interests in the region to join with them in taking action.
They should specifically call on the US to assume the mantle of global responsibility and, in the phrase that Obama used to describe the intervention in Libya, “create the conditions and coalitions for others to step up.”
The League would be asking the US to live up to its values and pursue its interests, while at the same time fulfilling its own responsibility as a regional organization.
The devastating and widening conflict in Syria does not present any good choices, only choices between bad and worse.
But a leader must choose, and Obama is making the wrong choice – for Syria, for the region, and for the US.
The Cracks in the G-20
MADRID – The world financial crisis has served as a quick and efficient catalyst to the G-20.
The first three G-20 summits of chiefs of state, in Washington, London, and Pittsburgh, will be remembered for advancing multilateralism and coordinated global action.
But the G-20 remains very much a work in progress – and one that needs much work to succeed, as its most recent summit in Toronto demonstrated.
The G-20 summit in Washington in 2008 was the first at which the member countries’ chiefs of state met since the group’s creation in 1997.
The G-8 was no longer an appropriate vehicle for global economic governance, given the need to stabilize financial markets around the world.
The voices of countries such as China, India, and Brazil had to be heard if a coordinated response to the crisis was to be found.
With the financial crisis worsening, the London summit in 2009 agreed to unprecedented fiscal and monetary stimulus and backed a stronger, more coherent regulatory and supervisory framework worldwide.
In view of the G-20’s success, the Pittsburgh summit recognized it as the main forum for international economic cooperation.
This recognition raised expectations for the G-20 and granted it the prestige that it deserved: it is the only forum in which world powers and emerging countries sit as equals at the same table.
The premise is clear: as the crisis made more evident than ever, the interdependence of countries is inescapable. In the face of today’s global challenges, the only possible response must be global.
There is no possible alternative. But the imprecision of the agreements reached at the summit in Toronto in June has left political leaders with a bitter taste in their mouths.
Two clearly defined gaps stand out as causes of discord. The first is transatlantic divergence over how best to assure a return to solid growth.
The United States favors continuing economic stimulus, while the European Union prefers fiscal consolidation.
The other source of dissension is disagreement over a bank tax.
The US, the EU, and Japan are in favor, while emerging countries, as well as Canada and Australia, are opposed.
While an agreement has been reached (2013 has been set as the year to reduce budget deficits by half; 2016 to stabilize sovereign debt), consensus is not headed in the right direction.
It is not a question of stimulus versus deficit. Both are necessary.
Even respecting the idiosyncrasies of each context, there is still sufficient common ground for more precision in the agreements.
The same can be said for transparency, accountability, and regulation of the bank tax.
I know very well that this is not an easy task, but it is essential for world leaders – not markets – to lead reforms.
Moreover, a custom has been repeated that should be changed.
Obviously, holding a G-8 summit just before a G-20 summit, as happened in Canada this June, simply serves to prolong maintenance of separate clubs, which is unsustainable.
The G-20’s role should increase further in importance, owing to emerging countries’ share of global GDP – projected to reach 60% in 2030 – and to the global nature of the challenges of the twenty-first century.
If we want to make progress on global governance problems we will have to work together on getting through this economic crisis and on other essential issues, such as nuclear non-proliferation.
The problem is that, despite the clear need for multilateralism, there is a risk of a relapse into bilateralism due to a lack of global leadership.
The attention of US President Barack Obama is centered on matters of great importance, such as the Middle East, his evolving strategy in Afghanistan, and the struggling American economy.
The same is true of the EU, where attention – and action – has been centered in recent months on defending the euro and resolving the economic difficulties on the Union’s periphery.
Meanwhile, the emerging powers continue to lean toward bilateralism and are aligning themselves with other countries.
Lack of agreement on the United Nations Security Council resolution against Iran does not help coordination and cooperation within the G-20, either.
Summits should be well prepared and provide a forum to debate today’s great global issues.
Coherent, well-defined, and precise proposals would produce results that are more acceptable to all.
But just as important as making decisions is explaining them well. A G-20 summit is not something that happens every day.
It is a global event. Especially in a time of crisis that has caused so much suffering, its decisions must be explained to the public clearly and without cacophony.
People’s anguish demands this effort, and it was lacking in Toronto.
The world remains in a very delicate transitional phase, and it is not clear yet in which direction the G-20 will lean.
The main challenge now is to continue using the “geometry of 20” to build instruments of world governance.
Although the economic storm has become less intense, it hasn’t calmed yet.
So there remains much to do.
With countries headed toward growth at different speeds, global strategy must continue to be a priority.
The degree of interdependence among countries is increasing, and the global nature of our problems is inherent.
Within the framework of multilateralism, countries must make the effort to smooth over their differences and deepen their relations: we must conquer the inertia that moves us towards old thinking – and old alliances.
The Crimes of Ratko Mladić
NEW YORK – Ratko Mladić is an easy man to hate.
In his prime, he not only talked and behaved like a thug, but he also looked like one – the kind of bull-necked, pale-eyed, snarling psychopath who would gladly pull out your fingernails just for fun.
Apart from many other cruelties, the Butcher of Bosnia was responsible, in the summer of 1995, for the killing of around 8,000 unarmed Bosnian Muslim men and boys in the woods around Srebrenica.
So it will give most of us a feeling of warm satisfaction that he has finally been arrested in the Serbian village of Lazarevo.
Serbia has gained respect by arresting Mladić, which should speed up its membership in the European Union.
The former victims of Mladić’s Bosnian Serb forces will feel that some justice is being done at last.
Yet the forthcoming trial of Ratko Mladić raises certain uncomfortable questions.
Why, in the first place, can’t he be put on trial in Belgrade, instead of The Hague?
And is it really wise to charge him with genocide, as well as crimes against humanity and war crimes?
Both questions reveal how much we still live in the shadow of the Nuremberg Tribunal, where the Nazi leaders were tried by an international judicial panel.
It was believed, perhaps correctly, that the Germans would be incapable of trying their own former leaders.
And the Nazi crimes had been so horrendous in scale and intent that new laws – “crimes against humanity” – had to be created to try those who had been formally responsible for them.
States, too, should be held accountable for their deeds – hence, in 1948, the Convention on the Prevention and Punishment of the Crime of Genocide.
The Holocaust was not the main issue at stake in the Nuremberg Trials.
Nevertheless, the allies thought that the Nazi project of exterminating an entire people called for an entirely new legal approach, to ensure that such an atrocity would never happen again.
The problem with genocide, as a legal concept, is that it is vague.
It refers to “acts committed with intent to destroy, in whole or in part, a national, ethnical, racial, or religious group.” The emphasis is on “intent,” not on the numbers of people whose lives are destroyed.
Mao Zedong murdered up to 40 million Chinese, but did he intend to destroy them as a group?
Surely not.
We know that Hitler did intend to destroy every last Jewish man, woman, and child.
Even though mass killings are not rare in history, Hitler’s extermination plan was, if not unique, certainly highly unusual.
However, the laudable effort to prevent such a thing from recurring has had unfortunate consequences.
For, in our zeal to learn lessons from history, we often learn the wrong ones, or distort history towards dubious ends.
In a way, the killings at Srebrenica also were affected by the memories of World War II.
The United Nations’ Dutch battalion promised to protect the Muslims there, even though it was in no position to do so.
It was a promise that partly reflected the feeling of guilt that still haunts the Dutch for looking the other way as the Germans rounded up and deported two-thirds of their country’s Jewish population to death camps.
This time, it would be different. This time, they would act.
Alas, outnumbered and outgunned by Mladić’s forces, the Dutch surrendered Srebrenica to its fate.
Because of the trauma of Hitler’s intention to murder all of the Jews, genocide has become the one compelling reason for military action, including armed invasion of other countries.
But what constitutes genocide?
Bernard Kouchner, the founder of Doctors Without Borders, wanted the world to intervene in Nigeria in 1970, because he saw the killing of Ibos by Nigerian troops as a genocidal echo of Auschwitz.
Others saw a brutal civil war, and cautioned that intervention would make things worse.
For some, we are forever living in 1938, or rather, 1942, when the Nazis approved what Hitler called “the final solution of the Jewish question.”
President George W. Bush and his cheerleaders, invoking the Munich Agreement at every opportunity, regarded the terrorist attacks of September 11, 2001, as a call to arms.
Military action can cause more violence, and more civilian deaths.
This is especially true of intervention in civil wars, where the sides cannot easily be divided into victims and aggressors, good and evil.
Of course, the world becomes much simpler if we choose to see it in black and white. And the Mladić trial will, no doubt, encourage this perception.
He will be tried for genocide, because the UN’s tribunal for ex-Yugoslavia and the International Court of Justice decided that the Bosnian Serbs were genocidal.
Since his subordinate, Radislav Krstić, was already sentenced for his complicity in the genocide at Srebrenica, Mladić will presumably be convicted.
We need not feel sorry for Mladić. There is no doubt that he is guilty of serious war crimes.
And a trial, however unsatisfactory, is in most cases still to be preferred to an assassination.
But trying him for genocide, even though it will be hard to prove that he ever intended to exterminate Bosnian Muslims as a group, just because they were Muslims, will further muddy the term’s already vague definition.
Mladić was engaged in ethnic cleansing, which, though reprehensible, is not the same as genocide.
Loose definitions will encourage more military interventions, thus more wars.
By invoking Hitler’s ghost too often, we trivialize the enormity of what he actually did.
The Crises of Summer
PRINCETON – Europe’s crisis is now poised at the moment that divides recovery and renewal from decline and death.
Whereas a few weeks ago, commentators and financial analysts argued that only a few months remained to rescue Europe, leading politicians, lurching from summit to summit, have recently talked in terms of days.
Summer crises are a familiar feature of European history – and of financial history.
Indeed, the twentieth century was shaped by three summer crises, whose seriousness was heightened in each case by the absence of major policymakers, who were on vacation.
In two years, Europeans will commemorate the centennial of the assassination of Archduke Franz Ferdinand on June 28, 1914, and the subsequent “July crisis” that triggered World War I that August.
On July 13, 1931, the German banking system collapsed, ensuring that what was previously an American economic downturn became the worldwide Great Depression.
On August 15, 1971, President Richard M. Nixon ended the United States’ commitment to a fixed gold price, leading to a decade of global currency instability.
Each of these crises involved a highly technical issue, but also a much broader set of political problems.
And, in each case, the intertwining of the technical and the political produced disaster.
In July 1914, diplomats were trying to devise a solution that would permit the Habsburg Empire to deal with the cross-border police investigation that was inevitable after a terrorist attack.
Political leaders were thinking about national revival and assertion.
In 1931, the experts were preoccupied with the complexities posed by the combination of reparations and war debts arising out of World War I with large private-sector indebtedness.
Populist political movements in many countries were still thinking about national revival and assertion.
In 1971, the technical issue concerned the role of the dollar in the international monetary system.
But politicians in other countries also felt uncomfortable about the continuing centrality of the US in the postwar order.
In each of these summer crises, addressing the technical issue was not enough to solve the problem.
That is true today as well.
Indeed, Europe’s current crisis reflects exactly the same mixture of elements, each requiring a different type of solution.
On the one hand, a complex set of national fiscal crises and Europe-wide banking problems calls for a comprehensive and detailed rescue operation.
On the other hand, an underlying European governance problem – at both the national level and that of supranational European Union institutions – has been intensifying since the early 1990’s.
What is now required to resolve the technical issue is some mechanism for assuming existing debt and preventing excessive borrowing in the future.
In the US, Alexander Hamilton famously negotiated the federal assumption of states’ debt in 1790, but many states behaved badly in the early nineteenth century, with multiple bankruptcies, until they adopted laws or amendments to their constitutions requiring balanced budgets.
The EU needs some fiscal authority of its own if it is to make Europe’s economic and monetary union work.
It is already a profound peculiarity that customs duties in a customs union are still administered nationally.
Hamilton made federal customs houses the key element of his proposal.
A Europeanization of some part of value-added tax would be a tremendous advance in combating the massive fraud that the existing system nurtures.
Labor mobility is also incomplete without a common pensions and benefits system: under current arrangements, a worker who spends five years in France, five years in Greece, and five years in Germany is left with a fragmented collection of small entitlements.
The crisis has already increased the extent of such migration within Europe.
But any solution will be unacceptable unless it finds broad acceptance across Europe, in debtor and creditor countries alike.
There is no reason why a constitutional solution that involves debt limitation should not command a large measure of public acceptance, especially in debtor countries, which have experienced the political and economic damage caused by previous profligate governments.
What has produced the populist backlash is the spectacle of political authorities devising technically complicated solutions that lack credibility.
Simply put, the experts need to stop treating Europe’s citizens as if they were stupid.
That is why Europe needs a longer-term constitutional renewal, through new treaties, as much as it desperately needs a short-term fix.
Working around existing treaties just looks like more of the same old recipe – a denial of a massive problem that everyone sees.
The public can be forgiven for choking on this kind of stuff.
Consider the European crises that produced good outcomes.
On June 16, 1940, Winston Churchill proposed a Franco-British political union in the aftermath of the German invasion of France.
A decade later, West German Chancellor Konrad Adenauer proposed a Franco-German political union.
That is the kind of boldness that is now needed.
In the past, it was war, immense dislocation, and suffering that could weld nations.
Is Europe’s current crisis severe and dislocating enough to generate an analogous effect?