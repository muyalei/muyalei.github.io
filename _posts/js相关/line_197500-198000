But there has, at last, been real movement towards openness – progress recognized by the European Union when it gave Romania the green light to join the European Union at the beginning of 2007.
Aside from achieving what the EU now deems a “functioning market economy,” key political and legal changes, which I have overseen as Minister of Justice, range from increased transparency and control in the funding of political parties to a shakeup of the judiciary.
Judicial reforms are, in turn, helping to root out corruption.
Indictments have been issued against former and current cabinet ministers, members of parliament, judges, prosecutors, lawyers, police and customs officers, and other public officials, as well as directors of private companies.
Specifically, 60% of the requests filed in Romania met, compared to 31% in France and 24% in Spain.
Other countries that performed well include Peru and Mexico, both of which adopted freedom of information laws in 2002, shortly after Romania.
But getting access to sensitive information in transitional democracies is not always easier, as I know from my previous work as a human rights lawyer with the Romanian Helsinki Committee.
Often we had to go to court to force disclosure of information, using Romania’s 2001 Freedom of Information Act.
For example, even when we won a case regarding access to the records of wiretaps authorized by the general prosecutor, the prosecutor simply ignored the court order.
We filed a civil action against the prosecutor, and the judge imposed a fine for every day the information was withheld.
But it was only when we caught the media’s attention that the data – detailing the number of wiretaps authorized over the previous 10 years, against whom, and for how long – were released.
With its publication, Romania began to move away from its Securitate-dominated past.
Now, inside government, I realize that sharing information with the public is sometimes hard.
But when painful reforms are necessary, there is no alternative.
We could not have achieved the economic and political reforms that qualified us for EU membership if we had not subjected policymaking to public scrutiny and accepted the increased public participation in decision-making that inevitably accompanies such openness.
Indeed, this has become a sine qua non of democratic government throughout the world.
When the United States adopted its Freedom of Information Act in 1966, it joined the exclusive company of Sweden and Finland.
Today, roughly 65 countries have such laws.
In Romania, those who fought for a freedom of information law have made full use of it.
As part of the Justice Initiative study, those who request information from Romania’s government were willingly supplied with the kind of information that would have been unthinkable to release just a few short years ago.
The Romanian Defense Ministry, for example, disclosed the number of armed forces personnel who died in 2003, as well as the causes of death (including 13 suicides, two shootings, and two combat deaths in Afghanistan).
Likewise, the Bucharest court was asked for the number of judges disciplined since the beginning of 2000, including grounds for any sanctions applied.
The Court transferred the request to the Superior Council of Magistrates, which provided a full response: a four-page list of all sanctioned judges, with details of the reasons and penalties.
Such information is both a product and a motor of continuing reform.
It has not always been comfortable for those in government, and there is still a tug-of-war over sensitive documents.
But, in the former communist bloc, the benefits brought by transparency have been undeniable.
At the same time, Romania’s experience demonstrates that official secrecy remains a threat to the core values of democratic governance, and that only constant vigilance, in both established and young democracies, can prevent its encroachment.
The Seven-Year Ditch
PRINCETON – There are historical precedents for sovereign-debt defaults by the countries of Europe’s southern periphery, but they are not instantly attractive ones.
Dealing with seemingly intractable problems often takes time. And it is difficult – especially in a democracy – to be patient.
The most obvious parallel to Europe’s current woes is the Latin American debt crisis of the 1980’s.
In August 1982, Mexico threatened to default, and was quickly followed by other large borrowers, notably Argentina and Brazil.
A default contagion would have brought down the banking systems of all the major industrial countries, and caused the world to relive something like the financial crisis of the Great Depression.
What followed was a seven-year play for extra time.
The initial approach was to link policy improvements in the borrowing countries not only with help from international institutions, but also with additional lending from the banks – which seemed to defy the most elementary canons of sensible bank behavior.
Three years after the outbreak of the Latin American crisis, United States Treasury Secretary James Baker announced a systematization of the initial response.
It was not very imaginative: Banks and multilateral development institutions should all lend more, and the debtors should continue their efforts to improve their macroeconomic policies.
The Baker Plan was a universal disappointment.
Growth faltered again, and the International Monetary Fund actually reduced its lending.
More than three years passed before new US Treasury Secretary Nicholas Brady set out a more satisfactory program, in which banks would be given a menu of options that included lower interest rates on the debt and a hefty discount on the principal.
If creditor banks were unwilling to accept some form of restructuring, they would have to put in new money.
The lending of the international institutions might also be used for buying back discounted debt.
Brady’s plan looked like a great success.
Confidence returned, capital flight from Latin America was reversed, and capital markets became willing to provide financing again.
Inevitably, the Brady Plan looks like a good model for southern Europe.
Why not avoid seven years of misery, and begin some similar rescue operation now that could lead to a return to economic vigor and dynamism?
The most obvious answer is that at an earlier stage in the Latin American saga, the banks simply could not have afforded to take such losses on their capital.
They needed to fake it for seven years in order to build up adequate reserves against losses.
The initiative for the Brady Plan did not come from the official sector at all.
It was the willingness of some large financial institutions to trade in discounted debt that established a market that could clear out the legacy of past mistakes.
Two institutions, in particular, took the lead: Citicorp in the US and Deutsche Bank in Europe.
Their CEOs at the time presented their actions as being motivated by far-sighted benevolence and a concern for the well-being of the world as a whole.
That may have been plausible, but the two banks also wanted to demonstrate publicly that they had better balance sheets than their weaker rivals.
In Germany, the Dresdner Bank and the Landesbanken could not afford to take such a hit.
Moreover, despite the obvious “reform fatigue” of Latin American electorates, the debtor countries had engaged in a substantial measure of reform.
Before the Brady Plan was announced, Mexico had accepted a wide-ranging Pact of Economic Solidarity and Growth, which immediately improve investor confidence and reduced sky-high domestic interest rates.
The final move to solve the debt problem came in the aftermath of international currency adjustment.
One of the problems that had made the crisis more difficult after 1982 was the US dollar’s appreciation.
Likewise, today’s euro crisis is harder to resolve because of the euro’s strength in currency markets.
The dollar’s slide after 1985 made the real burden of Latin America’s dollar debt much lighter.
Calling for a European Brady plan today does not guarantee the necessary conditions for such a plan to succeed.
On all fronts, there has not been much progress.
There is certainly plenty of frustration about the implementation of austerity, and no real indication of the long-term sustainability of reform efforts in southern Europe.
The problems of Europe’s banks are also far from being resolved.
A truly competitive European banking system would provide incentives for the larger and stronger banks to take more risks in the hope of growing even larger and stronger.
But, in the aftermath of the financial crisis, policymakers are too pre-occupied by the real problems posed by too-big-to-fail banks, and too terrified by the potential collapse of weaker banks, to allow such a solution.
Recapitalization has not yet reached the point where there are enough strong banks.
Finally, global uncertainty about currencies stands in the way of a solution.
The euro’s strength, despite the magnitude of the eurozone’s difficulties, makes an export-led recovery strategy harder to realize.
The common currency’s strength reflects problems elsewhere in the world, but a euro exchange rate that would enable the return of confidence and growth is no less elusive for that.
Without the preconditions that made the Brady Plan work, simply transplanting a debt write-off would only augment uncertainty and fuel the political revulsion that already threatens to undermine European integration.
And it may be that Europe – because of its relative prosperity – is less in a mood for the biblical seven lean years than was a much poorer Latin America in the 1980’s.
The Shadow of Depression
BERKELEY – Four times in the past century, a large chunk of the industrial world has fallen into deep and long depressions characterized by persistent high unemployment: the United States in the 1930’s, industrialized Western Europe in the 1930’s, Western Europe again in the 1980’s, and Japan in the 1990’s.
Two of these downturns – Western Europe in the 1980’s and Japan in the 1990’s – cast a long and dark shadow on future economic performance.
In both cases, if either Europe or Japan returned – or, indeed, ever returns – to something like the pre-downturn trend of economic growth, it took (or will take) decades.
In a third case, Europe at the end of the 1930’s, we do not know what would have happened had Europe not become a battlefield following Nazi Germany’s invasion of Poland.
In only one instance was the long-run growth trend left undisturbed: US production and employment after World War II were not significantly affected by the macroeconomic impact of the Great Depression.
Of course, in the absence of mobilization for WWII, it is possible and even likely that the Great Depression would have cast a shadow on post-1940 US economic growth.
That is certainly how things looked, with high levels of structural unemployment and a below-trend capital stock, at the end of the 1930’s, before mobilization and the European and Pacific wars began in earnest.
In the US, we can already see signs that the downturn that started in 2008 is casting its shadow on the future.
Reputable forecasters – both private and public – have been revising down their estimates of America’s potential long-run GDP.
For example, labor-force participation, which usually stops falling and starts rising after the business-cycle trough, has been steadily declining over the past two and a half years.
At least some monetary policymakers believe that recent reductions in the US unemployment rate, which have largely resulted from falling labor-force participation, are just as valid a reason for shifting to more austere policies as reductions in unemployment that reflect increases in employment.
And much the same processes and responses are at work – with even greater strength – in Europe.
Most important, however, has been what looks, from today’s perspective, like a permanent collapse in the risk-bearing capacity of the private marketplace, and a permanent and large increase in the perceived riskiness of financial assets worldwide – and of the businesses whose cash flows underpin them.
Given aging populations in industrial countries, large commitments from governments to social-insurance systems, and no clear plans for balancing government budgets in the long run, we would expect to see inflation and risk premiums – perhaps not substantial, but clearly visible – priced into even the largest and richest economies’ treasury debt.
Sometime over the next generation, the price levels of the US, Japan, and Germany might rise substantially after some government short-sightedly attempts to finance some of its social-welfare spending by printing money.
The price levels are unlikely to go down. Yet the desire to hold assets that avoid the medium-term risks associated with the business cycle has overwhelmed this long-run fundamental risk factor.
But the risk that the world’s investors currently are trying to avoid by rushing into US, Japanese, and German sovereign debt is not a “fundamental” risk.
There are no psychological preferences, natural-resource constraints, or technological factors that make investing in private enterprises riskier than it was five years ago.
Rather, the risk stems from governments’ refusal, when push comes to shove, to match aggregate demand to aggregate supply in order to prevent mass unemployment.
Managing aggregate demand is governments’ job.
While Say’s law – the view that supply creates its own demand – is false in theory, it is true enough in practice that entrepreneurs and enterprises can and do depend on it.
If the government falls down on the job, John Maynard Keynes wrote 76 years ago, and “demand is deficient…the individual enterpriser...is operating with the odds loaded against him.
The game of hazard which he plays is furnished with many zeros,” which represent “the increment [by which] the world’s wealth has fallen short of...savings,” owing to “the losses of those whose courage and initiative have not been supplemented by exceptional skill or unusual good fortune.
But if effective demand is adequate, average skill and average good fortune will be enough.”
For 62 years, from 1945-2007, with some sharp but temporary and regionalized interruptions, entrepreneurs and enterprisers could bet that the demand would be there if they created the supply.
This played a significant role in setting the stage for the two fastest generations of global economic growth the world has ever seen.
Now the stage has been emptied.
The Shadow of the Crescent
NEW YORK — As Pakistan atrophies in its existential crisis, a fundamental question about the nature of the country is coming to the fore: Are the country’s citizens Pakistanis who happen to be Muslims, or are they Muslims who happen to be Pakistanis?
Which comes first, flag or faith?
It is not a question that many Pakistanis can readily answer.
The vast majority of the country’s so-called “educated elite” seem to have no qualms about identifying themselves as Muslims first and Pakistanis second.
Some feel that their religion is the most important thing to them, and that that’s where their first loyalty will always lie.
Others admit to having scant regard for religion, but say that Pakistan has come to mean so little to them that their religion supersedes their loyalty to the country.
This willingness to subordinate state to God, even among the highly educated, lies at the heart of Pakistan’s crisis.
How can a country be expected to prosper if the majority of its citizens harbor only a secondary allegiance to the state?
How can it progress if, as the noted author M.J. Akbar wrote, “the idea of Pakistan is weaker than the Pakistani.”
But what is the idea of Pakistan?
Back in the heady days of the 1940’s, Mohammed Ali Jinnah rallied a people to nationhood.
Despite his Anglophone status and Victorian manners, he carved out a separate homeland for India’s Muslims.
But, today, an erudite, westernized lawyer like Jinnah who isn’t a wadhera or a jagirdar would find it impossible to win a popular election in Pakistan.
For the real Jinnah is now irrelevant in the country that reveres him as “Quaid-e-Azam,” or founder of the nation.
Few Pakistanis have the time or inclination to think about their founder’s ideas.
Jinnah’s idea of Pakistan – South Asian Muslim nationalism – has been overrun by the dogma of Islamic universalism.
The modern Pakistani identity is shaped largely by the negation of an Indian-Hindu identity and the adoption of a global pan-Islamic charter. Economic advancement is taken to mean Westernization or worse, Indianization.
At every turn, Pakistanis seem more likely to unite as brothers in Islam than as sons of the same soil.
Moreover, Pakistan’s fear of vilification and failure has given birth to an increasingly paranoid brand of Islam that seeks to impose stricter controls – on education, women’s rights, dancing, beardlessness, and sex – and close society to all forms of modernity.
This paranoid Islam, represented by hard-line outfits like the Tablighi Jamaat, is Pakistan’s fastest-growing brand of faith.
Pakistan is now at a crossroads, facing an uneasy moment of truth.
To survive, its citizens must act in unison or risk seeing every moderate tendency in the country purged by a clamor of illiberal, religious voices.
Today’s crisis calls for every thinking Pakistani to ask serious questions of themselves: What should be the idea of Pakistan?
Are you Pakistanis who happen to be Muslims, Christians, or Hindus? Or are you members of a global Islamic ummah who just happen to live in Karachi or Lahore?
The real challenge, and the ultimate solution, is to get people to think and talk about these questions.
But this must be a debate between people, and within people.
Nothing will be solved by searching for the “true Islam” or by quoting the Koran.
The point is that eventually, despite strong regional loyalties and various cultural and religious differences, the majority can identify as being simply “Pakistani” – even though they may harbor radical differences about what this might mean.
The real idea of Pakistan, ultimately, must be multiplicity.
Today, we have come to understand ourselves as composites; often contradictory and internally incompatible.
In the Babarnama , for example, we see the internal contradictions in the personality of the founder of the Mughal Empire.
When describing his conquest of Chanderi in 1528, Babar offers gruesome details of the gory slaughter of many “infidels” but just a few sentences later he talks at length about Chanderi’s lakes, flowing streams, and sweet water.
So who was Babar, bloodthirsty tyrant, humanist poet, or both – and not necessarily at odds with each other?
Pakistan’s selfhood must be expanded ad maximum and made so capacious that it accommodates its Punjabis, Sindhis, Pathans, and Balochis, and their religions – Sunni, Shia, Hindu, Christian, Parsi, Qadhianis – until it is possible to call them all equally “Pakistani.”
That must be the ultimate goal, and step one in the long, winding battle to save Pakistan.
That is a national idea worth striving for – and Pakistan’s intellectuals, its elite, and its youth must be at the forefront of the battle.
The Crescent has cast a seemingly interminable shadow across the length of Pakistan.
Its tragedies and failings are a result of what is happening in God’s name, not Jinnah’s.
To save Pakistan, Jinnah’s spirit, his moth-eaten ideals, must be renewed, and Pakistanis must ask themselves what Pakistan really means.
The Silver Lining in High Commodity Prices
Cambridge – Today’s soaring commodity prices scream a fundamental truth of modern life that many politicians, particularly in the West, don’t want us to hear: the world’s natural resources are finite, and, as billions of people in Asia and elsewhere escape poverty, Western consumers will have to share them.
Here is another truth: the price mechanism is a much better way to allocate natural resources than fighting wars, as the Western powers did in the last century.
The United States’ ill-considered bio-fuels subsidy program demonstrates how not to react.
Rather than acknowledge that high fuel prices are the best way to inspire energy conservation and innovation, the Bush administration has instituted huge subsidies to American farmers to grow grains for bio-fuel production.
Never mind that this is hugely inefficient in terms of water and land usage. 
Moreover, even under the most optimistic scenario, the US and the world will still be relying mainly on conventional fossil fuels until the hydrocarbon era comes to an end (which few of us will live to see).
Last but not least, diverting vast tracts of agricultural land into fuel production has contributed to a doubling of prices for wheat and other grains.
With food riots in dozens of countries, isn’t it time to admit that the whole idea was a giant, if well-intentioned, mistake?
Another wrong turn is the proposal recently embraced by two American presidential candidates to temporarily scrap taxes on gasoline.
As laudable as it may be to help low-income drivers deal with soaring fuel costs, this is not the way to do it.
The gas tax should be raised , not lowered.
The sad fact is that by keeping oil prices high, OPEC is doing far more for environmental conservation than Western politicians who seek to prolong the era of ecologically unsustainable Western consumerism.
Of course, it is not just oil prices that are high, but all commodity prices, from metals to food to lumber.
Prices for many commodities have doubled over the past couple of years.  Oil prices have risen almost 400% in the last five years.
The proximate cause is a global economic boom that has been stronger, longer, and more broad-based than any in modern history.
Asia has led the way, but the past five years have been the best Latin American and Africa have enjoyed in decades.
Broad-based commodity shortages often begin to emerge at the end of long global expansions, and in this respect, the present boom is no different.
Some politicians also complain about speculators who, more and more, are trading commodities on complex and growing markets that allow them to bet on whether, say, future demand from emerging markets is likely to outstrip growth in future supply.
But why is this a bad thing?
If “speculators” are bidding up today’s commodity prices because they realize that future generations are going to want commodities, too, isn’t that a healthy development?
High prices for commodities today mean more supply for future generations, while at the same time creating an incentive to develop new ways to conserve on consumption.
Again, high prices are helping in ways that Western politicians seem afraid to contemplate.
Admittedly, the global commodity price boom has had profound, albeit enormously complex and uncertain, effects on poverty.
While surging commodity prices are helping poor farmers and poor resource-rich countries, they are a catastrophe for the urban poor, some of whom spend 50% or more of their income on food.
One element of the solution is to compensate the very poor for the higher cost of survival.   Over the longer term, more money for fertilizer, and other aid to promote self-sufficiency, is also essential.
The World Bank, the United Nations, and even the Bush administration have moved to help, albeit in small measure relative to the scale of the problem.
Of course, it should be noted that if economic reform in resource-rich Africa had been proceeding at the same pace as in Asia, the era of soaring commodity prices might have been postponed for another century.
For now, though, instead of whining about high commodity prices, governments should be shielding only their very poorest citizens, and letting the price spikes serve as a wake-up call for the rest of us.
The end to Western consumerism is not yet at hand, but high commodity prices are a clear warning that big adjustments will be needed as Asia and other emerging nations begin to consume a larger share of the global pie. 
True, when today’s global economic boom ends, as it inevitably will, commodity prices will plummet, easily 25%, quite possibly 50% or more.
Western politicians will cheer, and many pundits will express relief that less money will be flowing to undemocratic countries in the developing world.
But today’s era of high commodity prices is not just a bad dream that should be forgotten when it ends.
High prices send a real message about scarcity in a globalizing world.
Those who ignore it, especially by blocking market forces, are making a tragic mistake.
The Sinai Powder Keg
TEL AVIV– The crisis in the Sinai Peninsula seems to have been dwarfed by Sunday’s drama in Cairo.
But Egyptian President Mohamed Morsi’s civilian coup, in which he dismissed General Mohamed Hussein Tantawi, the leader of the army’s supreme command, has not diminished the importance of the trouble there.
Earlier this month, jihadi terrorists ambushed an Egyptian military base in Sinai, killing 16 Egyptian soldiers.
They then hijacked two armored personnel carriers and sped toward the frontier with Israel.
One vehicle failed to break through the border crossing; the other penetrated Israeli territory, before being stopped by the Israel Defense Forces (IDF).
In response, Egypt’s military and security forces launched an offensive against Bedouin militants in Sinai, while Morsi forced the General Intelligence Service’s director to retire and dismissed the governor of Northern Sinai.
These episodes highlight the complexity of the Middle East’s changing geopolitical landscape, the fragility of Egypt’s post-Mubarak political order, and the explosive potential of Sinai, which, though sparsely populated, includes Egypt’s borders with Israel and the Palestinian enclave of Gaza.
Indeed, since Hosni Mubarak’s ouster last year, security in Sinai has deteriorated, and the region has become fertile ground for Islamic extremism.
The 1979 Egypt-Israel Peace Treaty mandated that Sinai be largely demilitarized in order to serve as a buffer zone between the two former enemies.
Tourism and natural-gas pipelines linking the two countries provided economic resources for the local Bedouin.
But, as Mubarak’s regime declined, so did the government’s control over the Bedouin.
Palestinian militants from Gaza – an active arena of Israeli-Palestinian confrontation since Hamas gained control in 2007 – and jihadi terrorists affiliated with Al Qaeda and the larger “global jihad” network penetrated Sinai, exploiting the government’s neglect of the region and inflaming the local population’s feelings of disenfranchisement.
Israelis complained about Mubarak’s “cold peace,” but they appreciated that he adhered to the treaty’s fundamental provisions.
Now the behavior of his successors from the military and the Muslim Brotherhood has revived security challenges and raised difficult questions about the region’s future.
For example, in August 2011, a jihadi group from Gaza seized control of an Egyptian outpost on Israel’s border and killed eight Israeli civilians.
Their objective was to damage further the Israeli-Egyptian relationship, which is already more fragile than ever, and they succeeded: the IDF accidentally killed several Egyptian soldiers during the incident.
The Egyptian security forces’ subsequent failure to prevent demonstrators from storming Israel’s embassy in Cairo brought matters to the brink of calamity.
The uncertainty and disorder that have plagued Egypt for the last 18 months are fueling increasing lawlessness in Sinai.
Last month, a pipeline carrying Egyptian natural gas to Israel and Jordan was bombed – the 15th such attack since Mubarak’s regime was toppled – and remains out of commission.
There are four principal actors in this arena: Israel, Egypt, Hamas, and the Sinai jihadis.
Israel wants, first and foremost, peace and stability.
To this end, Israeli leaders expect Egypt’s government to reestablish its authority in Sinai, and, despite the peace treaty’s provisions, have agreed to Egyptian requests to increase its military presence in the region.
Moreover, Israel has practically abandoned any hope of receiving agreed gas supplies from Egypt, and has not pressed its demand that Egypt block the passage of sophisticated weapons to Gaza.
Israel is determined not to act against terrorist groups and infrastructure on Egypt’s territory.
And, in view of Hamas’s close links with Egypt’s Muslim Brotherhood, whose political party backed Morsi’s successful presidential run, Israel has restrained its response to terrorist and rocket attacks from Gaza.
But Egypt’s divided government has not established a correspondingly coherent policy.
Relations with Israel are managed by the defense minister, now Lieutenant General Abdul Fattah al-Sisi, and the military security establishment, whose leaders are determined to maintain a peaceful relationship with Israel and to secure Egyptian sovereignty in the Sinai.
For them, the lawless Bedouin, the Sinai jihadis, and Hamas and other groups in Gaza threaten Egypt’s national security.
But their will and ability to translate this view into policy are limited.
Meanwhile, Morsi and the Muslim Brotherhood are playing a double game.
While Morsi denounced the recent violence (particularly the deliberate killing of Egyptian policemen) and issued an implicit threat against Hamas, the Brotherhood published a statement accusing Israel’s Mossad of perpetrating the attack – a claim that Hamas’s Prime Minister of Gaza, Ismael Haniyeh, has repeated.
In fact, Hamas, too, is playing a double game.
Having lost Syrian backing, it is hoping that the Egyptian Brothers will provide its kindred movement with political and logistical support.
Yet it allows radical Palestinians and jihadi groups in Gaza to conduct operations in Sinai.
The fourth actor, the Sinai jihadis, comprises primarily Bedouins, whose distinct origins and long-time marginalization have led some to identify with radical Islamist groups (often while working in the Arabian Peninsula).
While this group’s primary goal is to undermine Israel-Egypt relations, they do not shy away from operating directly against the Egyptian state.
Given their strategic location, Sinai jihadis could easily be used by larger terrorist networks to target strategically vital locations, such as the Suez Canal.
Egypt’s government was humiliated and incensed by the recent terrorist provocation.
But it is too early to tell whether its security crackdown in Sinai is a one-time operation, intended to placate angry citizens, or the beginning of a serious effort to address the interconnected problems in Sinai and Gaza.
It is also too early to tell whether Morsi’s shakeup in Cairo was related to, or expedited by, the Sinai events.
But Sinai’s explosive potential clearly has been increased by the Muslim Brotherhood’s takeover.
Israel’s longstanding channel of communication with Egypt through the military has disappeared, and may or may not be replaced by a similar channel.
Hamas and the Bedouins in Sinai have most likely been emboldened by the latest developments in Cairo.
But, as Egypt’s domestic politics take their course, Israel, which has had to tread softly in its relationship with Egypt and in Sinai since January 2011, will have to behave with even greater sensitivity in the days ahead.
The Single Mother Makeover
NEW YORK – In the 1992 United States presidential election, George H. W. Bush’s campaign made a political splash by going after the television show Murphy Brown – one of the first times, but far from the last, that a fictitious character was introduced to score political points in America.
Murphy Brown, played by actress Candice Bergen, was a TV anomaly at that time: a sympathetically portrayed single mother.
So Bush’s vice president, Dan Quayle, attacked the show for normalizing rather than stigmatizing single motherhood.
Much hand-wringing followed, with single mothers (never, at that time, single fathers) cast as harbingers of doom for core American values.
The implication was that selfish me-first feminists (if they were affluent white women) or feckless social parasites (if they were low-income women of color) were putting their own interests above their children’s.
Daniel Patrick Moynihan’s widely reprinted study The Negro Family: The Case for National Action painted a picture of single motherhood as the primary instigator of inner-city and especially African-American criminality, illiteracy, and drug use.
How times have changed.
Just as single mothers were irrationally castigated then, so today an equally irrational hagiography has risen around them.
(Europe has more single mothers than the US, but, characteristically, has less need to moralize about them one way or the other).
In US pop culture, the single mother has evolved from selfish yuppie or drug-dazed slut into a woman who is more fun, slightly more heroic, and certainly less frumpy than her married counterpart.
Indeed, single mothers are the new maternal ideal – women whose maternal drive is so selfless and intense that they choose to raise children even under the burden of their solitary status.
Angelina Jolie’s photo spread with her toddler son, adopted from Cambodia, in Vanity Fair heralded this shift: the sexy young woman and her son in a luxurious hotel bedroom made single motherhood look fun and glamorous.
Suddenly Hollywood stars and starlets who were otherwise unattached began to sprout little offspring: Calista Flockhart, who played TV’s ultimate desperate childless single woman, adopted a son – and, like a fairy tale, later met and married Harrison Ford.
Kourtney Kardashian of the reality show The Kardashians had a baby as an “unwed mother” – a stigmatizing term that has gone out of fashion – and is depicted as bravely holding down the fort and staying up all night with feedings as the child’s irresponsible father parties.
The tale of Sarah Palin’s daughter, Bristol, has also been spun as a tale of an admirable single mother and a loser father.
When they split up, the story line cast the heroic young mother against what was often depicted as the beer-drinking, immature dad.
Even Jennifer Aniston, whom Brad Pitt left because she did not want kids, now sighs in interviews, as she nears 42, that she has stopped waiting for Prince Charming, and that she, too, could be ready to adopt and go it alone.
Likewise, advertisements for convenience food and insurance – which used to feature only intact nuclear families – have begun to showcase single mothers lovingly spooning out meatballs, or protectively buying a life-insurance policy.
The glorification of single mothers represents a collective exasperation on the part of women in America – and women who make decisions in the mainstream media.
The 1990’s produced a demeaning narrative of women waiting in frustration as their “biological clocks” ticked, cursing themselves for putting career first at the expense of finding Mr. Right and having children.
In a post-sexual revolution world of marriage-shy men, women would supposedly do anything to secure the blessings of a wedded father for their children.
The message from the media was one of constant nagging and blame, such as the famous Newsweek cover that asserted (wrongly) that an older single woman had more likelihood of being in a terrorist attack than of finding a husband.
The whole narrative, as the writer Susan Faludi correctly perceived, was not about marriage at all; it reflected a backlash against feminism.
At some point, women became powerful enough that they collectively rejected the high social value this narrative placed on a male offer of a ring and flipped the stereotype on its head.
It started to occur to women that they could be employed and have a family – and that it could even be pretty nice.
Glamorizing single motherhood is not realistic, but it allows female pop culture to express a revenge fantasy at all the potential husbands and fathers who walked away, or who were not the dream husband and father after all, or who wanted the sex but not the kids and the tuition bills.
When Bill O’Reilly of Fox News recently accused Aniston of making a movie about single motherhood – The Switch – that seems to say “to a twelve or thirteen year old girl, ‘You don’t need a man,’” he is right.
These images do appeal to overworked, exasperated, baby-hungry women who may have spent years waiting for “the offer,” telling them that, in fact, you don’t need a man.
This trend hardly signifies the end of civilization as we know it; all things being equal, most women would still prefer the simple fantasy of a supportive partner in childrearing.
But the new image of single mothers – and of single motherhood – does show that it is getting harder – if not almost impossible – to coerce women by trying to fix upon them the scarlet letter.
Nationalism, Madness, and Terrorism
BOSTON – If we want to understand what drove the Boston Marathon bombing suspects, Tamerlan and Dzhokhar Tsarnaev, to terrorism, the answer almost certainly does not lie in Dagestan, where the brothers lived before moving to the United States, or in the two wars fought in Chechnya in the last 20 years.
Instead, a key to the Tsarnaevs’ behavior may perhaps be found in developments in England 500 years ago.
Several new phenomena appeared in sixteenth-century England that revolutionized human experience.
English society was redefined as a “nation” – that is, a sovereign community of equal members.
With that, the era of nationalism began, and social mobility became legitimate.
At the same time, a special variety of mental illness was first observed, which we would later call schizophrenia and depressive disorders – different from a multitude of mental illnesses already known.
It called into being a new term, “madness,” the first medical specialization (eventually named “psychiatry”), and special legislation regarding the “mad.”
Madness expressed itself in degrees of mental impairment, the common symptoms of which were chronic discomfort in one’s environment (social maladjustment), uncertainty about oneself, oscillation between self-loathing and megalomania, and sometimes a complete loss of identity.
Suicide became common, and the nature of violent crime changed, with a new type – irrational and unconnected to self-interest – becoming increasingly prevalent.
These phenomena were connected.
It was nationalism that legitimated mobility; the two of them together that produced madness; and the new mental disease that expressed itself in suicide and irrational violence.
Nationalism implied a specific image of society and reality in general – a consciousness that was to become the cultural framework of modernity.
In its original, English, form it was essentially democratic.
As it spread, it carried the seeds of democracy everywhere.
By considering a living community sovereign, nationalism implicitly but drastically reduced the relevance of God; even when combined with religion and presented in a religious idiom, it was essentially secular.
National consciousness, dramatically different from the fundamentally religious, hierarchical consciousness that it replaced, shapes how we live today.
Nationalist principles emphasize the self-governing individual, including the right to choose one’s social position and identity.
But this liberty, empowering and encouraging the individual to choose what to be, complicates identity formation.
A member of a nation cannot learn who or what s/he is from the environment, as would an individual in a religious and rigidly stratified social order, in which everyone’s position and behavior is defined by birth and divine providence.
Modern culture cannot provide us with the consistent guidance that other cultures give to their members.
By providing inconsistent guidance (for we are inevitably guided by our cultural environment), nationalism actively disorients us – a cultural insufficiency called anomie.
Because a clear sense of identity is a necessary condition for adequate mental functioning, malformation of identity leads to discomfort with one’s self and social maladjustment, reaching clinical proportions among the more fragile of us.
That is why the addition of madness to the roster of familiar mental illnesses coincided with the emergence of nationalism.
The more choices for the definition of one’s identity that a society offers – and the more insistent it is on equality – the more problematic the formation of identity in it becomes.
That is why the most open and freest society today, the Unites States, leads the world in rates of severe mental disease – supplanting England, yesterday’s freest and most open society.
Indeed, foreigners at one time considered madness “the English malady.”
Most examples of violent crime by mentally ill people were committed first in England, and then in the US, often seeming politically motivated, even when mediated by religion.
The first such case was likely that of Peter Berchet, a young Protestant, who felt that he had to kill the royal councilor Christopher Hatton, also a Protestant, whom Berchet believed to be a Catholic sympathizer.
Attempting to answer this calling, Berchet murdered another Protestant whom he mistook for Hatton.
To all appearances the act of a Puritan fanatic, the authorities suspected Berchet of being a part of an organized Puritan conspiracy.
He was to be questioned to divulge the names of his co-conspirators and then executed.
But it was quickly revealed, instead, that he was suffering from a “nawghtye mallenchollye.”
It was as natural for an Elizabethan Protestant to see the cause of his mental discomfort in a government overrun by Catholic sympathizers as it is for someone with a Muslim connection in the US today to see this cause in America as the embodiment of Western offenses against the faith.
Blaming one’s existential discomfort on external factors is a kind of self-therapy.
A story is constructed, which rationalizes one’s discomfort as reflecting an awareness of some general evil.
One may then join an organization committed to fighting that evil or be impelled to act on one’s own – to the point of committing murder.
The thinking behind such acts bears the most distinctive mark of delusion: the loss of the understanding of the symbolic nature of human reality, confusing symbols and their referents, and seeing people in terms of what they represent.
It is precisely this modern irrationality – a product of modernity itself – that the terrorist attack launched by the Tsarnaev brothers reflected.
The Slave Ethos and the African Economy
DAKAR – The slump in prices for Africa’s natural resources, which led to chronic deficits in the past, has been reversed.
Consumption, fueled by huge Asian demand for African commodities, is on the rise across the continent.
For much of Africa, this turn of events should mark a decisive break with endemic poverty.
But, unless African leaders change their ways, it will not.
Africa is estimated to hold more than 10% of global oil reserves and one-third of reserves of cobalt and base metals.
South Africa alone possesses 40% of the world’s gold, which has been skyrocketing in value since the onset of the global financial crisis.
Africa’s agricultural potential has barely been touched.
Long-term global demand for Africa’s commodities, land, and manpower is unlikely to diminish.
China, which has increased trade with Africa five-fold since 2003, has played a leading role in this turnaround, which has encouraged investors from elsewhere, including Europe and the United States, to rethink their approach to investing in Africa.
This has translated into a steady flow of multibillion-dollar investments in the region.
As a result, the IMF forecasts 4.7% GDP growth in sub-Saharan Africa this year, rising to nearly 6% in 2011.
Unfortunately, however, while the direction of Africa’s trade may be changing, its composition – raw-material exports and manufactured imports – is not.
In the half-century since colonialism ended (for the most part), sub-Saharan Africa has experienced many false dawns.
One remembers the 1960’s, when the global economy’s “long boom”gave Africa a chance to save its surpluses, invest in value-added industries, and increase productivity. Instead, Africa squandered the opportunity on consumption of foreign goods.
This pattern of behavior conforms to some of the worst undertakings of Africa’s elite.
Hundreds of years ago, many African kings effectively engaged with the West’s rising imperial powers to halt the growth of indigenous industry in pre-colonial Africa.
Instead of having their peoples begin to manufacture their own goods, these rulers chose to import them from Europe in exchange for their own subjects – or the subjects of neighboring rulers – whom they exported as slaves.
Back then, imported goods consisted mainly of beads, alcohol, and, most importantly, weapons, which enabled these rulers to exercise extreme violence over their people.
It was a system that no doubt worked to the benefit of Africa’s partners. But it would not have existed without the active and willing participation of African elites.
When slavery was abolished in the mid-nineteenth century, the terms of these rulers’ partnership with Western colonizers changed from trade in slaves to trade in commodities.
After independence in the early 1960’s, during the Cold War, many African rulers played the West against the Soviet bloc to retain their predatory economies.
Today, they pursue the same goal with the help mainly of China, but also of Iran, Venezuela, and occasionally India and Brazil.
Elite networks continue to conspire with foreign interests to cannibalize their economies and retain the perquisites of power: fleets of luxury cars, private jets, bank accounts in tax havens, overseas properties, consumer goods, weaponry, etc.
Their contempt for local products extends to health and education: rather than invest in these vital sectors, Africa’s rulers use private overseas healthcare and schools for themselves and their children.
This culture of exploiting Africa’s human and natural resources at will – not through entrepreneurial endeavor and wealth creation, but by means of predatory politics – is deeply entrenched.
Indeed, the few trade unionists, intellectuals, and others who challenge these regimes usually do not seek to change how the state functions, but rather to ensure that, as they put it, they get their “turn to eat.”
One might expect that national and international non-governmental organizations would fill the gap. Not so.
If a few NGOs have achieved great results fighting the ills of Africa’s poor, the vast majority are perceived as either a “fifth column” of the West or a refuge for fringe members of the elite who use widespread poverty as another source of lucre.
According to estimates published in the Financial Times on June 1st, at least $854 billion (€702.6 billion) have been siphoned from Africa since 1970 in cumulative capital flight.
However, this cost is nothing compared to the scars associated with Africans being seen around the world as the “wretched of the earth.”
Apart from war-ravaged regions, only Africa has so many young people – some 60% of the population – who are prepared to brave any danger to flee their land.
However sad and painful it is to admit, a fleet of slave ships docking on Africa’s shores today would be overrun by willing would-be deportees.
At the turn of the millennium, a meeting of African church leaders convened on the Senegalese island of Gorée to urge Africans to assess their share of responsibility for the slave trade.
The call went unheeded, in keeping with the general atmosphere of self-denial surrounding the issue.
Historians and others who venture to break the silence are lambasted as traitors to the black cause.
Yet the truth cannot be denied.
Africa faces a crisis of leadership and governance, owing to a dysfunctional ethos.
If Africans want to change this, they cannot spare themselves a collective debate about their elites’ complicity in widespread impoverishment.
The Sleeping Volcano of Global Finance
The rejection of the European Union’s Constitutional Treaty by French and Dutch voters was, according to all evidence, more a rejection of unregulated globalization than it was a rejection of Europe.
The general instability of social relations – most importantly, but not only, of employment – is slowly becoming intolerable for a growing part of the population in many developed countries, not just in Europe.
Ever since, the international financial system has endured almost constant instability.
Crises have multiplied, with each seemingly worse than the one that came before.
Throughout the rich world, poverty has come roaring back.
Internal and international inequalities have been increasing at breakneck speed.
Employment is increasingly precariousness.
And where unemployment is preferred to universal job insecurity, it has become impossible to suppress.
It is to this state of affairs that the French and Dutch said “no” two months ago.
Paradoxically, however, a united Europe is likely to be needed even more in the near future than it was in the past.
After all, beyond the social misery produced by the re-institutionalized cruelty of the current global economic system, the greatest danger facing the world nowadays is that very system’s inherent instability.
I don’t see any institution other than the EU that has enough size and heft to protect Europeans from a possible implosion.
Consider the simple fact that the American economy is now more than $600 billion in debt.
The United States cannot function without being able to borrow $1.9 billion dollars each and every day of the year, mainly from the emerging economies of Asia, and China above all.
But this support could weaken or even cease if the dollar falls too low, if the price of oil rises too high, or if the American economy backfires.
In fact, the US economy has become increasingly detached from reality.
Its manufacturing sector now accounts for a mere 11% of America’s GDP.
Ford and General Motors are in dire financial straits.
Meanwhile, two speculative bubbles – in the real estate market and in mortgages – have become grafted upon each other and now dominate economic activity in the US.
A crash, or at least a sharp spasm, is quite likely in the near future, and the consequences – for America and the world – may be catastrophic.
This instability also makes it difficult to address other grave problems affecting the global financial system.
Sovereign debt, needed by all countries, but particularly by the poorest, suffers profoundly from erratic interest-rate and exchange-rate movements.
The absence of a lender of last resort in today’s world only magnifies the threat involved in each crisis.
To make matters worse, national failures can no longer be addressed without aggravating the situation.
Of course – indeed, above all – in such circumstances the immense investments needed to overcome underdevelopment and the disabilities that it entails are increasingly forgotten by the world of international finance.
With rich countries threatened by instability and poor countries largely left to their own devices, the reconstruction of the world financial system should be at the top of the international agenda.
A new Bretton Woods could not be more urgent!
Five Years in Limbo
NEW YORK – When the US investment bank Lehman Brothers collapsed in 2008, triggering the worst global financial crisis since the Great Depression, a broad consensus about what caused the crisis seemed to emerge.
A bloated and dysfunctional financial system had misallocated capital and, rather than managing risk, had actually created it.
Financial deregulation – together with easy money – had contributed to excessive risk-taking.
Monetary policy would be relatively ineffective in reviving the economy, even if still-easier money might prevent the financial system’s total collapse.
Thus, greater reliance on fiscal policy – increased government spending – would be necessary.
Five years later, while some are congratulating themselves on avoiding another depression, no one in Europe or the United States can claim that prosperity has returned.
The European Union is just emerging from a double-dip (and in some countries a triple-dip) recession, and some member states are in depression.
In many EU countries, GDP remains lower, or insignificantly above, pre-recession levels.
Almost 27 million Europeans are unemployed.
Similarly, 22 million Americans who would like a full-time job cannot find one.
Labor-force participation in the US has fallen to levels not seen since women began entering the labor market in large numbers.
Most Americans’ income and wealth are below their levels long before the crisis.
Indeed, a typical full-time male worker’s income is lower than it has been in more than four decades.
Yes, we have done some things to improve financial markets.
There have been some increases in capital requirements – but far short of what is needed.
Some of the risky derivatives – the financial weapons of mass destruction – have been put on exchanges, increasing their transparency and reducing systemic risk; but large volumes continue to be traded in murky over-the-counter markets, which means that we have little knowledge about some of our largest financial institutions’ risk exposure.
Likewise, some of the predatory and discriminatory lending and abusive credit-card practices have been curbed; but equally exploitive practices continue.
The working poor still are too often exploited by usurious payday loans.
Market-dominant banks still extract hefty fees on debit- and credit-card transactions from merchants, who are forced to pay a multiple of what a truly competitive market would bear.
These are, quite simply, taxes, with the revenues enriching private coffers rather than serving public purposes.
Other problems have gone unaddressed – and some have worsened.
America’s mortgage market remains on life-support: the government now underwrites more than 90% of all mortgages, and President Barack Obama’s administration has not even proposed a new system that would ensure responsible lending at competitive terms.
The financial system has become even more concentrated, exacerbating the problem of banks that are not only too big, too interconnected, and too correlated to fail, but that are also too big to manage and be held accountable.
Despite scandal after scandal, from money laundering and market manipulation to racial discrimination in lending and illegal foreclosures, no senior official has been held accountable; when financial penalties have been imposed, they have been far smaller than they should be, lest systemically important institutions be jeopardized.
The credit ratings agencies have been held accountable in two private suits. But here, too, what they have paid is but a fraction of the losses that their actions caused.
More important, the underlying problem – a perverse incentive system whereby they are paid by the firms that they rate – has yet to change.
Bankers boast of having paid back in full the government bailout funds that they received when the crisis erupted.
But they never seem to mention that anyone who got huge government loans with near-zero interest rates could have made billions simply by lending that money back to the government.
Nor do they mention the costs imposed on the rest of the economy – a cumulative output loss in Europe and the US that is well in excess of $5 trillion.
Meanwhile, those who argued that monetary policy would not suffice turned out to have been right.
Yes, we were all Keynesians – but all too briefly.
Fiscal stimulus was replaced by austerity, with predictable – and predicted – adverse effects on economic performance.
Some in Europe are pleased that the economy may have bottomed out.
With a return to output growth, the recession – defined as two consecutive quarters of economic contraction – is officially over.
But, in any meaningful sense, an economy in which most people’s incomes are below their pre-2008 levels is still in recession.
And an economy in which 25% of workers (and 50% of young people) are unemployed – as is the case in Greece and Spain – is still in depression.
Austerity has failed, and there is no prospect of a return to full employment any time soon (not surprisingly, prospects for America, with its milder version of austerity, are better).
The financial system may be more stable than it was five years ago, but that is a low bar – back then, it was teetering on the edge of a precipice.
Those in government and the financial sector who congratulate themselves on banks’ return to profitability and mild – though hard-won – regulatory improvements should focus on what still needs to be done.
The glass is, at most, only one-quarter full; for most people, it is three-quarters empty.
Exceptions Become the Rule
NEW YORK – Long ago, I worked as an analyst on Wall Street.
The first company that I analyzed was Federal Express, which at the time had not yet shipped its first package.
The idea behind FedEx was simple and compelling: The cost of complexity was higher than the cost of air transport, so the company would ship all its packages overnight to Memphis, Tennessee.
By radically simplifying the myriad combinations of start and end points – the only routes were to and from Memphis – all of the packages could be delivered the next day reliably.
All of that has changed, thanks to developments in information technology.
With today’s powerful computers, we can look at massive amounts of information and simulate complex situations, making pretty good predictions about many things: What will traffic flows look like on Tuesday at 5:00 in the afternoon if we put a detour at this highway intersection?
What percentage of people on this drug will get better – and, more interestingly, which particular individuals will respond positively, and which ones are likely to be harmed?
Indeed, with the ability to make reliable predictions, we can put people and things into categories, whether market segments, disease risks, likely loan defaulters, potential purchasers, and so on.
That’s big data.
But now we can also do “small data.”
We can treat many things, even packages, like individuals.
The exceptions – whether individual genotypes, individual privacy preferences, or digital rights to use content in specific contexts –&#160;have become the rule.
We don’t need to guess at everyone’s privacy preferences or settle for one-size-fits-all policies.
Over time, we will be able to figure out which people, based on their genotype, will be helped or harmed by a particular drug, or how children can learn best with personalized feedback, or how to produce furniture and clothing in a world of 3D printers and real-time modeling.
And so on.
Would you like your customized car seat in leather or cloth, sir?
The market will rise to the challenge.
People who care about something will be able to specify their preferences to an extremely precise degree and get exactly what they want.
For others, actually setting those preferences or defining what they want may take more attention than they care to devote to the task.
So the design challenge of the future will be to create good defaults with easy editing/customization tools for those who care.
But this change will raise challenging social and political questions as well, particularly concerning privacy preferences and health care – both already controversial issues.
Of course, no one can define or guarantee privacy.
But individuals could get the opportunity to control the use of their data – and entities that want to use it could negotiate with them.
Currently, Web advertisers and publishers say that their businesses depend on their ability to track people and collect and resell the data that they gather.
They argue, further, that it is too complicated to respect individuals’ preferences, too difficult to tell them how their data is being used, and pointless to treat them as individuals.
Yet, somehow, the data collectors can manage to record individuals’ purchase histories, their airline seat preferences, and so on.
There is no reason why they could not also record how and by whom each piece of such information can be used.
Indeed, millions of people now do set specific privacy preferences within Facebook, opt out of being tracked, and the like.
At the same time, they gladly share data with vendors and even track their own data – whether airline mileage or steps walked, check-ins at their favorite venues (especially if they can earn discounts or special offers), or their movie, music, or book purchases.
Now suppose that you could tell people to whom you had sold their data.
Most people would not care, but those who did would appreciate the transparency and perhaps want a little share.
Suppose you started a business that managed data on behalf of the users.
That is not such a crazy idea – the airlines, among other companies, are already doing it to some extent.
United, American, and British Airways all know my travel patterns on their airlines, and help me manage both my past trips (and related rewards) and my future reservations.
Mint does the same for my financial data; WellnessFX for my blood biomarkers.
A new start-up called Moven plans to track small payments so that you can see in real time how you are sticking to or deviating from a budget.
All of this works well in markets for goods and services, where people who want choice can pay for it.
Businesses can treat customers as individuals, and give them the amount of special consideration that they are willing to pay for.
Companies can also decide not to serve certain customers, focusing on the most profitable segments.
But this approach does not work for things that the government (that is, other people’s taxes) pays for.
In the public sector, the one-size-fits-all approach still prevails.
In democracies, each citizen gets one vote.
So shouldn’t everyone get the same benefits?
Yes, we tax rich people more and give poor people some more benefits, and that is contentious enough.
But consider all of the qualitative services and conditions for which individuals have different preferences, needs, and outcomes that are now more predictable.
If we can predict individual outcomes, what is an individual’s responsibility, and what remains a collective task?
These questions will become especially acute in areas such as education and health care.
For example, we treat children differently in school according to their potential – as we understand it.
But, if we help some children “to realize their potential,” are we thereby limiting the potential of others?
Likewise, how do we allocate health-care resources?
What responsibility do individuals have to modify their behavior in response to their individual vulnerabilities and predispositions?
And, most important, who – if anyone – should impose that responsibility?
The Social Dimensions of Globalization
The war on terrorism and in Iraq has distracted much of the world's attention from the pressing issue of how globalization should be managed so that it benefits everyone.
A new report, issued by the International Labor Organization's Commission on the Social Dimensions of Globalization, reminds us how far the Bush administration is out of line with the global consensus.
The ILO is a tripartite organization with representatives of labor, government, and business.
The Commission, chaired by the presidents of Finland and Tanzania, has 24 members (of whom I was one) drawn from different nationalities, interests groups, and intellectual persuasions, including members as diverse as the head of Toshiba and the leader of the AFL-CIO.
Yet this very heterogeneous group was able to crystallize the emerging global consensus that globalization, despite its positive potential, has not only failed to live up to that potential, but has actually contributed to social distress.
The fault lies with how globalization has been managed - partly by countries, but most importantly, by the international community, including institutions like the World Bank, World Trade Organization, and the IMF, which are responsible for establishing the "rules of the game."
The Commission even reached consensus on a number of concrete measures to help put a "human face" on globalization - or at least mitigate some of its worst effects.
The gap between the emerging consensus on globalization, which this report reflects, and the Bush administration's international economic policies help explain today's widespread hostility towards America's government.
Consider two issues that have been part of recent bilateral trade agreements pushed aggressively by the Bush administration.
The crises in East Asia and the recent recessions in Latin America show that premature capital market liberalization can result in enormous economic volatility, increasing poverty, and destruction of the middle class.
Even the IMF now recognizes that capital market liberalization has delivered neither growth nor stability to many developing countries.
Yet, whether driven by narrow ideology or responding to the demands of special interests, the Bush administration is still demanding an extreme form of such liberalization in its bilateral trade agreements.
The second issue concerns the unbalanced intellectual property provisions (TRIPs) of the Uruguay Round of trade talks, dictated by America's pharmaceutical and entertainment industries.
These provisions restricted countries from making generic imitations of drugs, making many critically important medicines unaffordable in developing countries.
Spearheaded by worries about AIDS, activists around the world demanded that something be done.
Just before last year's trade talks in Cancun, the US made some concessions, so that it was no longer the only holdout.
In its bilateral trade agreements, however, the US is demanding what is becoming known as "TRIPs plus," which would strengthen intellectual property rights further, to ensure that countries only have the right to produce inexpensive generic drugs during epidemics and other emergencies.
The global consensus, reflected in the Commission report, calls for more exceptions, so that, say, drugs can be made available in any case where to do so could save a life.
To those confronting the prospect of death, what matters is access to life-saving drugs, not whether what is killing the person is part of an epidemic.
Bilateral agreements form the basis of enhanced ties of friendship between countries. But America's intransigence in this area is sparking protests in countries facing the "threat" of such an agreement, such as Morocco, and is forming the basis of long-lasting resentment.
The Commission highlights other issues that have received insufficient global attention - such as tax competition among developing countries, which shifts more of the tax burden from business to workers.
In still other areas, the Commission's report argues for more "balanced" perspectives.
On exchange rates, for example, it is more sympathetic towards mixed systems - in contrast to the traditional belief that countries must choose between the extremes of a completely flexible system and a hard peg (of the kind that contributed so importantly to Argentina's woes).
As this example shows, bringing different voices to the table in discussions of globalization brings new perspectives.
Until now, the main worry for most experts on globalization has been excessive government intervention in the economy.
The Commission fears just the opposite.
It argues that the state has a role to play in cushioning individuals and society from the impact of rapid economic change.
The way that globalization has been managed, however, has eroded the ability of the state to play its proper role.
At the root of this problem is the global political system - if such it can be called.
Key players like the IMF and World Bank must become more transparent and their voting structures must be changed to reflect the current distribution of economic power - as opposed to that prevailing in 1945 - let alone to reflect basic democratic principles.
Whatever one thinks of the many concrete suggestions made by the Commission, this much is clear: we need a more inclusive debate about globalization, one in which more voices are heard, and in which there is more focus on the social dimensions of globalization.
This is a message the world would do well to heed, lest discontent with globalization continue to grow.
The Efficient Networker
NEW YORK – Long ago, I used to run a conference called PC Forum. People would say to me, “If only I could sit next to Bill Gates....”
Just last week, a friend pulled me aside at a conference to ask for advice. “What’s the best way to make conferences more efficient for meeting people?” he asked.
“Just like marketing, it seems it’s only about 50% effective.”
He was asking the wrong question.
The interaction has to be two-way: Do other people really want to meet him?
A conference optimized to help my friend would fail, because the people everyone wants to meet are unlikely to show up.
Indeed, does my friend have a right to their time?
He told me that he had actually sat in a breakout group with 20 people, each of whom he wanted to meet, but he managed to talk to only two of them.
I was a bit concerned.
Was this guy I was trying to help actually a stalker?
What he wanted was a tool that would allow him to make contact with each person on the list of people he had picked out.
This is probably a legitimate goal – he sells decent products – but the reality of the world is that relationships, even around so-called shared interests, are not necessarily symmetrical.
More people want to meet Bill Gates or Marissa Mayer than vice versa.
If we were to create a real market, it would become unpleasantly commercial – a bit like those charity auctions where celebrities or tycoons donate the pleasure of their company for lunch with the highest bidder, except that the VIP pockets the money.
At yet another conference, I was musing about how to make introductions efficient but not transactional when I was introduced to Lisa Anderson of Werqit by Megan Smith of Google.