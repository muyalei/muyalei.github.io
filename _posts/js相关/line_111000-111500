While there have been major, albeit incomplete, reforms of international finance, efforts in 2009 and 2010 to reform the international monetary system – including the proposed changes at the IMF – have led to no significant action.
The reform proposals came from diverse quarters: the governor of the People’s Bank of China; a commission convened by the United Nations General Assembly on reform of the international monetary and financial system, headed by the Nobel laureate Joseph E. Stiglitz; and the French Palais Royal Initiative, led by former IMF Managing Director Michel Camdessus.
There have been myriad academic contributions to the debate as well.
The first element of reform should be to give a greater role to the only true international money that the world now has: the IMF’s Special Drawing Rights, created in 1969 as the result of another dollar crisis.
The establishment of SDRs was accompanied by a commitment, included in the IMF Articles of Agreement, to “making the special drawing right the principal reserve asset in the international monetary system” (Article VIII, Section 7 and Article XXII).
But this commitment has remained a dead letter, except for periodic emissions of SDRs during crises, including the equivalent of $250 billion in 2009.
The IMF Articles of Agreement should be amended to allow more flexible use of SDRs, replicating the way central banks operate.
That is, SDRs could be created during global recessions and withdrawn during booms.
They should be the major source of IMF financing as well, replacing quota subscriptions or lending to the Fund by member countries (potentially making the IMF a purely SDR-based institution, as proposed decades ago by the late IMF economist Jacques Polak).
The simplest approach would be for countries to “deposit” the SDRs that they receive at the IMF, which could then lend them to countries and invest the remainder in sovereign bonds.
This should be combined with a more active role for the IMF – rather than the G-20 – as the true instrument of global macroeconomic policy coordination.
One essential (and generally agreed) goal of such coordination should be to reduce global imbalances like those caused in recent years by the European Union’s rising external surplus, which has forced many emerging economies to run growing deficits.
Because other global currencies – the dollar, the euro, and increasingly the renminbi – would continue to coexist with the SDRs as global reserve assets, another essential element of global macroeconomic cooperation should be defining the particular obligations of countries (or regions) issuing reserve currencies.
Still another element of international monetary reform is better management of the global exchange-rate system (or, again, “non-system”), which should aim at avoiding currency “manipulation” – but only after defining precisely what that means.
It should also reduce the high levels of exchange-rate volatility – “a tax on international specialization,” as the economist Charles Kindleberger once put it – that we have seen in recent years, including among major currencies.
Long-term global-finance discussions have also made clear that, under certain conditions, regulation of capital flows (“capital-flow management measures,” in current IMF terminology) are warranted on “macroprudential” grounds – an understanding accepted by the G-20 at its 2011 Cannes meeting and by the IMF last year.
Last but not least, the international monetary system requires governance reform, which includes giving a stronger voice to emerging and developing countries.
Aside from the final adoption of the 2010 reforms and the forthcoming 2014 quota debates, the changes should include those recommended by the Stiglitz Commission, the Palais Royal Initiative, and the 2009 Committee on IMF Governance Reform headed by Trevor Manuel, among others.
One central element of these proposals is to eliminate, once and for all, the veto power of any individual country.
With the world waiting anxiously for some of America’s political leaders to behave like adults, the cost of maintaining the current non-system has become all too obvious.
After the Dollar
NEW YORK – It is symbolic that the recent BRICS summit in Fortaleza, Brazil, took place exactly seven decades after the Bretton Woods Conference that created the International Monetary Fund and the World Bank.
The upshot of the BRICS meeting was the announcement of the New Development Bank, which will mobilize resources for infrastructure and sustainable development projects, and a Contingent Reserve Arrangement to provide liquidity through currency swaps.
The Bretton Woods Conference marked one of history’s greatest examples of international economic cooperation.
And, while no one can say yet whether the BRICS’ initiatives will succeed, they represent a major challenge to the Bretton Woods institutions, which should respond.
Rethinking the role of the US dollar in the international monetary system is a case in point.
One key feature of the Bretton Woods system was that countries would tie their exchange rates to the US dollar.
While the system was effectively eliminated in 1971, the US dollar’s central role in the international monetary system has remained intact – a reality that many countries are increasingly unwilling to accept.
Dissatisfaction with the dollar’s role as the dominant global reserve currency is not new.
In the 1960s, French Finance Minister Valéry Giscard d’Estaing famously condemned the “exorbitant privilege” that the dollar’s status bestowed upon the United States.
The issue is not merely one of fairness.
According to the Belgian economist Robert Triffin, an international monetary system based on a national currency is inherently unstable, owing to the resulting tensions among the inevitably divergent interests of the issuing country and the international system as a whole.
Triffin issued his warning more than 50 years ago, but it has recently gained traction, as China’s rise has made the world increasingly disinclined to tolerate the instability caused by a dollar-denominated system.
The solution, however, lies not in replacing the dollar with the renminbi, but in strengthening the role of the world’s only truly global currency: the IMF’s Special Drawing Rights.
Following the creation of SDRs in 1969, IMF members committed to make them “the principle reserve asset in the international monetary system,” as stated in the Articles of Agreement.
But the peculiar way in which SDRs were adopted limited their usefulness.
For starters, the separation of the IMF’s SDR account from its general account made it impossible to use SDRs to finance IMF lending.
Furthermore, though countries accrue interest on their holdings of SDRs, they have to pay interest on the allocations they receive.
In other words, SDRs are both an asset and a liability, functioning like a guaranteed credit line for the holder – a sort of unconditional overdraft facility.
Nonetheless, SDRs have proved to be useful.
After initial allocations in 1970-1972, more were issued to increase global liquidity during major international crises: in 1979-1981, in 1997, and, in particular, in 2009, when the largest issue – the equivalent of $250 billion – was made.
While developed countries, including the US and the United Kingdom, have drawn on their allocations, the major users have been developing and, in particular, low-income countries.
In fact, this is the only way in which developing countries (China aside) share in the creation of international money.
Several estimates indicate that, given the additional demand for reserves, the world could absorb annual allocations of $200-300 billion or even more.
This has prompted many – including People’s Bank of China Governor Zhou Xiaochuan; the United Nations-backed Stiglitz Commission; the Palais-Royal Initiative, led by former IMF Managing Director Michel Camdessus; and the Triffin International Foundation – to call for changes to the international monetary system.
In 1979, the IMF economist Jacques Polak, who had been part of the Dutch delegation at the Bretton Woods conference, outlined a plan for doing just that.
His recommendations include, first and foremost, making all of the IMF’s operations in SDRs, which would require ending the separation of the IMF’s SDR and general accounts.
The simplest way to fulfill this vision would be to allocate SDRs as a full reserve asset, which countries could either use or deposit in their IMF accounts.
The IMF would use those deposits to finance its lending operations, rather than having to rely on quota allocations or “arrangements to borrow” from members.
Other provisions could be added.
To address developing countries’ high currency demands, while enhancing their role in the creation of international money, a formula could be created to give them a larger share in SDR allocations than they now receive.
The private use of SDRs could also be encouraged, though that would likely be met with strong opposition from countries currently issuing international reserve currencies, especially the US.
Keeping SDRs as pure “central-bank money” would eliminate such opposition, enabling them to complement and stabilize the current system, rather than upend it.
Just as the Bretton Woods framework restored order to the global economy after WWII, a new monetary framework, underpinned by a truly international currency, could strengthen much-needed economic and financial stability.
Everyone – even the US – would benefit from that.
Waste Not, Want Not
ROME – Every year, we waste or lose 1.3 billion metric tons of food – one-third of the world’s annual food production.
The sheer scale of the number makes it almost impossible to grasp, no matter how one approaches it.
Try to imagine 143,000 Eiffel Towers stacked one on top of another, or a pile of 10 trillion bananas.
The figure is all the more unfathomable, given that, alongside this massive wastage and loss, 840 million people experience chronic hunger on a daily basis.
Many millions more suffer from “silent hunger” – malnutrition and micronutrient deficiencies.
For the more economically minded, here is another number: food wastage and loss, expressed in producer prices, costs roughly $750 billion per year.
If we were to consider retail prices and the wider impacts on the environment, including climate change, the figure would be much higher.
In an era of austerity, it is difficult to understand how such a massive hemorrhage of resources could be neglected.
In fact, in some places, the volume of food wastage is rising.
Now a new report by the United Nations Food and Agriculture Organization focuses on another troubling aspect of the problem: the negative consequences for the environment and the natural resources on which we rely for our survival.
When food is lost or wasted, the energy, land, and water resources that went into producing it are squandered as well.
At the same time, large amounts of greenhouse gases are released into the atmosphere during production, processing, and cooking.
From any perspective – ethical, economic, environmental, or in terms of food security – we simply cannot tolerate the annual wastage of 1.3 billion tons of food.
This is why serious reduction of food loss and wastage is one of the five elements of UN Secretary General Ban Ki-moon’s “Zero Hunger Challenge” and a major focus of the UN High Level Task Force on Global Food Security.
We are working together within the UN system and with a broad coalition of other partners to ensure universal access to adequate food all year round; eliminate childhood stunting; make all food systems sustainable; and eradicate rural poverty.
Next week, the Global Green Growth Forum in Copenhagen will allow for a deeper look at this issue.
There is much that can be done.
For starters, food loss and wastage needs to be seen as a cross-cutting policy issue, rather than a lifestyle choice to be left in the hands of individual consumers and their consciences.
The world needs to wake up to the need for policies that address all stages of the food chain, from production to consumption.
Food loss – on farms, during processing, transport, and at markets – undermines food security in most developing countries, where post-harvest losses can reach as high as 40% of production.
Investment in infrastructure for transport, storage, and marketing of food is badly needed, as are programs to train farmers in best practices.
In developed countries, food-retailing practices require a rethink.
For example, rejection of food products on the basis of aesthetic concerns is a major cause of wastage.
Some supermarkets have already begun relaxing standards on fruit appearance, selling “misshapen” items at reduced prices and helping to raise awareness that ugly does not mean bad.
More approaches like this – and concerted efforts to find markets or uses for surplus food – are needed.
Businesses and households alike should monitor where and how they waste food and take corrective steps, because prevention of wastage is even more important than recycling or composting.
Yes, 1.3 billion tons is a mindboggling figure.
But these simple steps are easy enough to grasp – and within reach for everyone.
The world confronts many seemingly intractable problems; food wastage is one issue that we all can do something about now.
Argentina’s Griesafault
NEW YORK – On July 30, Argentina’s creditors did not receive their semiannual payment on the bonds that were restructured after the country’s last default in 2001.
Argentina had deposited $539 million in the Bank of New York Mellon a few days before.
But the bank could not transfer the funds to the creditors: US federal judge Thomas Griesa had ordered that Argentina could not pay the creditors who had accepted its restructuring until it fully paid – including past interest – those who had rejected it.
It was the first time in history that a country was willing and able to pay its creditors, but was blocked by a judge from doing so.
The media called it a default by Argentina, but the Twitter hashtag #Griesafault was much more accurate.
Argentina has fulfilled its obligations to its citizens and to the creditors who accepted its restructuring.
Griesa’s ruling, however, encourages usurious behavior, threatens the functioning of international financial markets, and defies a basic tenet of modern capitalism: insolvent debtors need a fresh start.
Sovereign defaults are common events with many causes.
For Argentina, the path to its 2001 default started with the ballooning of its sovereign debt in the 1990s, which occurred alongside neoliberal “Washington Consensus” economic reforms that creditors believed would enrich the country.
The experiment failed, and the country suffered a deep economic and social crisis, with a recession that lasted from 1998 to 2002.
By the end, a record-high 57.5% of Argentinians were in poverty, and the unemployment rate skyrocketed to 20.8%.
Argentina restructured its debt in two rounds of negotiations, in 2005 and 2010.
More than 92% of creditors accepted the new deal, and received exchanged bonds and GDP-indexed bonds.
It worked out well for both Argentina and those who accepted the restructuring.
The economy soared, so the GDP-indexed bonds paid off handsomely.
But so-called vulture investors saw an opportunity to make even larger profits.
The vultures were neither long-term investors in Argentina nor the optimists who believed that Washington Consensus policies would work.
They were simply speculators who swooped in after the 2001 default and bought up bonds for a fraction of their face value from panicky investors.
Economically, though, it makes no sense.
When a country pays a risk premium on its debt, it means that default is a possibility.
But if a court rules that a country always must repay the debt, there is no default risk to be compensated.
Repayment on Griesa’s terms would devastate Argentina’s economy.
NML Capital and the other vultures comprise just 1% of the creditors, but would receive a total of $1.5 billion.
Other holdouts (6.6% of total creditors) would receive $15 billion.
And, because the debt restructuring stipulated that all of the creditors who accepted it could demand the same terms as holdouts receive, Argentina might be on the hook for $140 billion more.
Every Argentine might thus owe more than $3,500 – more than one-third of average annual per capita income.
In the United States, applying the equivalent proportion would mean forcing every citizen to pay roughly $20,000 – all to line the pockets of some billionaires, intent on wringing the country dry.
What’s more, the existence of credit default swaps creates the possibility of further gains for the vultures.
A CDS insures against a default, paying off if the bonds do not. They can yield substantial returns, regardless of whether the bonds are repaid – thus reducing their holders’ incentive to achieve an agreement.
In the run-up to July 30, the vultures conducted a scare campaign.
A second default in 13 years would be a big setback for Argentina, they claimed, threatening the country’s fragile economy.
But all of this presumed that financial markets would not distinguish between a default and a Griesafault.
Fortunately, they did: Interest rates for different categories of Argentine corporate loans have not reacted to the event.
In fact, borrowing costs on July 30 were lower than the average for the whole year.
Ultimately, though, the Griesafault will carry a high price – less for Argentina than for the global economy and countries needing access to foreign financing.
America will suffer, too.
Its courts have been a travesty: As one observer pointed out,it was clear that Griesa never really fathomed the issue’s complexity.
The US financial system, already practiced at exploiting poor Americans, has extended its efforts globally.
Sovereign borrowers will not – and should not – trust the fairness and competence of the US judiciary.
The market for issuance of such bonds will move elsewhere.&nbsp;
Debeaking the Vultures
NEW YORK – In the midst of the ongoing dispute between Argentina and the “vulture funds” that hold its bonds, a broad consensus has emerged concerning the need for sovereign-debt restructuring mechanisms (SDRMs).
Otherwise, US Federal Judge Thomas P. Griesa’s ruling that Argentina must pay the vultures in full (after 93% of other bondholders agreed to a restructuring) will give free rein to opportunistic behaviors that sabotage future restructurings.
Most recently, the International Capital Market Association (ICMA) recommended new terms for government bonds.
Though the ICMA’s proposal leaves unresolved the hundreds of billions of bonds written under the old terms, the new framework says in effect that Griesa’s interpretation was wrong, and recognizes that leaving it in place would make restructuring impossible.
The ICMA’s proposed contractual terms clarify the pari passu clause that was at the heart of Griesa’s muddle-headed ruling.
The intent of the clause – a standard component of sovereign-bond contracts – was always to ensure that the issuing country treated identical bondholders identically.
But it has always been recognized that senior creditors – for example, the International Monetary Fund – are treated differently.
Griesa did not seem to grasp the common understanding of the clause.
After Argentina defaulted on its sovereign debt in 2001, vulture funds bought defaulted bonds in the secondary market at a fraction of their face value, and then sued for full payment.
According to Griesa’s interpretation of pari passu, if Argentina paid the interest that it owed to creditors that accepted the restructuring, it had to pay the vultures in full – including all past interest and the principal.
The vultures’ business was enabled in part by litigation over the so-called champerty defense – based on a longstanding English common-law doctrine, later adopted by US state legislatures, prohibiting the purchase of debt with the intent of bringing a lawsuit.
Argentina is simply the latest victim in the vultures’ long legal battle to change the rules of the game to permit them to prey on poor countries seeking to restructure their debts.
In 1999, in Elliot Associates, LP v. Banco de la Nacion and the Republic of Peru, the Second Circuit Court of Appeals determined that the plaintiff’s intent in purchasing the discounted debt was to be paid in full or otherwise to sue.
The court then ruled that Elliot’s intent, because it was contingent, did not meet the champerty requirement.
Though some other courts accepted the Second Circuit’s narrow reading of the champerty defense, the vultures were not satisfied and went to the New York state legislature, which in 2004 effectively eliminated the defense of champerty concerning any debt purchase above $500,000.
That decision contradicted understandings according to which hundreds of billions of dollars of debt had already been issued.
Investors who acquire defaulted sovereign debt at huge discounts should not expect repayment in full; the discount is an indication that the market does not expect that, and it is only through litigation that one could hope to receive anything close to it.
An important change in the legal framework, such as the elimination of the champerty defense, is de facto a change in “property rights,” with the debtors losing, and creditors who purchase the bonds intending to sue if they are not paid what they want – the vultures – gaining.
The vultures were thus unjustly enriched, doubly so with the novel and unjustified interpretation of the pari passu clause.
Will so-called collective-action clauses (CACs) – another aspect of the ICMA “reform” aimed at debeaking the vultures – save the day?
In many countries, CACs stipulate that if, say, two-thirds of the investors accept a company’s (or a country’s) restructuring proposal, the other investors are bound to go along.
This mechanism prevents speculative holdouts from holding up the restructuring process and demanding ransom.
But CACs do not exist for sovereign debt written in many jurisdictions, leaving the field open for the vultures.
Moreover, CACs are no panacea.
If they were, there would be no need for domestic bankruptcy law, which spells out issues like precedence and fair treatment.
But no government has found CACs adequate for resolving domestic restructuring.
So why should we think that they would suffice in the much more complex world of sovereign-debt restructurings?
In particular, CACs suffer from the problem of “aggregation.”
If a CAC required, say, 75% of the holders of each bond class, vultures could buy 26% of only one bond class and block the entire restructuring.
The recent Greek debt restructuring had to confront this issue.
The ICMA’s new framework seems to provide a way out: The supermajority would be defined by the acceptance of the aggregate principal amount of outstanding debt securities of all of the affected series.
The supermajority’s decisions would be binding on all other investors.
But this, too, poses a problem: The more junior creditors could vote to have themselves treated in the same way as more senior creditors.
What recourse would the senior creditors then have?
In bankruptcy court, they would have grounds for objecting, and the judge would have to weigh the equities.
These issues are especially important in the context of sovereign-debt restructurings, because the claimants to a country’s resources include not only formal creditors; others, too – for example, pensioners – might not be paid if bondholders are paid in full.
Chapter 9 of the US Bankruptcy Code (which applies to public entities) recognizes these rights – unlike Griesa and the vultures.
Today, the international community faces two challenges.
One is to deal with the hundreds of billions of dollars of debt written under the old terms, which cannot be restructured under Griesa’s ruling.
The second is to decide on the terms that should be imposed in the future.
The investing community has made a serious proposal.
But changes of this magnitude must be based on discussions among creditors and debtor governments – and more is needed than tweaking the terms of the agreements.
An initiative at the United Nations to encourage the establishment of SDRMs is receiving the support of prominent academic economists and practitioners.
Global efforts are good first steps to remedy the damage to international financial markets that the US courts have inflicted.
For the sake of a healthy global economy, the vultures must be grounded.
The Innovation Enigma
NEW YORK – Around the world, there is enormous enthusiasm for the type of technological innovation symbolized by Silicon Valley.
In this view, America’s ingenuity represents its true comparative advantage, which others strive to imitate.
But there is a puzzle: it is difficult to detect the benefits of this innovation in GDP statistics.
What is happening today is analogous to developments a few decades ago, early in the era of personal computers.
In 1987, economist Robert Solow – awarded the Nobel Prize for his pioneering work on growth – lamented that “You can see the computer age everywhere but in the productivity statistics.”
There are several possible explanations for this.
Perhaps GDP does not really capture the improvements in living standards that computer-age innovation is engendering.
Or perhaps this innovation is less significant than its enthusiasts believe.
As it turns out, there is some truth in both perspectives.
Recall how a few years ago, just before the collapse of Lehman Brothers, the financial sector prided itself on its innovativeness.
Given that financial institutions had been attracting the best and brightest from around the world, one would have expected nothing less.
But, upon closer inspection, it became clear that most of this innovation involved devising better ways of scamming others, manipulating markets without getting caught (at least for a long time), and exploiting market power.
In this period, when resources flowed to this “innovative” sector, GDP growth was markedly lower than it was before.
Even in the best of times, it did not lead to an increase in living standards (except for the bankers), and it eventually led to the crisis from which we are only now recovering.
The net social contribution of all of this “innovation” was negative.
Similarly, the dot-com bubble that preceded this period was marked by innovation – Web sites through which one could order dog food and soft drinks online.
At least this era left a legacy of efficient search engines and a fiber-optic infrastructure.
But it is not an easy matter to assess how the time savings implied by online shopping, or the cost savings that might result from increased competition (owing to greater ease of price comparison online), affects our standard of living.
Two things should be clear.
First, the profitability of an innovation may not be a good measure of its net contribution to our standard of living.
In our winner-takes-all economy, an innovator who develops a better Web site for online dog-food purchases and deliveries may attract everyone around the world who uses the Internet to order dog food, making enormous profits in the process.
But without the delivery service, much of those profits simply would have gone to others.
The Web site’s net contribution to economic growth may in fact be relatively small.
Moreover, if an innovation, such as ATMs in banking, leads to increased unemployment, none of the social cost – neither the suffering of those who are laid off nor the increased fiscal cost of paying them unemployment benefits – is reflected in firms’ profitability.
Likewise, our GDP metric does not reflect the cost of the increased insecurity individuals may feel with the increased risk of a loss of a job.
Equally important, it often does not accurately reflect the improvement in societal wellbeing resulting from innovation.
In a simpler world, where innovation simply meant lowering the cost of production of, say, an automobile, it was easy to assess an innovation’s value.
But when innovation affects an automobile’s quality, the task becomes far more difficult.
And this is even more apparent in other arenas: How do we accurately assess the fact that, owing to medical progress, heart surgery is more likely to be successful now than in the past, leading to a significant increase in life expectancy and quality of life?
Still, one cannot avoid the uneasy feeling that, when all is said and done, the contribution of recent technological innovations to long-term growth in living standards may be substantially less than the enthusiasts claim.
A lot of intellectual effort has been devoted to devising better ways of maximizing advertising and marketing budgets – targeting customers, especially the affluent, who might actually buy the product.
But standards of living might have been raised even more if all of this innovative talent had been allocated to more fundamental research – or even to more applied research that could have led to new products.
Yes, being better connected with each other, through Facebook or Twitter, is valuable.
But how can we compare these innovations with those like the laser, the transistor, the Turing machine, and the mapping of the human genome, each of which has led to a flood of transformative products?
Of course, there are grounds for a sigh of relief.
Although we may not know how much recent technological innovations are contributing to our wellbeing, at least we know that, unlike the wave of financial innovations that marked the pre-crisis global economy, the effect is positive.
Creating a Learning Society
NEW YORK – Citizens in the world’s richest countries have come to think of their economies as being based on innovation.
But innovation has been part of the developed world’s economy for more than two centuries.
Indeed, for thousands of years, until the Industrial Revolution, incomes stagnated.
Then per capita income soared, increasing year after year, interrupted only by the occasional effects of cyclical fluctuations.
The Nobel laureate economist Robert Solow noted some 60 years ago that rising incomes should largely be attributed not to capital accumulation, but to technological progress – to learning how to do things better.
While some of the productivity increase reflects the impact of dramatic discoveries, much of it has been due to small, incremental changes.
And, if that is the case, it makes sense to focus attention on how societies learn, and what can be done to promote learning – including learning how to learn.
A century ago, the economist and political scientist Joseph Schumpeter argued that the central virtue of a market economy was its capacity to innovate.
He contended that economists’ traditional focus on competitive markets was misplaced; what mattered was competition for the market, not competition in the market.
Competition for the market drove innovation.
A succession of monopolists would lead, in this view, to higher standards of living in the long run.
Schumpeter’s conclusions have not gone unchallenged.
Monopolists and dominant firms, like Microsoft, can actually suppress innovation.
Unless checked by anti-trust authorities, they can engage in anti-competitive behavior that reinforces their monopoly power.
Moreover, markets may not be efficient in either the level or direction of investments in research and learning.
Private incentives are not well aligned with social returns: firms can gain from innovations that increase their market power, enable them to circumvent regulations, or channel rents that would otherwise accrue to others.
But one of Schumpeter’s fundamental insights has held up well: Conventional policies focusing on short-run efficiency may not be desirable, once one takes a long-run innovation/learning perspective.
This is especially true for developing countries and emerging markets.
Industrial policies – in which governments intervene in the allocation of resources among sectors or favor some technologies over others – can help “infant economies” learn.
Learning may be more marked in some sectors (such as industrial manufacturing) than in others, and the benefits of that learning, including the institutional development required for success, may spill over to other economic activities.
Such policies, when adopted, have been frequent targets of criticism.
Government, it is often said, should not be engaged in picking winners.
The market is far better in making such judgments.
But the evidence on that is not as compelling as free-market advocates claim.
America’s private sector was notoriously bad in allocating capital and managing risk in the years before the global financial crisis, while studies show that average returns to the economy from government research projects are actually higher than those from private-sector projects – especially because the government invests more heavily in important basic research.
One only needs to think of the social benefits traceable to the research that led to the development of the Internet or the discovery of DNA.
But, putting such successes aside, the point of industrial policy is not to pick winners at all.
Rather, successful industrial policies identify sources of positive externalities – sectors where learning might generate benefits elsewhere in the economy.
Viewing economic policies through the lens of learning provides a different perspective on many issues.
The great economist Kenneth Arrow emphasized the importance of learning by doing.
The only way to learn what is required for industrial growth, for example, is to have industry.
And that may require either ensuring that one’s exchange rate is competitive or that certain industries have privileged access to credit – as a number of East Asian countries did as part of their remarkably successful development strategies.
There is a compelling infant economy argument for industrial protection.
Moreover, financial-market liberalization may undermine countries’ ability to learn another set of skills that are essential for development: how to allocate resources and manage risk.
Likewise, intellectual property, if not designed properly, can be a two-edged sword when viewed from a learning perspective.
While it may enhance incentives to invest in research, it may also enhance incentives for secrecy – impeding the flow of knowledge that is essential to learning while encouraging firms to maximize what they draw from the pool of collective knowledge and to minimize what they contribute.
In this scenario, the pace of innovation is actually reduced.
More broadly, many of the policies (especially those associated with the neoliberal “Washington Consensus”) foisted on developing countries with the noble objective of promoting the efficiency of resource allocation today actually impede learning, and thus lead to lower standards of living in the long run.
Virtually every government policy, intentionally or not, for better or for worse, has direct and indirect effects on learning.
Developing countries where policymakers are cognizant of these effects are more likely to close the knowledge gap that separates them from the more developed countries.
Developed countries, meanwhile, have an opportunity to narrow the gap between average and best practices, and to avoid the danger of secular stagnation.
South Africa Breaks Out
NEW YORK – International investment agreements are once again in the news.
The United States is trying to impose a strong investment pact within the two big so-called “partnership” agreements, one bridging the Atlantic, the other the Pacific, that are now being negotiated. But there is growing opposition to such moves.
South Africa has decided to stop the automatic renewal of investment agreements that it signed in the early post-apartheid period, and has announced that some will be terminated.
Ecuador and Venezuela have already terminated theirs.
India says that it will sign an investment agreement with the US only if the dispute-resolution mechanism is changed.
For its part, Brazil has never had one at all.
There is good reason for the resistance.
Even in the US, labor unions and environmental, health, development, and other nongovernmental organizations have objected to the agreements that the US is proposing.
The agreements would significantly inhibit the ability of developing countries’ governments to protect their environment from mining and other companies; their citizens from the tobacco companies that knowingly purvey a product that causes death and disease; and their economies from the ruinous financial products that played such a large role in the 2008 global financial crisis.
They restrict governments even from placing temporary controls on the kind of destabilizing short-term capital flows that have so often wrought havoc in financial markets and fueled crises in developing countries.
Indeed, the agreements have been used to challenge government actions ranging from debt restructuring to affirmative action.
Advocates of such agreements claim that they are needed to protect property rights.
But countries like South Africa already have strong constitutional guarantees of property rights.
There is no reason that foreign-owned property should be better protected than property owned by a country’s own citizens.
Moreover, if constitutional guarantees are not enough to convince investors of South Africa’s commitment to protecting property rights, foreigners can always avail themselves of expropriation insurance provided by the Multilateral Investment Guarantee Agency (a division of the World Bank) or numerous national organizations providing such insurance.
(Americans, for example, can buy insurance from the Overseas Private Investment Corporation.)
But those supporting the investment agreements are not really concerned about protecting property rights, anyway.
The real goal is to restrict governments’ ability to regulate and tax corporations – that is, to restrict their ability to impose responsibilities, not just uphold rights.
Corporations are attempting to achieve by stealth – through secretly negotiated trade agreements – what they could not attain in an open political process.
Even the notion that this is about protecting foreign firms is a ruse: companies based in country A can set up a subsidiary in country B to sue country A’s government.
American courts, for example, have consistently ruled that corporations need not be compensated for the loss of profits from a change in regulations (a so-called regulatory taking); but, under the typical investment agreement, a foreign firm (or an American firm, operating through a foreign subsidiary) can demand compensation!
Worse, investment agreements enable companies to sue the government over perfectly sensible and just regulatory changes – when, say, a cigarette company’s profits are lowered by a regulation restricting the use of tobacco.
In South Africa, a firm could sue if it believes that its bottom line might by harmed by programs designed to address the legacy of official racism.
There is a long-standing presumption of “sovereign immunity”: states can be sued only under limited circumstances.
But investment agreements like those backed by the US demand that developing countries waive this presumption and permit the adjudication of suits according to procedures that fall far short of those expected in twenty-first-century democracies.
Such procedures have proved to be arbitrary and capricious, with no systemic way to reconcile incompatible rulings issued by different panels.
While proponents argue that investment treaties reduce uncertainty, the ambiguities and conflicting interpretations of these agreements’ provisions have increased uncertainty.
Countries that have signed such investment agreements have paid a high price.
Several have been subject to enormous suits – and enormous payouts.
There have even been demands that countries honor contracts signed by previous non-democratic and corrupt governments, even when the International Monetary Fund and other multilateral organizations have recommended that the contract be abrogated.
Even when developing-country governments win the suits (which have proliferated greatly in the last 15 years), the litigation costs are huge.
The (intended) effect is to chill governments’ legitimate efforts to protect and advance citizens’ interests by imposing regulations, taxation, and other responsibilities on corporations.
Moreover, for developing countries that were foolish enough to sign such agreements, the evidence is that the benefits, if any, have been scant.
In South Africa’s review, it found that it had not received significant investments from the countries with which it had signed agreements, but had received significant investments from those with which it had not.
It is no surprise that South Africa, after a careful review of investment treaties, has decided that, at the very least, they should be renegotiated.
Doing so is not anti-investment; it is pro-development.
And it is essential if South Africa’s government is to pursue policies that best serve the country’s economy and citizens.
Indeed, by clarifying through domestic legislation the protections offered to investors, South Africa is once again demonstrating – as it has repeatedly done since the adoption of its new Constitution in 1996 – its commitment to the rule of law.
It is the investment agreements themselves that most seriously threaten democratic decision-making.
South Africa should be congratulated.
Other countries, one hopes, will follow suit.
American Delusions Down Under
NEW YORK – For better or worse, economic-policy debates in the United States are often echoed elsewhere, regardless of whether they are relevant.
Australian Prime Minister Tony Abbott’s recently elected government provides a case in point.
As in many other countries, conservative governments are arguing for cutbacks in government spending, on the grounds that fiscal deficits imperil their future.
In the case of Australia, however, such assertions ring particularly hollow – though that has not stopped Abbott’s government from trafficking in them.
Even if one accepts the claim of the Harvard economists Carmen Reinhart and Kenneth Rogoff that very high public debt levels mean lower growth – a view that they never really established and that has subsequently been discredited – Australia is nowhere near that threshold.
Its debt/GDP ratio is only a fraction of that of the US, and one of the lowest among the OECD countries.
What matters more for long-term growth are investments in the future – including crucial public investments in education, technology, and infrastructure.
Such investments ensure that all citizens, no matter how poor their parents, can live up to their potential.
There is something deeply ironic about Abbott’s reverence for the American model in defending many of his government’s proposed “reforms.”
After all, America’s economic model has not been working for most Americans.
Median income in the US is lower today than it was a quarter-century ago – not because productivity has been stagnating, but because wages have.
The Australian model has performed far better.
Indeed, Australia is one of the few commodity-based economies that has not suffered from the natural-resource curse.
Prosperity has been relatively widely shared.
Median household income has grown at an average annual rate above 3% in the last decades – almost twice the OECD average.
To be sure, given its abundance of natural resources, Australia should have far greater equality than it does.
After all, a country’s natural resources should belong to all of its people, and the “rents” that they generate provide a source of revenue that could be used to reduce inequality.
And taxing natural-resource rents at high rates does not cause the adverse consequences that follow from taxing savings or work (reserves of iron ore and natural gas cannot move to another country to avoid taxation).
But Australia’s Gini coefficient, a standard measure of inequality, is one-third higher than that of Norway, a resource-rich country that has done a particularly good job of managing its wealth for the benefit of all citizens.
One wonders whether Abbott and his government really understand what has happened in the US?
Does he realize that since the era of deregulation and liberalization began in the late 1970s, GDP growth has slowed markedly, and that what growth has occurred has primarily benefited those at the top?
Does he know that prior to these “reforms,” the US had not had a financial crisis – now a regular occurrence around the world – for a half-century, and that deregulation led to a bloated financial sector that attracted many talented young people who otherwise might have devoted their careers to more productive activities?
Their financial innovations made them extremely rich but brought America and the global economy to the brink of ruin.
Australia’s public services are the envy of the world.
Its health-care system delivers better outcomes than the US, at a fraction of the cost.
It has an income-contingent education-loan program that permits borrowers to spread their repayments over more years if necessary, and in which, if their income turns out to be particularly low (perhaps because they chose important but low-paying jobs, say, in education or religion), the government forgives some of the debt.
The contrast with the US is striking.
In the US, student debt, now in excess of $1.2 trillion (more than all credit-card debt), is becoming a burden for graduates and the economy.
America’s failed financial model for higher education is one of the reasons that, among the advanced countries, America now has the least equality of opportunity, with the life prospects of a young American more dependent on his or her parents’ income and education than in other advanced countries.
Abbott’s notions about higher education also suggest that he clearly does not understand why America’s best universities succeed.
It is not price competition or the drive for profit that has made Harvard, Yale, or Stanford great.
None of America’s great universities are for-profit-institutions.
They are all not-for-profit institutions, either public or supported by large endowments, contributed largely by alumni and foundations.
There is competition, but of a different sort.
They strive for inclusiveness and diversity.
They compete for government research grants.
America’s under-regulated for-profit universities excel in two dimensions: the ability to exploit young people from poor backgrounds, charging them high fees without delivering anything of value, and the ability to lobby for government money without regulation and to continue their exploitative practices.
Australia should be proud of its successes, from which the rest of the world can learn a great deal.
It would be a shame if a misunderstanding of what has happened in the US, combined with a strong dose of ideology, caused its leaders to fix what is not broken.
Europe’s Austerity Zombies
NEW YORK – “If the facts don’t fit the theory, change the theory,” goes the old adage.
But too often it is easier to keep the theory and change the facts – or so German Chancellor Angela Merkel and other pro-austerity European leaders appear to believe.
Though facts keep staring them in the face, they continue to deny reality.
Austerity has failed.
But its defenders are willing to claim victory on the basis of the weakest possible evidence: the economy is no longer collapsing, so austerity must be working!
But if that is the benchmark, we could say that jumping off a cliff is the best way to get down from a mountain; after all, the descent has been stopped.
But every downturn comes to an end.
Success should not be measured by the fact that recovery eventually occurs, but by how quickly it takes hold and how extensive the damage caused by the slump.
Viewed in these terms, austerity has been an utter and unmitigated disaster, which has become increasingly apparent as European Union economies once again face stagnation, if not a triple-dip recession, with unemployment persisting at record highs and per capita real (inflation-adjusted) GDP in many countries remaining below pre-recession levels.
In even the best-performing economies, such as Germany, growth since the 2008 crisis has been so slow that, in any other circumstance, it would be rated as dismal.
The most afflicted countries are in a depression.
There is no other word to describe an economy like that of Spain or Greece, where nearly one in four people – and more than 50% of young people – cannot find work.
To say that the medicine is working because the unemployment rate has decreased by a couple of percentage points, or because one can see a glimmer of meager growth, is akin to a medieval barber saying that a bloodletting is working, because the patient has not died yet.
Extrapolating Europe’s modest growth from 1980 onwards, my calculations show that output in the eurozone today is more than 15% below where it would have been had the 2008 financial crisis not occurred, implying a loss of some $1.6 trillion this year alone, and a cumulative loss of more than $6.5 trillion.
Even more disturbing, the gap is widening, not closing (as one would expect following a downturn, when growth is typically faster than normal as the economy makes up lost ground).
Simply put, the long recession is lowering Europe’s potential growth.
Young people who should be accumulating skills are not.
There is overwhelming evidence that they face the prospect of significantly lower lifetime income than if they had come of age in a period of full employment.
Meanwhile, Germany is forcing other countries to follow policies that are weakening their economies – and their democracies.
When citizens repeatedly vote for a change of policy – and few policies matter more to citizens than those that affect their standard of living – but are told that these matters are determined elsewhere or that they have no choice, both democracy and faith in the European project suffer.
France voted to change course three years ago.
Instead, voters have been given another dose of pro-business austerity.
One of the longest-standing propositions in economics is the balanced-budget multiplier – increasing taxes and expenditures in tandem stimulates the economy.
And if taxes target the rich, and spending targets the poor, the multiplier can be especially high.
But France’s so-called socialist government is lowering corporate taxes and cutting expenditures – a recipe almost guaranteed to weaken the economy, but one that wins accolades from Germany.
The hope is that lower corporate taxes will stimulate investment.
This is sheer nonsense.
What is holding back investment (both in the United States and Europe) is lack of demand, not high taxes.
Indeed, given that most investment is financed by debt, and that interest payments are tax-deductible, the level of corporate taxation has little effect on investment.
Likewise, Italy is being encouraged to accelerate privatization.
But Prime Minister Matteo Renzi has the good sense to recognize that selling national assets at fire-sale prices makes little sense.
Long-run considerations, not short-run financial exigencies, should determine which activities occur in the private sector.
The decision should be based on where activities are carried out most efficiently, serving the interests of most citizens the best.
Privatization of pensions, for example, has proved costly in those countries that have tried the experiment.
America’s mostly private health-care system is the least efficient in the world.
These are hard questions, but it is easy to show that selling state-owned assets at low prices is not a good way to improve long-run financial strength.
All of the suffering in Europe – inflicted in the service of a man-made artifice, the euro – is even more tragic for being unnecessary.
Though the evidence that austerity is not working continues to mount, Germany and the other hawks have doubled down on it, betting Europe’s future on a long-discredited theory.
Why provide economists with more facts to prove the point?
A Healthier, Wealthier Africa
ZURICH – The BRIC countries (Brazil, Russia, India, and China) have long been the focus of emerging-market investors.
But it is in Africa, a region with the world’s second-fastest growth, where the next big business opportunities lie.
In about one-third of the continent’s 55 countries, annual GDP growth is above 6%, and Sub-Saharan Africa grew at an estimated 5.1% pace in 2013.
Foreign investors report that their return on investment in Africa is higher than in any other emerging region.
By 2040, the continent’s working-age population will total an estimated 1.1 billion people, providing businesses with a larger labor pool than even China or India.
Moreover, economic expansion is taking place not just in urban centers, but in small towns and villages as well.
However, Africa also faces major challenges.
One of the biggest concerns is inadequate health care.
Preventable and treatable diseases plague the population.
Life expectancy is 14 years below the global average.
Nearly one in 20 adults in the 15-49 age group in Sub-Saharan Africa lives with HIV – roughly six times the global average.
Every minute, an African child dies from malaria.
Africa’s rapid economic growth and urbanization are also creating new health challenges.
Non-communicable diseases are projected to become the most common causes of death by 2030.
The number of diabetes cases, for example, is expected nearly to double over the next two decades.
If left unaddressed, the dual burden of communicable and non-communicable diseases could jeopardize Africa’s economic potential.
To avoid this, three critical areas of health care must be addressed: technology, infrastructure, and education.
Africa has witnessed some remarkable technological leaps.
A decade ago, telecoms infrastructure was almost non-existent.
Today, one in six people owns a mobile phone, the benefits of which go far beyond easy communication.
Africa has pioneered the use of mobile banking, with local players like M-Pesa and global corporations like Citi demonstrating how new technology can provide vital financial services to the unbanked population.
Mobile money and digital wallets accessed on cellphones eliminate the need for physical cash in rural areas, where financial services are limited and carrying large amounts of cash is risky.
Joining this mobile-phone revolution, Novartis is working with five African governments and private-sector partners to improve drug distribution and monitor the supply of anti-malaria medicines in rural areas, using text messaging and electronic mapping.
Previously, patients would travel to distant health clinics only to find that the medicines that they needed were no longer in stock.
Now, thanks to the SMS for Life project, vital medicines can be quickly redistributed to where they are most needed.
A second crucial aspect of improving Africa’s health care is infrastructure.
Good rail lines, roads, and ports allow products and services to be distributed widely at lower cost, and to benefit from economies of scale.
This is an essential component of any country’s economic development.
And nowhere are the benefits more evident than in the provision of health care.
Fortunately, several companies are rising to the challenge.
Coca-Cola, for example, is contributing its supply-chain expertise to map out health facilities and implement stock-management software to support the distribution of bed nets, contraceptives, anti-AIDS drugs, and vaccines to remote villages.
Meanwhile, Novartis is piloting a “social venture” business model to give poor people in remote villages access to vital medicines that are packaged in small, affordable doses.
Finally, education is one of the most powerful tools for reducing poverty and generating sustainable, inclusive economic growth.
But, given limited resources and few teachers, too many children are being left behind.
Public-private partnerships are helping to change this.
The IT company Cisco, for example, works with NGOs to connect communities and help students develop skills in information and communications technology.
Poor education also affects health.
Although Africa accounts for one-seventh of the world’s population, it bears one-quarter of the global disease burden, yet has a mere 2% of the world’s doctors.
Unsurprisingly, many health problems stem from ignorance about diseases and basic hygiene.
So Novartis has formed a partnership with the Earth Institute, the United Nations, and private-sector groups to train and deploy by 2015 a million community health workers in Sub-Saharan Africa to deliver basic treatment and preventive care, and to track disease outbreaks.
The idea is that local people will learn how to support their communities, rather than rely on handouts, and thereby lift themselves out of poverty permanently.
Africa is increasingly showing signs of its promise, but innovative solutions to improve its people’s health are essential if the continent is to approach its potential.
This will require more than philanthropy: it will demand new commercial models that solve health-care issues, help the economy to grow, and benefit those who invest in Africa’s future.
The Old World’s New Roles
CAMBRIDGE – China’s rise has raised many questions for the West, with some wondering whether it is set to usurp a struggling Europe’s global leadership role.
As one columnist put it, “there is nothing much European governments can do in East Asia, save serve as marketing managers for their domestic businesses.”
With neither the diplomatic weight nor the military heft to make an impression in the region, Europe had better leave the heavy lifting to the United States.
But this does not have to be the case.
For Europe, the implications of China’s rise are far-reaching, beginning with the United States’ strategic “pivot” toward Asia.
After more than 70 years as a top US priority, Europe is beginning to lose its privileged position in the eyes of American policymakers.
Moreover, European sales of high-tech dual-use products that complicate America’s security role in Asia is bound to create friction.
Nonetheless, warnings that the Atlantic partnership is eroding are unduly dire.
Tellingly, US President Barack Obama’s administration has replaced the term “pivot,” which implies a turn away from something, with “rebalancing.”
This change reflects a recognition that China’s increasing economic dominance does not negate the importance of the European Union, which remains the world’s largest economic entity and a leading source of economic innovation, not to mention values like the protection of human rights.
This is not to say that Asia’s rise will not demand adjustments.
When the Industrial Revolution began, Asia’s share of the global economy began to decline from more than 50% to just 20% by 1900.
By the second half of this century, Asia is expected to recover its former economic dominance – that is, account for 50% of global output – while lifting hundreds of millions of people out of poverty.
This power shift – perhaps the most consequential of the twenty-first century – implies serious risks.
Historians often warn that the fear and uncertainty generated by the emergence of new powers like China can trigger serious conflict, like that which Europe experienced a century ago, when Germany overtook the United Kingdom in industrial production.
With Asia riven by territorial disputes and historical tensions, maintaining a stable security balance will not be easy.
But there are levers in place that can help.
In the 1990s, when US President Bill Clinton’s administration was considering how to respond to China’s increasing economic might, some urged a policy of containment.
Clinton rejected that advice: It would have been impossible to forge an anti-China alliance, given the enduring desire of China’s neighbors to maintain good relations with it; more important, such a policy would have guaranteed future enmity with China.
Instead, Clinton chose a policy that could be called “integrate and insure.”
While China was welcomed into the World Trade Organization (WTO), America revived its security treaty with Japan.
If China pursues a “peaceful rise,” its neighbors will focus on building strong economic relationships with it.
If it throws its weight around – which some say is implied by its recent actions on the Indian border and in the East and South China Seas – its neighbors will seek to balance its power, with an American naval presence offering backup.
Where does Europe fit into this picture?
For starters, it should monitor and restrain sensitive exports to avoid making the security situation more dangerous for the US.
Even in trading terms, Europe has an interest in regional stability and secure sea lanes.
Furthermore, Europe can contribute to the development of the norms that shape the security environment.
For example, Europe can play an important role in reinforcing a universal interpretation of the United Nations Convention on the Law of the Sea, rather than China’s idiosyncratic version – especially given that the US has not even ratified the treaty.
Contrary to the claims of some analysts, China is not a revisionist state like Nazi Germany or the Soviet Union, eager to overthrow the established international order.
Indeed, it is not in China’s interest to destroy international institutions – such as the United Nations, the WTO, and the International Monetary Fund – that have helped to facilitate its rise.
Given Europe’s leading roles in such institutions, it can help China gain the multilateral legitimacy that it seeks, in exchange for responsible behavior.
Though China is not attempting to upend the global order, it is now undergoing a profound – and destabilizing – transformation.
With the rise of transnational issues like climate change, terrorism, pandemics, and cyber crime – brought about by rapid technological progress and social change – power is being diffused not among states, but among a wide range of non-governmental entities.
Addressing these challenges will require broad international cooperation, with China, the US, and Europe each playing an important role.
Finally, there is the question of values.
Europe, together with the US, has already resisted Chinese (and Russian) demands for greater Internet censorship.
And European countries like Norway and Germany have willingly taken economic hits in the name of human rights.
While it is impossible to predict how Chinese politics will evolve, other countries’ experiences suggest that political change often occurs when per capita income reaches roughly $10,000.
If such change does occur, Europe will have an opening to promote its core values even more effectively.
Whether China’s economic interest in an impartial world order based on the rule of law will lead to greater protection of individual rights remains to be seen.
Only China will decide that.
But Europe can make a strong case.
Duties Without Borders
CAMBRIDGE – More than 130,000 people are said to have died in Syria’s civil war.
United Nations reports of atrocities, Internet images of attacks on civilians, and accounts of suffering refugees rend our hearts.
But what is to be done – and by whom?
Recently, the Canadian scholar-politician Michael Ignatieff urged US President Barack Obama to impose a no-fly zone over Syria, despite the near-certainty that Russia would veto the United Nations Security Council resolution needed to legalize such a move.
In Ignatieff’s view, if Syrian President Bashar al-Assad is allowed to prevail, his forces will obliterate the remaining Sunni insurgents – at least for now; with hatreds inflamed, blood eventually will flow again.
In an adjoining article, the columnist Thomas Friedman drew some lessons from the United States’ recent experience in the Middle East.
First, Americans understand little about the social and political complexities of the countries there.
Second, the US can stop bad things from happening (at considerable cost), but it cannot make good things happen by itself.
And, third, when America tries to make good things happen in these countries, it runs the risk of assuming responsibility for solving their problems.
So what are a leader’s duties beyond borders?
The problem extends far beyond Syria – witness recent killings in South Sudan, the Central African Republic, Somalia, and other places.
In 2005, the UN General Assembly unanimously recognized a “responsibility to protect” citizens when their own government fails to do so, and in 2011 it was invoked in UN Security Council Resolution 1973, authorizing the use of military force in Libya.
Russia, China, and others believe that the principle was misused in Libya, and that the guiding doctrine of international law remains the UN Charter, which prohibits the use of force except in self-defense, or when authorized by the Security Council.
But, back in 1999, when faced with a Russian veto of a potential Security Council resolution in the case of Kosovo, NATO used force anyway, and many defenders argued that, legality aside, the decision was morally justified.
So which arguments should political leaders follow when trying to decide the right policy to pursue?
The answer depends, in part, on the collectivity to which he or she feels morally obliged.
Above the small-group level, human identity is shaped by what Benedict Anderson calls “imagined communities.”
Few people have direct experience of the other members of the community with which they identify.
In recent centuries, the nation has been the imagined community for which most people were willing to make sacrifices, and even to die, and most leaders have seen their primary obligations to be national in scope.
In a world of globalization, however, many people belong to multiple imagined communities.
Some – local, regional, national, cosmopolitan – seem to be arranged as concentric circles, with the strength of identity diminishing with distance from the core; but, in a global information age, this ordering has become confused.
Today, many identities are overlapping circles – affinities sustained by the Internet and cheap travel.
Diasporas are now a mouse click away.
Professional groups adhere to transnational standards.
Activist groups, ranging from environmentalists to terrorists, also connect across borders.
As a result, sovereignty is no longer as absolute and impenetrable as it once seemed.
This is the reality that the UN General Assembly acknowledged when it recognized a responsibility to protect endangered people in sovereign states.
But what moral obligation does this place on a particular leader like Obama?
The leadership theorist Barbara Kellerman has accused former US President Bill Clinton of the moral failure of insularity for his inadequate response to the genocide in Rwanda in 1994.
In one sense, she is right.
But other leaders were also insular, and no country responded adequately.
Had Clinton tried to send American troops, he would have encountered stiff resistance in the US Congress.
Coming so soon after the death of US soldiers in the 1993 humanitarian intervention in Somalia, the American public was in no mood for another military mission abroad.
So what should a democratically elected leader do in such circumstances?
Clinton has acknowledged that he could have done more to galvanize the UN and other countries to save lives in Rwanda.
But good leaders today are often caught between their personal cosmopolitan inclinations and their more traditional obligations to the citizens who elected them.
Fortunately, insularity is not an “all or nothing” moral proposition.
In a world in which people are organized in national communities, a purely cosmopolitan ideal is unrealistic.
As Obama wrestles with determining his responsibilities in Syria and elsewhere, he faces a serious moral dilemma.