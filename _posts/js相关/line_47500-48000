Darkness at Darfur
Darfur is shorthand for the latest example of a recurring international problem, one that gained headlines a decade ago in Rwanda.
What should the world do when a large number of people are the victims of violence originating from within their own country?
Darfur itself is a region of Western Sudan comprised of Arab and African Muslims.
Conflict erupted in early 2003 when rebels of the Sudan Liberation Movement attacked government troops in an effort to gain greater autonomy and resources for their region.
Sudan government aircraft and government-supported troops (known as jangaweed) retaliated against not only armed rebels but also against civilians deemed to be supporting them.
Villages have been emptied, women raped, non-Arab men killed.
The origins of the current crisis may be in some dispute, but the costs are not.
More than 50,000 men, women and children have lost their lives; more than 1.5 million have been made homeless.
This is arguably genocide, a word used by the U.S. government but by few others to describe what is going on in Darfur.
Meanwhile, world leaders are debating what if anything should be done.
UN Security Council Resolution 1564, passed on 18 September 2004, reserves the bulk of its criticism for the government of Sudan.
But the UN is not yet prepared to go beyond words.
The resolution threatens that the Security Council will consider imposing sanctions against Sudanese leaders or against the country’s important oil sector, but introduces no penalties at this time.
Why the hesitation?
More than anything else it stems from international reluctance to challenge any government over what it is doing within its own territory.
This reflects a widely-held view of sovereignty, one that allows governments to do essentially what they want within their own borders.
Such thinking is inadequate and outmoded.
To begin with, there is a moral element. There is something wrong in looking the other way when one’s fellow human being is being slaughtered.
We all have some basic obligation to one another.
There are as well pragmatic considerations.
In a global world, what happens within one country can all too easily affect others.
For example, refugees leaving Sudan can strain the stability of neighboring Chad.
Opposition to genocide and other large-scale acts of violence against a population also reflects the established principle that citizens as well as governments have rights.
This principle is enshrined in various international documents, beginning with the 1948 Universal Declaration of Human Rights.
Governments ought not to be allowed to massacre their own people. And weak governments should not be allowed to permit massacres to take place on their own territory even if they are not themselves carrying out the massacre.
What all this adds up to is a requirement for a concept of state sovereignty that is less than absolute.
To be precise, we need to embrace a contractual approach to sovereignty, one that recognizes the obligations and responsibilities as well as the rights of those who enjoy it.
Such an approach to sovereignty would essentially communicate to governments and their leaders that the rights and protections they associate with statehood are in fact conditional, and that governments and leaders would forfeit some or, in extreme cases, all of these rights and protections if they failed to meet their obligations.
This idea will only have an impact if the international community is prepared to go beyond voicing this principle and accept the necessary consequence: that other states and the world at large have a right and a duty to act to protect innocent life when it is jeopardized on a large scale.
Some movement in just this direction was suggested by widespread international support for the humanitarian interventions in Bosnia, Kosovo, and East Timor.
Another sign of change is the basic document (“Constitutive Act”) of the African Union, the regional organization launched in July 2000 to replace the ineffective Organization of African Unity.
After citing the principle of non-interference by one member state in the internal affairs of another, the document goes on to declare “the right of the Union to intervene in a member state pursuant to a decision of the assembly in respect of grave circumstances, namely war crimes, genocide and crimes against humanity.”
Intervention in such circumstances can take any number of forms, from public rhetoric and private diplomacy to economic and political sanctions to armed intervention.
All of which brings us back to Darfur. What needs doing?
There is a need for massive assistance to the displaced people of Darfur. Those who have survived conflict require help if they are not to succumb to disease and starvation.
There is also every reason to renew diplomatic efforts to bring about a lasting cease-fire and, following that, a settlement that addresses the grievances that helped bring about this crisis in the first place.
Two other points require highlighting, though.
First, and consistent with UN Security Council Resolution 1564, countries should provide the African Union with the logistical, material, and financial help it has asked for.
With such support, AU-authorized troops could guard the refugee camps and, over time, protect villages so that men, women and children could return home in safety.
Second, the UN ought to make good on its threat and impose sanctions against the Sudanese government unless it stops using its aircraft to destroy villages and unless it stops supporting the jangaweed.
Criminal indictments for war crimes ought to be issued against specific officials who do not comply.
It is important that the world act, not simply to save the people of Darfur, but to prevent future Darfurs.
A great deal of innocent human life depends on it.
Darkness or Dawn in Belarus?
WASHINGTON, DC – As pro-democracy protests sweep the Arab world, Belarus, Europe’s grim quasi-Soviet redoubt, has taken a turn for the worse since President Aleksander Lukashenko violently suppressed post-election demonstrations in December and imprisoned seven of the nine candidates who stood against him.
Wages are much lower than official figures suggest – perhaps as low as $200-300 per month.
The unemployment rate is 0.7%, but largely because those who register at labor offices are put to work in community-service jobs paying $10-15 per month.
Prices are high, owing to trade restrictions and government support for inefficient state enterprises.
Economic growth, pumped up during the run-up to the presidential election by enormous fiscal spending – two-thirds of the economy is state-owned – was officially 7.6% in 2010, but the rate has plummeted since, though no one is saying by how much.
Whatever its past successes in maintaining basic living standards, today it is evident that Belarus cannot match its neighbors’ dynamism.
While many pensioners and some workers remain content with life under Lukashenko, young people, and those with the most education and talent, voted against him in December – and much evidence suggests that he won less than 50%.
That, apparently, was too much for the president’s fragile ego.
Amid mounting repression and growing Western protest, Lukashenko vowed in his inauguration address in January that he would tolerate no threat to “stability.”
And yet Lukashenko is no fool.
He might not respect the outcome of elections, but he cannot afford to ignore what the last one revealed about the depths of his regime’s unpopularity.
His problem is the regime’s utter loss of legitimacy, which means that repression will not be enough.
He needs to strike a new bargain with Belarusians, and he knows it: economic modernization with political “stability.”
The first steps will be taken this year.
The regime currently operates a bloated, inefficient industrial sector to maintain employment, which is possible because the government derives most of its revenue from natural-resources trade (mainly refined Russian oil and domestic potash deposits) and transit fees for deliveries of Russian oil and natural gas to Europe.
Yet now, Lukashenko wishes to harness entrepreneurship and foreign direct investment in a bid to modernize the economy.
Belarus already has a budding software industry that reports net annual revenues of $300 million.
And, with Austrian money and World Bank support, officials are drawing up a privatization program that will cover a significant proportion of the country’s mainly state-owned industry.
While the specific companies to be sold have not been announced, the government has indicated that it would sell a 25% share of the giant potash producer Belaruskali.
Lukashenko hopes that dramatic economic reforms will win Western support, disarm foreign and domestic critics, and achieve economic modernization under authoritarian rule.
But he risks angering laid-off workers and empowering inefficient crony capitalists, while moving too slowly to satisfy the desires of those who voted (and took to the streets to protest) against him.
And Belarus is no China: it is not too big to be punished for its government’s behavior.
Yet Western policy towards Belarus must be carefully modulated, and it should emphasize Europe’s soft power – the attractiveness of its social model to an increasing number of Belarusians.
The West needs to cultivate relations with these people and invest in the economy, while rejecting the brutality of the regime, which may become easier if privatization moves ahead.
Visa policy will be a big part of the solution.
Belarusians currently pay €60 for an EU tourist visa, which is a major obstacle in view of low wages.
Poland has announced that it will grant Belarusians visas at no cost, while denying visas to a long list of regime officials that it says were involved in the recent electoral fraud.
The rest of the EU has followed suit on sanctions against top officials, but could do more to liberalize travel for citizens.
Economic engagement is more controversial.
Investment in Belarus arguably strengthens the regime, but it also may be necessary for the eventual emergence of a democratic Belarus.
In fact, his work revolutionized the field.
Before Darwin, philosophical speculation shaped our psychological understanding.
But even great philosophers – Plato, Aristotle, Hobbes, Hume, Locke, Kant, Schopenhauer, Nietzsche, and others – could only describe current mental events and behaviors; they could not explain their causes.
Darwin provided the profound understanding that evolution has influenced the shape of our minds as strongly as it has the shape of our bodies.
Since humans evolved from the same primate ancestor as modern chimpanzees or gorillas, he suggested one could learn more by comparing human instincts, emotions, and behaviors to those of animals than one can surmise from subjective speculation.
As he put it, “he who understands baboon would do more towards metaphysics than Locke.”
Philosophy is inadequate to understand the roots of human psychology, because self-reflection does not make us aware of the forces that drive most of our reactions to the environment.
Rather, we are subject to inborn tendencies, which develop through the reciprocally influential forces of natural and sexual selection.
Natural selection is the process by which the variants within a species that are best adapted to survive in their environment win the reproductive contest – at least until an even better-adapted variant comes along.
The traits that enable people to feed and protect themselves increase the likelihood that they will live long enough to produce offspring, whom they will be able to feed and protect until maturity.
In a sense, sexual selection is the psychological extension of natural selection.
But, instead of gaining an advantage from traits that enhance one’s ability to survive, one gains an advantage from qualities that potential mates have evolved to find appealing.
Given that humans’ sexual choices determine who reproduces most and, in turn, which physical and psychological features are favored over time, a trait that may not help a person to survive can still provide a reproductive advantage that is passed along to offspring.
In other words, in choosing a mate, one shapes the course of evolution.
Moreover, the principle of sexual selection implies that, in addition to regulating bodily functions, the nervous system indirectly influences the progressive development of bodily and mental structures such as ornamental appendages; cognitive skills like musical ability; and characteristics such as courage and perseverance.
Peacocks have evolved to have long, colorful feathers simply because peahens have evolved to find them attractive.
Darwin explained that such qualities are propagated and enhanced over generations, through “the exertion of choice, the influence of love and jealousy, and the appreciation of the beautiful in sound, color, or form.”
Indeed, while natural selection is blind, sexual selection has an eye for beauty – although the nature of beauty is always in the eye of the beholder.
Given that human psychology has developed through a sometimes-uneasy balance of natural and sexual selection, evolution and psychology influence and interact with each other.
Darwin’s contributions to understanding human psychology involved careful study of child development, which he reported in 1877 in “Biographical Sketch of an Infant.”
For the first three years of his first-born son William’s life, Darwin observed him with the practiced eye of a naturalist, recording developments as diverse as his ability to follow a candle with his eyes to the first manifestations of conscience.
Darwin also pioneered the experimental tools of scientific psychology, such as the use of photographs of facial expressions and surveys to determine the universality of human emotions.
Darwin had already made most of his major psychological discoveries even before he identified natural selection as the mechanism of evolution – but he waited 35 years before publishing his findings.
This decision can be partly attributed to his meticulous approach to research, which entailed carefully collecting and studying evidence before presenting theories.
But Darwin also knew that, if he needed time to accept his own conclusions, the rest of the world was not ready to face such a materialist view of humanity.
He shied away from the inevitable confrontation with critics – among whom were friends and colleagues.
By the time Darwin died, his ideas had gained significant influence among psychologists and neuroscientists – even if they did not always fully realize it.
Sigmund Freud never met Darwin, but most of his mentors were enthusiastic Darwinists.
Just as Isaac Newton revolutionized astronomy and physics by “standing on the shoulders” of his predecessors, Freud built on Darwin’s evolutionary insights in order to understand psychological symptoms, dreams, myths, art, anthropology, and much more.
Freud’s biographer, Ernest Jones, was mistaken in calling Freud “the Darwin of the mind.”
Darwin himself was the Darwin of the mind; Freud was his great popularizer.
Since Darwin, academic psychology has expanded significantly, enriched by the sophisticated tools of cognitive science, cybernetics, and brain imaging.
But most of these developments have been derivative elaborations of Darwin’s grand evolutionary model.
The fundamentals of our conception of human nature can all be found in Darwin’s notebooks, written 175 years ago and before his thirtieth birthday.
The Hidden Danger of Big Data
CAMBRIDGE – In game theory, the “price of anarchy” describes how individuals acting in their own self-interest within a larger system tend to reduce that larger system’s efficiency.
It is a ubiquitous phenomenon, one that almost all of us confront, in some form, on a regular basis.
For example, if you are a city planner in charge of traffic management, there are two ways you can address traffic flows in your city.
Generally, a centralized, top-down approach – one that comprehends the entire system, identifies choke points, and makes changes to eliminate them – will be more efficient than simply letting individual drivers make their own choices on the road, with the assumption that these choices, in aggregate, will lead to an acceptable outcome.
The first approach reduces the cost of anarchy and makes better use of all available information.
The world today is awash in data.
In 2015, mankind produced as much information as was created in all previous years of human civilization.
Every time we send a message, make a call, or complete a transaction, we leave digital traces.
We are quickly approaching what Italian writer Italo Calvino presciently called the “memory of the world”: a full digital copy of our physical universe.
As the Internet expands into new realms of physical space through the Internet of Things, the price of anarchy will become a crucial metric in our society, and the temptation to eliminate it with the power of big data analytics will grow stronger.
Examples of this abound.
Consider the familiar act of buying a book online through Amazon.
Amazon has a mountain of information about all of its users – from their profiles to their search histories to the sentences they highlight in e-books – which it uses to predict what they might want to buy next.
As in all forms of centralized artificial intelligence, past patterns are used to forecast future ones.
Amazon can look at the last ten books you purchased and, with increasing accuracy, suggest what you might want to read next.
But here we should consider what is lost when we reduce the level of anarchy.
The most meaningful book you should read after those previous ten is not one that fits neatly into an established pattern, but rather one that surprises or challenges you to look at the world in a different way.
Contrary to the traffic-flow scenario described above, optimized suggestions – which often amount to a self-fulfilling prophecy of your next purchase – might not be the best paradigm for online book browsing.
Big data can multiply our options while filtering out things we don’t want to see, but there is something to be said for discovering that 11th book through pure serendipity.
What is true of book buying is also true for many other systems that are being digitized, such as our cities and societies.
Centralized municipal systems now use algorithms to monitor urban infrastructure, from traffic lights and subway use, to waste disposal and energy delivery.
Many mayors worldwide are fascinated by the idea of a central control room, such as Rio de Janeiro’s IBM-designed operations center, where city managers can respond to new information in real time.
But with centralized algorithms coming to manage every facet of society, data-driven technocracy is threatening to overwhelm innovation and democracy.
This outcome should be avoided at all costs.
Decentralized decision-making is crucial for the enrichment of society.
Data-driven optimization, conversely, derives solutions from a predetermined paradigm, which, in its current form, often excludes the transformational or counterintuitive ideas that propel humanity forward.
A certain amount of randomness in our lives allows for new ideas or modes of thinking that would otherwise be missed.
And, on a macro scale, it is necessary for life itself.
If nature had used predictive algorithms that prevented random mutation in the replication of DNA, our planet would probably still be at the stage of a very optimized single-cell organism.
Decentralized decision-making can create synergies between human and machine intelligence through processes of natural and artificial co-evolution.
Distributed intelligence might sometimes reduce efficiency in the short term, but it will ultimately lead to a more creative, diverse, and resilient society.
The price of anarchy is a price well worth paying if we want to preserve innovation through serendipity.
My Best Investment
JAKARTA – When I was boy, I dreamed of becoming a doctor.
I was born in Indonesia in the early 1950’s, a time when most families in my country lacked access to health care.
As a result, thousands of children died each year from preventable diseases such as measles, polio, and malaria.
But revolutionary breakthroughs in medicine were starting to turn the tide on these killers, and Indonesia’s doctors were celebrated as heroes.
I wanted to be a hero, too, so I studied hard and enrolled in medical school.
My plans changed, however, when my father got ill.
He was a hard-working man who made pedicabs in Surabaya, and I left university when he could no longer manage the family business.
Ultimately, I became a successful entrepreneur and established a bank that is now one of Southeast Asia’s largest financial institutions.
Looking back, I have no regrets.
Indeed, I know that I have been incredibly blessed.
Millions of children in developing countries in Africa, Asia, and the Western Pacific are forced into extreme poverty when a parent gets sick or dies.
And millions more suffer from diseases that prevent them from leading a healthy and productive life.
That is why I have decided to invest $65 million in the Global Fund to Fight AIDS, Tuberculosis, and Malaria.
When the Global Fund was created a decade ago, HIV incidence was rising around the world, and the drugs used to treat the virus were still prohibitively expensive.
Malaria was killing one million people each year, with mortality concentrated among pregnant women and children under the age of five.
More than two million people were dying from tuberculosis, because they lacked access to low-cost, first-line treatment.
Since then, the Global Fund has played a key role in reversing the course of these epidemics.
Worldwide, HIV incidence has fallen by one-third, and the cost of HIV drugs has dropped more than 99%.
The number of African households that sleep under insecticide-treated bed nets has risen from 3% to 53%, and malaria deaths have fallen by one-third.
And TB mortality has dropped more than 40%.
Altogether, support provided by the Global Fund has saved more than nine million lives – a remarkable achievement.
But the impact of the Global Fund extends far beyond lives saved.
When an HIV-positive mother receives antenatal treatment so that her child can be born HIV-free, we change the future.
When a child is spared cerebral malaria and the lifelong mental disabilities that it can cause, new possibilities are created.
The world becomes a better place when we commit to giving our children a better start.
The Global Fund also plays a critical role in strengthening health care in developing countries.
Its resources have helped to train new generations of doctors, nurses, and technicians, while dramatically improving the overall quality of maternal and child health.
These investments are helping countries take direct ownership of the fight against infectious disease, ultimately reducing their dependence on foreign aid.
For example, support from the Global Fund has helped Indonesia, which has the world’s fourth-highest TB burden, develop an efficient and effective national TB-control program.
As a result, more than one million active cases have been successfully treated, while the incidence of the disease – which has cost Indonesia’s economy millions of dollars in lost productivity – has been driven down.
With technical support like this, Indonesia’s government is creating an affordable and sustainable national health-care system that will be fully financed with domestic resources by 2019.
I believe that the Global Fund is one of the smartest investments that we can make – as many governments have understood from the outset.
The United States has been the Global Fund’s largest supporter, but many others have done their part.
For example, Australia has provided US$410 million to the Global Fund since 2002, an amount that has enabled nearly 200,000 people to receive lifesaving HIV treatment, 80,000 people to receive TB treatment, seven million people to be cured of malaria, and 14 million bed nets to be delivered to families in need.
The Global Fund has brightened the futures of hundreds of thousands of children across Africa and Asia – a heroic achievement that should make donor countries’ citizens proud.
I hope that others with the means to support its work will invest so wisely.
Does Europe Need Britain?
NEW YORK – Many people in the United Kingdom believe that their country can do perfectly well outside the European Union.
Members of the UK Independence Party even think that Britain would do better, as do a considerable number of Conservative “Euro-skeptics.”
They dream of Britain as a kind of Singapore of the West, a commercial powerhouse ruled from the City of London.
That is why Prime Minister David Cameron felt obliged to offer the British people a referendum on a simple question: in or out.
Cameron does not personally want Britain to leave the EU, but he knows that some form of democratic consent is needed for future British governments to settle the matter.
The year of the promised referendum, 2017, is comfortably far away.
Many things may change in the meantime.
If the eurozone forges ahead, what countries outside the zone do may not matter much anymore.
Moreover, other Europeans may end up agreeing with Cameron that ever closer political union in Europe is undesirable – if they have a choice, that is, which is by no means certain.
In the meantime, there is another question to be considered: how many Europeans want Britain to stay in the EU?
The answer depends partly on nationality.
The smaller northern countries, such as the Netherlands, have traditionally wanted Britain to be in.
Without Britain, they would be bossed around by France, and even more so by Germany.
And yet, as memories of World War II fade, more and more people in the Netherlands and Scandinavia feel content to be under Germany’s powerful wings.
But Germany itself would probably prefer to keep its British partner, rather than face the Mediterranean countries alone.
Culture still matters.
And the Germans have much in common with the British – more than they do with the Greeks, or even the Italians.
France is a different matter.
According to a recent poll, 54% of the French would prefer Britain to leave the EU.
This, too, might have something to do with culture.
Britain never was very popular in France.
President Charles de Gaulle blocked British entry into the European Economic Community twice.
Like many French leaders, de Gaulle was deeply suspicious of the “Anglo-Saxons.”
France, in his grandiose view, was the natural guardian of European values, which, according to him, were coextensive with French values.
In 1930, Winston Churchill said of his country: “We are with Europe, but not of it.”
It is a sentiment still shared by many in Britain.
De Gaulle agreed.
As he once put it, somewhat ironically, Britain would lose its identity as a member of a European union, and this would be a great pity.
But culture and nationality, or even Gaullist chauvinism, cannot explain everything.
There is an important political dimension to the pro- or anti-British sentiments in Europe.
The French who said they wanted Britain to leave the EU were largely on the left, while many who held the opposite view were further to the right.
It is not entirely clear why, though it is probably because the right includes neo-liberals, who share the British attitude to business and free trade.
Like leftists everywhere, the French left favors a large degree of state control of the economy, together with technocratic rather than liberal solutions to social and economic problems.
This type of thinking has played a vital role in the development of European institutions.
Jean Monnet, one of the godfathers of European unification, embodied this tendency – a born bureaucrat who distrusted politicians.
Democratic politics is messy and divisive, and riddled with compromises.
Monnet hated all that.
He was obsessed by the ideal of unity.
And he wanted things to get done, uncompromised by political wheeling and dealing.
Monnet and other European technocrats were not exactly opposed to democracy, but in their zeal to unify Europe’s diverse nation-states, they often appeared to ignore it.
The Eurocrats knew what was best for Europe’s citizens, and they knew what needed to be done.
Too much public debate, or interference from citizens and their political representatives, would only slow things down.
Hence the typical EU language about “unstoppable trains” and “irreversible decisions”: citizens are not supposed to question the wisdom of great planners.
This emphasis on planning was one reason why the “European project” always appealed to the left – and not only in France.
The technocratic belief in ideal models is inherently utopian.
Those on the left also shared a deep aversion to nationalism, born of two disastrous European wars.
The British, whose Churchillian nationalism helped them to prevail against Hitler’s attacks, never shared this aversion.
And their deep pride in Britain’s liberal-democratic tradition made them suspicious of meddling Brussels bureaucrats.
Some of this is doubtless the result of chauvinism, even xenophobia.
How can one possibly share political authority with foreigners?
But it would be wrong simply to dismiss British doubts about the European drive toward greater unity.
It is not just a nationalist reaction.
Many Europeans now resent the expanding powers of EU bureaucracy.
British resistance to grand European plans is the democratic grit in an enterprise that could become authoritarian, despite having the best intentions, and should serve as a necessary corrective to the utopianism of the technocrats.
Those who favor European unification should take criticisms of its political flaws very seriously.
Doing so is the only chance to ensure that a united Europe, whatever form it takes, will be democratic, as well as economically beneficial.
That is why Europe needs Britain: not as an offshore center of banking and commerce, but as a difficult, questioning, stubbornly democratic partner.
David Cameron’s Culture War
NEW YORK – British Prime Minister David Cameron’s government has announced some of the most draconian public-sector cuts any developed country government has ever attempted. Indeed, his minister of education recently declared that funding for Britain’s universities would be slashed by as much as 40%.
But the most shocking aspect of the move is that arts and humanities departments will be targeted more aggressively than science and engineering, which are supposedly better for business.
The war against the arts and humanities is nothing new – though this is the first time that the fight has migrated so directly to Britain.
Ronald Reagan pioneered a wave of policy and propaganda in the United States in the 1980’s that demonized the National Endowment for the Arts.
Ever since, Republican governments in the US have slashed funding for ballet, poetry in schools, and sculpture, while demagogues like former New York Mayor Rudolph Giuliani have gained political traction by attacking controversial visual arts.
But the Cameron government’s approach is more sinister than the old right-wing tactic of taking aim at disciplines that can be derided as effete.
The British cuts reveal a push in developed countries – one that also started in the US – to target the kinds of education that lead to an open, vigorous civil society and a population that is hard to suppress.
In the former Soviet bloc, it was the poets, dramatists, cartoonists, and novelists whose works encoded forbidden themes of freedom, and who were targeted by the secret police.
Today, they are bullied, silenced, and tortured in places like Iran, Syria, China, and Myanmar.
Obviously, neither the US nor Britain has reached that point.
But the attack on the arts and humanities is a giant step in the direction of a pliable, dumbed-down citizenry.
Indeed, the war against the arts and humanities in the US coincided with the emergence of an increasingly ignorant and passive population and a government that serves at the pleasure of corporate interests.
Academics in the arts and humanities are notoriously bad at defending why their work has value.
But, apart from strengthening civil society and the habits of freedom, these disciplines yield bottom-line benefits as well.
Who needs to read closely, seek evidence, and make a reasoned argument – skills that the study of poetry, the novel, history, and philosophy provide?
Who needs to study languages and comparative literature?
Having lost its empire, Britain retains outsized global influence simply because of the power of its civilization and the education absorbed by its decision-makers.
That allure is why foreign students from emerging countries around the world flock to Britain, putting millions of pounds annually into the coffers of its universities.
By slashing the funding for the institutions which created that civilization, Cameron has guaranteed that tomorrow’s Britain will be a nation not of world-class politicians, writers, and cultural innovators, but of wonky technocrats raised on bad TV, with little influence beyond their tiny island.
If what has been cut is not restored, Cameron and his ideological heirs will create a nation of quiescent citizens who, like their US counterparts, are better suited to a society whose official policies are more directly aligned to the will of corporate interests.
While the fiscal savings may appear attractive to Cameron in the short term, for the British people – and for the rest of the world, which benefits from Britain’s liveliness, civilization, and tradition of democracy – the cost is far too high.
The UK’s Immigration Distraction
LONDON – Do British voters hate foreigners, or merely freeloaders?
That is essentially the question British Prime Minister David Cameron posed in his long-awaited speech on immigration from other European Union countries, delivered last month at the headquarters of the construction-equipment manufacturer JCB.
Cameron’s gamble is that voters do not mind Poles or Lithuanians operating JCB’s machines on construction sites all over the United Kingdom.
What they mind is people immigrating to the UK to take advantage of its welfare benefits.
The speech was Cameron’s answer to the recent defections of two of his Conservative Party MPs to the anti-EU, anti-immigration UK Independence Party, which he fears could steal Conservative votes in the general election next May.
But, as clever as the speech was, it is unlikely to succeed in beating back UKIP – and it leaves the British debate about EU membership focused on the wrong issue.
To be sure, Cameron’s speech was a more statesmanlike gamble than many – even within his own government – had anticipated.
Some of his remarks in recent weeks had suggested that he might reject outright the free movement of EU citizens – one of the union’s founding principles – and dare other European governments to oppose him.
Instead, Cameron emphasized the UK’s enduring desire to be open to the world while restricting EU migrants’ welfare rights.
Specifically, he proposed requiring EU migrants to spend four years working in the UK before becoming eligible for the top-up welfare payments that low-paid British workers receive, and to end benefit payments for migrant workers’ children living in their home country.
Though such moves could be challenged in the European Court of Justice on grounds of discrimination, any cases are unlikely to arise until well after the upcoming election.
The political danger for the Conservatives is that they are misjudging public sentiment.
Perhaps voters have just as much of a problem with foreigners as they do with freeloaders.
If that is the case, reducing immigrants’ welfare benefits will be inadequate.
This is not to say that Cameron should target foreigners.
On the contrary, he must change the conversation to reassure voters that their economic future is bright.
As it stands, the British, like their counterparts elsewhere in Europe, feel overtaxed for spotty public services that are constantly being cut, and are angry that their incomes have been falling for the last five years.
As a result, many resent welfare recipients and fear competition for jobs – a sentiment that is fueling the rise of populist, anti-EU, and anti-immigrant parties across Europe.
The key difference in the UK is that UKIP is gaining ground with voters despite rapid economic growth (more than 3% annually, the highest rate in the EU).
Between now and May, Cameron must persuade voters that the British economy will continue to perform strongly, fueling an increase in real incomes.
A change of subject would also be in the national interest.
By banging on about immigration, Cameron risks making it central to the question of whether Britain should remain in the EU.
He is, after all, the politician who has promised to hold a referendum in 2017, if he is still Prime Minister, on whether to stay or leave, following a period variously described as a “renegotiation” of the UK’s membership and “reform” of the EU’s structure.
He is now making it seem as if immigration will be the key test of that effort.
That is a big problem. For starters, there is little or nothing that other EU governments can concede on the free movement of people, which is enshrined in the founding document of European integration, the 1957 Treaty of Rome.
In any case, Britain receives more migrants from outside the EU than from other EU countries.
And migrants from Poland, Italy, or France are much more likely than those from Somalia, Syria, or India to return home eventually.
If the British public truly opposes immigration, they are probably referring to non-EU immigration, not the arrival of, say, Italian university graduates.
In fact, there is a far more important issue that should serve as the main focus of the British debate on the EU: the UK’s future status inside the EU as a non-member of the eurozone.
This is what Cameron and Chancellor of the Exchequer George Osborne were focusing on until UKIP surged in the opinion polls.
It is time to renew that focus.
Cameron’s government must recognize that there is only one good reason to wonder whether the UK might be better off leaving the EU: the risk that remaining outside of the eurozone would leave Britain at a serious disadvantage in EU decision-making, even about rules affecting the single market.
It sounds a bit technical, and it is.
But sorting out the relationship between EU members within and outside the eurozone can be done in a fairly short time without new treaties; indeed, progress has already been made on this front.
Cameron and Osborne would then be able to boast that they have procured for the British public the best of both worlds: much faster economic growth than the stagnant eurozone members, together with all of the advantages of EU membership, including access to the single market and enhanced global influence.
That narrative would be far more powerful than a few tweaks to welfare rights for immigrants.
David Cameron and the EU’s Waterloo
PRINCETON – Two hundred years ago this month, at the Battle of Waterloo, Napoleon Bonaparte’s defeat at the hands of an allied army, led by the Duke of Wellington, reshaped Europe’s future.
Britain may now be poised to do so once again.
The United Kingdom, whose new majority Conservative government has pledged to hold a referendum on European Union membership by the end of 2017, perhaps even next year, is not the outlier that it is often portrayed as being.
Indeed, it is at the vanguard of the EU’s institutional atrophy.
Even if it does retain its EU membership, the UK will continue to move steadily away from Europe.
With more attractive commercial opportunities elsewhere, most European countries will follow suit.
For the EU, meeting the UK’s demands – to restrict benefits for migrant workers, limit financial regulation that could hurt the City of London, and disavow the goal of “ever closer union” – would require a fundamental transformation, including utterly unfeasible changes to the treaties that underpin European institutions.
The discussion has thus turned to the possibility of providing the UK with a special status or allowing it to opt out of more EU provisions.
But, given intensifying doubts about the benefits of integration, even that solution would risk unraveling the EU.
By emphasizing that Europe no longer offers an economic dividend, the UK’s move away from the EU will spur ever louder calls for change elsewhere.
Simply put, talk of “Brexit” has exposed Europe’s economic and political fault lines – and there is no going back.
Robert Peston, in his 2005 biography of then-Chancellor of the Exchequer Gordon Brown, described Brown’s “pragmatic view that the EU was a good thing only insofar as it delivered practical benefits of peace and prosperity to Britain.”
While the British have been particularly open about the nationalist nature of their support for European integration, other EU members have been no less mindful of their domestic interests.
In the wake of World War II, European countries’ national interests were aligned.
Nonetheless, efforts to establish political unity through a common army failed in 1954, highlighting that economic common ground was the key to European integration.
And, indeed, the 1957 Treaty of Rome, which opened national borders within the new European Economic Community, enabled the rapid proliferation of intra-European trade, thereby contributing to a shared economic recovery.
The material gains of these commercial relationships fostered empathy among Europeans, fueling increased support for – and trust in – shared institutions.
This process began later for the UK, which joined the community in 1973; but it followed a similar trajectory, with British citizens responding to economic gains by supporting increased integration.
Prime Minister Margaret Thatcher, by advocating the 1986 Single European Act, sought to maximize those gains.
In the spirit of the Treaty of Rome, she ensured that the act focused on developing an open and competitive common market, in which all members participated on equal terms.
By contrast, her German counterpart, Chancellor Helmut Kohl, often regarded as one of that generation’s preeminent European champions, offered only lukewarm support for the Single European Act.
He preferred to help steer Europe toward monetary and political union, just when its economy had begun to fall behind the rest of the world.
The result was the 1991 Maastricht Treaty – the point when European integration went into overdrive.
But the Treaty’s architects were so busy playing internal power games that they failed to recognize that monetary union could not stem Europe’s decline, especially as the United States was experiencing rapid productivity growth and Asia’s economic rise had begun.
With Europe’s post-war recovery long complete, the logic of integration had to be rethought.
Unfortunately, that did not happen.
As a result, intra-European trade and support for European institutions, having soared in the previous two decades, began to decline practically before the ink on the Maastricht Treaty was dry.
The eurozone’s protracted crisis, which began in 2008, has exacerbated this trend, with member countries still struggling to restore economic and financial stability – and likely to continue to lag behind the rest of the world in terms of GDP growth.
No institutional framework can survive unless it serves the material interests of its constituency.
In the nineteenth century, when the Tunisian craft guilds failed to adapt to industrialization, they became irrelevant, and the amins, or guild masters, were left as figureheads in shell institutions.
Today, European institutions could face the same fate.
Unencumbered by the euro, and benefiting from long-standing commercial relationships beyond Europe, Britain is in a particularly strong position to push back against EU institutions.
This may be self-serving, but it should not be a surprise; in fact, with businesses throughout Europe seeking markets elsewhere, Britain’s approach may well herald similar developments elsewhere.
European integration in the shadow of WWII was a wise and magnificent achievement.
But, with that historic task now complete, European institutions need a new rationale.
And with the EU, unlike other federations, lacking a common political destiny, that rationale must center on material benefits.
The EU must return to the fundamental driver of its success, pursuing a renewed single-market agenda that reflects the rationale of the Treaty of Rome.
Unfortunately, Europe is so divided nowadays that the ability to achieve such an outcome is not promising, with new initiatives along these lines facing “increasing political resistance.”
If Europeans merely invoke the lofty mantra of “an ever closer union,” their institutions will atrophy.
Without a new unifying objective – one based on shared material gains, not on fear of Vladimir Putin’s resurgent Russia – the European amins will soon be out of business.
Beyond Inflation Targets
EDINBURGH – Over the last three decades or so, central bankers and academics have become increasingly confident that inflation targeting is the key to preserving macroeconomic stability.
But this is virtually impossible to prove, and the 2008 financial crisis suggested to many that monetary policy should focus on more than the prices of goods and services.
So how should a revised mandate for central banks be structured to maintain their focus on low inflation while allowing monetary policy to address other issues when appropriate?
The contribution that inflation targeting makes to macroeconomic stability is difficult to discern for a simple reason: it is impossible to know what would happen if a country’s central bank pursued the opposite course.
Unable to compare outcomes directly, researchers have employed a variety of strategies to identify the impact of inflation targeting, and have typically found it to be substantial (though the effect becomes small or even zero when countries’ starting points are taken into account).
For example, a case study of the United Kingdom for 1997-2007 – a period of full inflation targets and policy independence for the Bank of England (BoE) – indicates considerable improvement from a poor starting point.
The focus on price stability was accompanied by relatively low inflation (compared to the past, as well as to other major economies), strong growth, and little output volatility.
However, over this period the UK also experienced sustained exchange-rate misalignment, which the BoE’s Monetary Policy Committee was unwilling or unable to address within its existing mandate.
The MPC also failed to respond to any of the three bouts of rapid growth in house prices that preceded the financial crisis, arguing that they were structural in nature –caused by the decline in inflation and interest rates since the 1980’s – so a monetary response was not appropriate.
In the run-up to the crisis, the US Federal Reserve Board (an informal inflation targeter) was even more committed than the MPC to this orthodox view – so committed, in fact, that it barely allocated any time or resources to analyzing house-price fluctuations.
So, while inflation-targeting central banks worked hard to nail down inflation expectations in goods and services, they made no effort to influence asset-price expectations.
Indeed, the “Greenspan put” (former Fed Chairman Alan Greenspan’s monetary-policy approach) eliminated the downside risk by setting a floor under asset prices; but it set no ceiling on the upside.
If major banks had had a “lean against the wind” strategy in reserve, that would have influenced expectations and at least partly stabilized asset prices, possibly mitigating – or even averting – the most damaging effects of the financial crisis.
The notion of integrating asset-price concerns into inflation targeting remains a source of significant controversy, with opponents citing the Tinbergen Principle: if policymakers have one instrument, the interest rate, they can pursue only one objective, price stability.
Attempts to pursue multiple objectives, the logic goes, would confuse financial markets and private-sector agents.
But central banks have pursued multiple objectives simultaneously, without bringing about bad outcomes or destroying their own credibility.
Likewise, since 2008, many central banks – under pressure from governments – have been reacting specifically to other developments in the real economy (particularly unemployment), rather than maintaining their single-minded focus on long-term price stability.
This is particularly obvious in the UK, where the government, desperate to see an economic recovery before the 2015 elections, has been pushing the BoE to introduce credit subsidies and, more recently, forward guidance.
As a result, the BoE’s decisions have been attuned to the short-run trade-off between growth and inflation – meaning that it has been making decisions about goals, not just instruments.
What central banks need now is a revised mandate that upholds price stability as the primary, long-term objective, while allowing policymakers to pursue other objectives when appropriate.
Specifically, the central bank should be able to move slowly on price stability if it regards the real economy as unacceptably weak, or if an asset-price misalignment threatens financial stability and cannot be defused quickly by the new macro-prudential instruments.
Under the current arrangements for the BoE, if inflation moves more than one percentage point away from its target (in either direction), the bank’s governor is obliged to write an open letter to the Chancellor of the Exchequer explaining the deviation and providing a plan to eliminate it, including a projected timeline.
A revised remit for the BoE – or, in some form, for other inflation-targeting central banks – could take this requirement further, specifying a primary and long-term target for inflation from which policymakers can deviate if doing so is deemed necessary.
In such cases, the central bank’s governor should be required to write an open letter explaining the bank’s decision, including how long it expects to give priority to the non-inflation objective and how it foresees returning to normal operations.
Such an arrangement would allow the central bank to take a wider view of its responsibilities in a context of transparency and accountability, but in a way that both preserves its anti-inflationary credibility and prevents politically motivated or otherwise inappropriate policy decisions.
A Long-Term Plan for Syria’s Refugees
BEIRUT – After spending just three days with refugees and aid workers in Lebanon and Turkey, the apocalyptic nature of the Syria crisis is all too apparent: more than 100,000 deaths, nine million people displaced, two million children out of school, diseases like polio resurfacing, and neighboring countries struggling to cope with waves of refugees.
Countless heartrending stories of lost husbands, wives, siblings, and children, to say nothing of homes and livelihoods destroyed, provide yet more troubling evidence of how Syria’s civil war has become a regional conflict (as the bombing of Iran’s Beirut embassy suggests).
Anti-Assad rebels are now fighting each other, as jihadists make gains.
Experts no longer talk of the conflict lasting months; they speak in terms of years, or even decades.
Despite heroic efforts by aid agencies like the International Rescue Committee (IRC) to save lives and bring hope to the region, the terrible truth is that it is not possible to protect civilians, especially from snipers and stray missiles, never mind hunger and homelessness.
Warring factions do not even recognize the notion of unaffiliated noncombatants, and flout international norms of war.
In addition to the use of chemical weapons, the United Nations estimates that 2.5 million civilians lack food, water, and medicines, because some towns and villages are too hard to reach, with an estimated 250,000 people completely cut off from outside help.
Syria’s neighbors have been overwhelmed by calls for help.
Lebanon is trying to accommodate nearly one million refugees.
In Turkey, an estimated 200,000 refugees are in official camps, but at least twice that number are struggling alone in towns and cities.
Support from around the world is fitful: only 60% of aid pledges have come in, with only a fraction actually reaching the intended beneficiaries. Although some agencies have been able to get aid supplies across national borders, they cannot get through the frontlines of the fighting to reach those caught in the crossfire.
International diplomatic efforts must therefore focus on achieving temporary ceasefires to bring in the most urgently needed help, such as polio vaccines for children.
Aid should not be a mere side show to the seemingly endless peace talks taking place in Geneva; as United Nations emergency relief coordinator Valerie Amos insisted, it must be central to those negotiations.
But, with the conflict and its impact expected to drag on for years, agencies must also plan for the longer term. This includes building capacity in neighboring states, as the World Bank is doing in Jordan and Lebanon, to provide services for refugees.
This can be done in creative ways.
The IRC, for example, is involved in three areas:
· Innovative education: Mainstream school systems in neighboring countries cannot cope with the refugee influx; and, with more than 80% of refugees living in urban areas rather than in camps, there is little point to focusing on camp-based models of teaching.
Instead, a more informal system, supported by networks of local and refugee teachers – a model that was successfully pioneered in Congo and Afghanistan – can provide accredited learning.
· Exploiting technology: Syrians are generally literate, numerate, and technologically sophisticated.
A pioneering social-networking platform called Tawasul (“Connection”), established by the IRC and the non-profit news organization Internews, has been set up to help refugees help one another through the exchange of information and advice.
· Doing business: Syrian refugees are accustomed to working in a market economy, so programs that allow them to trade, and therefore support themselves, should be encouraged.
The IRC is investing in “cash for work” programs that will help refugees (and their hosts) build businesses.
If we are to lessen the horrors of the Syrian conflict and its consequences, we must think not only about emergency action to save lives, but also about meeting longer-term needs that make those lives worth living.
Bringing medical aid into conflict zones, setting up water and sanitation facilities, and protecting victims during harsh winters are crucial to saving lives; but we must also think about how to safeguard the education and livelihoods of those who survive.
Saving General Petraeus
PRINCETON – The United States has moved from the high of a presidential election to the low of a political sex scandal in one short week.
For many Americans, the election demonstrated what is best about the country, only to be followed by the sadly familiar process of knocking heroes off their pedestals.
For many non-Americans, the election brought the welcome and reassuring victory of Barack Obama, whereas the resignation of David Petraeus as Director of the CIA was an unnecessary, self-inflicted wound.
In fact, both the election and Petraeus’s resignation are pieces of a larger whole: an America that lives up to its promises.
The election reminded many Americans that the US is a country committed to and capable of progress – of moving forward toward an ideal vision.
Obama was supported by a coalition of minorities: African-Americans, Latinos, Asian-Americans, Muslim-Americans, gay and lesbian Americans, and an under-represented majority – women – all of whom perceive continuing inequalities and injustices that need to be remedied.
But the winners were all who believe that America is, in fact, dedicated to “equal justice under law,” the words emblazoned on the pediment of the Supreme Court.
In the election of an African-American president less than a half-century after the end of official racial segregation in much of the country, these Americans see the triumph of the values enshrined in the US Constitution over America’s legacy of social, political, and economic prejudice.
They see a president committed to the advancement of all Americans, regardless of race, gender, creed, ethnic origin, sexual orientation, handicap, or economic status.
They also see a country that truly does reflect the world, attracting immigrants from every nation and giving them an equal chance to succeed as Americans.
And they see a president with a vision of a country that can rebuild its infrastructure, reform its health care, strengthen its educational system, and boost its economic prosperity in ways that require all citizens to contribute – and that will, in turn, allow all citizens to flourish.
But how is this vision connected to the resignation of Petraeus, a storied and much-decorated general before he took over the CIA, following the revelation that he had an extra-marital affair?
Judging by my Twitter feed, most foreign observers simply cannot understand why a man serving his country in one of its highest and most sensitive positions should step down over something that happened in his private life – something that directly affects only those involved and their families.
American culture, I explained, judges extra-marital affairs very harshly, so a senior official caught in such a position could easily be subject to blackmail – something that a CIA director, of all people, must avoid.
My foreign interlocutors replied that, with the affair now exposed, the blackmail threat has been removed, so Petraeus should stay in office.
Many Americans agree.
Indeed, Obama himself was reportedly reluctant to accept Petraeus’s resignation.
From my perspective, however, Petraeus did the right thing: resigning was the only course open to him if he is to have any chance of repairing his reputation.
Petraeus, after all, had been General Petraeus, a four-star general who had spent his life in the military, commanding the US missions in Iraq and Afghanistan.
He graduated from – and later taught at – the US Military Academy at West Point, an institution guided by the motto “Duty, Honor, Country.”
In our cynical age, many might scoff at such an old-fashioned motto (or, indeed, at the power of any motto or slogan).
West Point cadets do not.
As General Douglas MacArthur told them in 1962, those three words “build your basic character.
They mold you for your future roles as the custodians of the nation’s defense.
But, by and large, the men and women of the US military believe in these ideals and do their best to live up to them, just as US citizens generally believe in their Constitution’s lofty words and seek to correct their national shortcomings.
Petraeus violated his own personal code of honor and duty toward his wife and family – and thus, in his eyes, toward his country, particularly to the men and women whom he was entrusted to lead at the CIA.
When his affair came to light, he faced up to his failure, took responsibility for the consequences, and did what he believed that duty, honor, and country required.
The hubbub of tawdry disclosures and investigations addressing every aspect of a widening scandal may well last for weeks.
In the meantime, Americans can only hope that their national elected representatives show an equal willingness to face up to and take responsibility for their failures, pettiness, and insistence on putting partisanship ahead of the country’s manifest and urgent needs.
These officials must now carry out their most fundamental duty: to govern.
They must be willing to negotiate in good faith and compromise in order to enact laws, solve problems, avert crises, and build faith in the future.
Let us hope that their oaths to defend and uphold the Constitution are more than just words.
Every Breath You Take
BANGKOK – Fearsome stories about migrating Indonesian haze, post-Diwali smog in northern India, and the return of the “airpocalypse” in China tell of Asia’s recent air-pollution woes.
Not confined to Asia, outdoor particulate pollution claims over 3.1 million lives worldwide every year, five times the number of deaths from malaria and slightly less than double the current AIDS death rate.
Airborne pollutants, especially fine particles (smaller than 2.5 microns, or roughly the width of a strand of a spider web), enter deep into the lungs and from there enter the blood stream, causing cardiopulmonary disease, cancer, and possibly premature births.
Just how significant are these health risks?
Unfortunately, discussion of the subject is often opaque.
Poor air quality is often described as reaching a certain “AQI” (Air Quality Index) level, or as being a certain degree above a particular World Health Organization standard.
But the general public might better understand the situation if it were framed in terms that compare easily with more familiar hazards.
For instance, the immediate risks of breathing polluted air could be described in terms of the “micromort,” the unit representing a one-in-a-million chance of dying.
An average person on an average day faces a risk of roughly one micromort from non-natural causes.
This provides a useful starting point for comparison: the risk of scuba diving, for example, is around five micromorts per dive; sky diving is ten micromorts per jump; and giving birth in the United Kingdom is around 120 micromorts.
By comparison, breathing in Beijing on its most polluted days equals approximately 15 micromorts.
But the more troubling risks from air pollution arise from chronic exposure.
This can be expressed in terms of “microlives” – a unit developed by Cambridge statistician David Spiegelhalter to describe a person’s cumulative risks over a lifetime.
One microlife represents 30 minutes of an average young adult’s expected lifespan.
The average person uses up about 48 microlives per day; but lifestyle affects how fast one expends one’s microlives.
Settling in smoggy Beijing will use up roughly an additional 2-3 microlives per day, implying a reduction in life expectancy of almost three years.
Living in Hong Kong or Santiago, Chile, will cost one additional microlife per day, whereas daily life in New Delhi, one of the world’s most polluted cities, costs an estimated 4-5 microlives.
By comparison, smoking four cigarettes a day will cost the smoker around two microlives, roughly equivalent to living in Beijing.
But the aging clock can also be slowed down – 20 minutes of daily exercise will extend life expectancy by two microlives per day (unless done in the smog), and drinking two or three cups of coffee daily saves an additional microlife per day.
Furthermore, recent research suggests that some, if not all, of the microlives lost from living in Beijing could be recovered by moving to, say, Vancouver, with its pristine air.
Besides skipping town, one can mitigate at least some of the risks by limiting one’s exposure on particularly hazardous days.
If citizens had access to current air-quality data, they could choose to take protective measures, such as minimizing physical exertion, staying indoors (ideally in filtered air), and wearing a mask (an N95-rated mask, at a minimum, not a surgical mask).
Unfortunately, air-quality information, especially for the most pernicious fine particles, is not easily available in many highly polluted cities.
But such monitoring is not beyond the reach of developing countries, because the necessary equipment is not prohibitively expensive.
Ideally, air-quality data should be collected, translated into easily understandable language, and widely disseminated in real-time via social media so that city dwellers can take appropriate action (particularly important for vulnerable individuals).
In this regard, China has made big strides, providing a good example for other developing countries to follow.
Of course, governments should not stop at monitoring; they should take active measures to reduce air pollution.
Against the pressure of Asia’s rapid urbanization and industrial development, this will take sustained effort, involving complex policy decisions and painful economic trade-offs.
Making air-quality information available to all – essentially democratizing the data – allows people to engage better in the debate on what sacrifices are acceptable in the fight against air pollution; it also provides basic input for desperately needed research into the health effects of these new highly polluted environments.
These far-reaching health effects, in addition to the immediate benefit of empowering citizens to protect themselves, should spur even the most financially strapped governments to start a transparent air-quality monitoring campaign today.
A New Progressive Political Economy
LONDON – In an article in Foreign Affairs entitled “The Future of History,” Francis Fukuyama pointed out that, despite widespread anger at Wall Street bailouts, there has been no great upsurge of support for left-wing political parties.
Fukuyama attributed this – rightly, I believe – to a failure of ideas.
The 2008 financial crash revealed major flaws in the neoliberal view of capitalism, and an objective view of the last 35 years shows that the neoliberal model has not performed well relative to the previous 30 years in terms of economic growth, financial stability, and social justice.
But a credible progressive alternative has yet to take shape.
What should be the main outlines of such an alternative?
First, a progressive political economy must be based on a firm belief in capitalism – that is, on an economic system in which most of the assets are privately owned, and markets largely guide production and distribute income.
But it must also incorporate three defining progressive beliefs: the crucial role of institutions; the need for state involvement in their design in order to resolve conflicting interests and provide public goods; and social justice, defined as fairness, as an important measure of a country’s economic performance.
It was a great mistake of neoclassical economists not to see that capitalism is a socioeconomic system, and that institutions are an essential part of it.
The recent financial crisis was made far worse by profound institutional failures, such as the high level of leverage that banks were permitted to have.
Empirical research has shown that four sets of institutions have a major impact on the performance of firms and, therefore, on a country’s economic growth.
These include the institutions underpinning its financial and labor markets, its corporate-governance arrangements, its education and training system, and its national system of innovation (the network of public and private institutions that initiate and diffuse new technologies).
The second defining belief of progressive thinking is that institutions do not evolve spontaneously, as neoliberals believe.
The state must be involved in their design and reform.
In the case of institutions underpinning labor and financial markets, as well as corporate governance, the state must mediate conflicting interests.
Likewise, a country’s education and training system and its national system of innovation are largely public goods, which have to be provided by the state.
It should be clear that the role for the state that I have been describing is an enabling or market-supporting one. It is not the command-and-control role promoted by traditional socialists or the minimalist role beloved by neoliberals.
The third defining belief of progressive thinking rejects the neoliberal view that a country’s economic performance should be assessed solely in terms of GDP growth and freedom.
If one is concerned with a society’s wellbeing, it is not possible to argue that a rich country in which the top 1% hold most of the wealth is performing better than a slightly less wealthy country in which prosperity is more widely shared.
Moreover, fairness is a better measure of social justice than equality.
This is because it is difficult to devise practical and effective policies to achieve equality in a market economy.
Moreover, there is a real tradeoff between equality and economic growth, and egalitarianism is not a popular policy even for many low-income people.
In my experience, trade unions are much more interested in wage differentials than in a simple policy of equal pay for all.
These are the core principles that I believe a new progressive political economy should embrace.
I also believe that Western countries that do not adopt this framework, and instead cling to a neoliberal political economy, will find it increasingly difficult to innovate and grow.
In the new global economy, which is awash with cheap labor, Western economies will not be able to compete in a “race to the bottom,” with firms seeking ever-cheaper labor, land, and capital, and governments seeking to attract them by deregulating and shrinking social benefits.
The only way Western economies will be able to compete and improve their standard of living is by seeing themselves as being involved in a race to the top.
That is, firms must improve their value added through innovation in existing industries, and by developing the capability to compete in new and more sophisticated industries, where value added is generally higher.
Companies will be able to do this only if governments abandon the belief that they have no role to play in the economy.
In fact, the state has a key role to play in providing the conditions that enable dynamic companies to innovate and grow.
A Wrong Turn for Human Rights
NEW YORK – The world has plunged into a period of brutality, with impunity for the perpetrators of violence.
Syria is suffering untold civilian casualties as a divided United Nations Security Council sits on the sidelines.&nbsp;Gaza was pummeled to dust yet again with the world watching on.&nbsp;Iraq is in flames, with no end in sight.
Atrocities are mounting in South Sudan and the Central African Republic, which are also being swept by an epidemic of sexual violence.
Even Europe is not immune: a civilian aircraft was shot down over a conflict zone in eastern Ukraine, and officials were prevented from investigating.
Twenty-five years after the fall of the Berlin Wall, and more than a decade after the establishment of the International Criminal Court (ICC), shockingly little is being done to stop these abuses, and the prospects of the victims ever getting justice, let alone bringing the perpetrators to account, seem ever more remote.
For many years, the world seemed to be progressing toward greater recognition of human rights and demands for justice.
As democracies emerged in Latin America and Central and Eastern Europe in the 1980s and 1990s, these issues assumed increasing importance.
Although wars, conflicts, and atrocities continued, the global powers tried, and occasionally managed – albeit chaotically and usually late – to stop the killing.
Moreover, the international community created frameworks for justice to deal with the consequences of violence, a move that was scarcely imaginable during the Cold War.
New UN-backed international and hybrid tribunals were created to bring to account perpetrators of atrocities in the Balkans, Rwanda, Sierra Leone, and Cambodia.
The ICC, with jurisdiction over atrocities committed in 122 member states, was established to try cases referred to it by state parties or the Security Council (even though three permanent members – the United States, Russia, and China – have not ratified or acceded to the Rome Statute, which places parties under ICC jurisdiction).
In addition, many governments, with UN support, created mechanisms for transitional justice at home, including more than 40 “truth commissions” (such as in Argentina, El Salvador, East Timor, Morocco, and South Africa), reparation programs, and prosecutions.
None of these efforts has been perfect, but they have given victims a voice and recognized their suffering, while signaling to culprits that their crimes will not be forgotten.
Such measures have also deeply affected, and in some cases transformed, public discourse for the better.
Today, however, the international community appears to be backsliding on its human-rights commitments.
The world’s emerging powers lack any sense of urgency in addressing abuses, preferring the pursuit of narrower, short-term interests to investing in long-term peace and justice.
Without robust support from the international community, the institutions of justice are coming under pressure – and losing their momentum.