The more Europe suffers, the more its people will correctly perceive an incrementalist agenda for reform as nothing more than an exercise in futility.
The Crisis of Consumerism
FLORENCE – Major economic crises are inevitably also structural milestones.
There is no simple return to a pre-crisis normalcy. Something changes permanently.
As we learned in 2009, patterns of expectations and demand take a new shape.
Our current crisis is not simply a blowback effect of financial globalization.
Financial globalization misfired because it took a bet on a type of economy that was becoming unsustainable.
During the past quarter-century, but especially over the five years leading up to 2008, the world seemed to revolve around the American consumer. 
American-style consumption offered a new model of economic development. It inspired widespread emulation.
Over the course of a few decades, major city centers across the world began to resemble each other much more closely, with the same brands, designs, and lifestyles.
Consumption or, more precisely, consumerism , appeared to be globalized.
American universities delivered new curricula based on studies of consumption and consumerism.
In the aftermath of the terrorist attacks of September 11, 2001, President George W. Bush advised Americans that they should not allow the trauma of the attacks to interfere with their ordinary shopping, and implied that buying had become a patriotic duty and virtue.
The United States had become the world’s consumer of last resort.
The post-2007 crisis was not simply a financial affair.
It began as a result of weakness in a specific sector – residential mortgages – after a bubble in home prices allowed many Americans to borrow massively, and often disastrously, against the putative value of their equity in order to finance purchases. 
The global imbalances that many people view as being at the root of the problem reflected savings rates of near zero in the US, as well as in those economies that seemed best to emulate the American model – the United Kingdom, Ireland, and Spain.
In the course of the crisis, these debt-ridden economies’ consumers abruptly changed their purchasing habits. Savings rates shot up.
Spending on automobiles ground to a halt, until government stimulus programs in many countries revived it.
But schemes such as public subsidies for scrapping old and fuel-inefficient cars simply led many people to re-schedule their purchases.
The automobile market was rescued in 2009 at the expense of sales in 2010 and subsequent years.
The crisis also revealed the extent of the massive over-capacity in the US retail market, with estimates that at least one-fifth of American shopping malls would be forced to close.
The response to the crisis will be a hastening of the movement toward on-line purchases, in which physical location is no longer a preeminent part of shopping decisions.
High-value and luxury brands were brutally affected by a wave of deep discounting in the last quarter of 2008.
The aura of a brand is easily destroyed.
One reaction has been to look for radically new marketing strategies, as in the case of the up-market Jimmy Choo brand of women’s shoes, now to be sold in a simpler guise by the low-price mass retailer Hampamp;M.
Such recovery as there has been in the luxury market has been largely limited to so-called affordable luxuries – shoes, handbags, or ties, rather than yachts or fast cars.
Purchases of these relatively low-cost items can be regarded as the withdrawal symptoms of the world’s consumer binge.
The consumer age was the product of two previous crises.
The Great Depression of the 1930’s was interpreted as the result of inadequate consumption, of poverty in the midst of plenty.
Governments took on the responsibility of stabilizing and organizing consumption on a wider level.
A second major global crisis, in the 1970’s, shook the heavy-industrial basis of manufacturing, and with it the idea that governments should manage the economy.
The 1970’s produced a new model of sustaining consumption through individualized desire.
Production was decentralized and focused on the creation of niche products for highly specialized markets.
Consumption became primarily an individual phenomenon through which people could distinguish themselves.
In retrospect, the 1970’s mark the end of an era dominated by mass production, and the beginning of the new consumer age.
Sometimes the age of productionism is also called “Fordism,” after Henry Ford, who supplied large quantities of cheap but identical cars.
Rather unimaginatively, the consumer age is dubbed post-Fordism.
In fact, it could be called “Tom Fordism,” after the young American designer who realized the iconic potential of the Italian fashion house of Gucci.
What will replace the age of consumerism?
The most obvious answer is that the shift to a service economy – already well under way by the beginning of the new millennium – will become more complete.
Consumerism depended on a radical notion of individualism.
We become indebted in order to consume because we are convinced that our utility schedule is more important than someone else’s.
If I see a beautiful piece of jewelry or a bright new car in a shop, I am convinced that it should be mine, and that it can be more usefully employed in my possession than in someone else’s.
In that way, greed feeds on a kind of pride or self-regard.
The empirical study of happiness has produced evidence that the satisfaction from buying objects is short-lived and depends on continued repetition.
That is socially, morally, and environmentally wasteful.
On the other hand, the consumption of experiences (rather than objects) produces a more sustained satisfaction.
The new service economy emphasizes human interaction more than individualistic consumption.
In an extreme form, luxury hotels are now organizing service in local community projects as a way of engaging the passions of their wealthy patrons.
This service economy may generate higher levels of overall well-being if it emphasizes that humans do not exist as separate islands, but existentially depend on their relations with others.
The Crisis of Thai Democracy
One year after he was re-elected in a landslide, Thailand’s Prime Minister Thaksin Shinawatra has been forced to dissolve the National Assembly and call a snap election.
Although his Thai Rak Thai (TRT) party commands a 75% majority in the assembly, Thaksin is embattled.
He remains immensely popular with rural voters and the urban poor, who comprise more than 60% of Thailand’s electorate, but he has been battling a fervent Bangkok-based insurrection against his rule by the intelligentsia and middle classes.
They accuse Thaksin, Thailand’s wealthiest businessman, of corruption and treason for the tax-free sale of his family-owned Shin Corporation to the Singapore government’s Temasek Holdings for $1.9 billion.
Thaksin’s rapid reversal of political fortune attests to the limits of the ballot box, as well as to democratic shortcomings that now beset a host of developing countries, including regional neighbors such as the Philippines.
Until recently, Thaksin appeared to be as unassailable at home as he was bold and credible abroad.
Exploiting Thailand’s deep urban-rural divide, Thaksin bulldozed his way to power in 2001 on a populist platform.
He stirred up national pride and promised rural Thais that their country would rise to greatness following the devastating 1997 economic crisis.
A raft of populist policies underpinned his first four-year government, from rural debt suspension and cheap universal healthcare to handing out $25,000 to each of 77,000 villages for entrepreneurial start-up funds.
Reminiscent of development strategies prevalent in East Asia, Thaksin picked strategic niche industries to propel Thailand’s economic expansion, focusing on automobiles, fashion, food, healthcare, and tourism.
In foreign affairs, Thaksin carved out his own space on the international stage with ambitious regional cooperation schemes anchored around the 25-member Asia Cooperation Dialogue (ACD), the Ayeyawady-Chao Phraya-Mekong Economic Cooperation (ACMECS) and a clutch of bilateral free-trade agreements with the major powers, including the US, China, Japan, India, and Australia.
The ACD was designed to make Thailand the political center of gravity in Southeast Asia; ACMECS was to make it the region’s linchpin of economic development; and the bilateral agreements would cement relations with the biggest players in the region.
At his zenith, many saw Thaksin as a worthy successor to former Malaysian Prime Minister Mahathir Mohamad, buoyed by authoritarian rule at home and assertive leadership abroad.
Voters overwhelmingly returned Thaksin to office in February 2005.
But his personal popularity then plummeted, owing to the spate of separatist violence in the Muslim-dominated south and the scourge of corruption.
Sins overlooked during his first term – from harassment of the media and coercion of civil society groups to extrajudicial killings in an anti-drug campaign and conflicts of interest that benefited his Shinawatra telecommunications empire – soon caught up with him.
The shady sale of Shin Corporation in February galvanized long-simmering discontents.
The deal was viewed as the epitome of Thaksin’s sophisticated corruption and a betrayal of his proclaimed nationalism, thus exhausting his moral authority and political legitimacy.
The company’s value quadrupled during Thaksin’s reign, with assets such as satellites, a mobile phone service, and an airline having originated from state concessions that were conditioned on majority Thai ownership.
For Thaksin’s opponents, the sale of these assets to a foreign company owned by a foreign government amounted to putting Thailand’s economic sovereignty on the block.
Thaksin’s days appear numbered, for Thailand seems poised to eject a popularly elected prime minister.
The number of street protesters has since swelled from five digits to six.
His predicament illustrates the common dictum in the politics of developing countries where rural electorates elect governments but urban elites get to throw them out.
Indeed, the anti-Thaksin coalition will settle for no less than his ouster from office, permanent banishment from Thai politics, and possibly exile.
But the opposition has decided to boycott the snap election, which the TRT would likely win again by a large margin, because Thaksin has captured and manipulated the institutions established by the constitution to safeguard against graft and uphold separation of powers within the state.
Thai politics has thus reached an impasse.
Only intervention by King Bhumibol Adulyadej, who is widely revered, seems capable of saving the day.
In a fiercely contested battle between Thaksin and his opponents, King Bhumibol is the fundamental difference between Thailand and the Philippines, where “people power” revolts regularly undermine and sometimes overthrow presidents.
The King’s intervention would put an unconditional stop to the street confrontations.
But the Thai people cannot afford to look to their aging and ailing King every time they have a problem.
Moreover, a royal intervention would risk returning Thailand to square one, seeking to rewrite its constitution to remedy the shortfalls of its democratic culture.
What Thai democracy needs in order to mature is not a political safety net, but a vigilant citizenry to ensure disciplined enforcement of the constitution’s provisions and institutions, so that they can no longer be hijacked by the likes of a Thaksin.
The Cruelty of Chance
MOSCOW – In Russia, somewhere behind every event lurks the question: Who is to blame?
What if no fog prevented the safe landing at Smolensk airport?
What if the plane was not a 20-year-old, Russian-made Tupolev-154, but a newer and safer model?
What if the Polish pilot had obeyed the Russian air traffic controller who tried to divert the plane to Moscow or Minsk?
Unfortunately, the cruelty of chance also lies at the heart of the centuries of mistrust between Poland and Russia.
The irony (if there is an irony at all) is that this tragedy came at a time when mistrust seemed, at long last, to be giving way to better, more businesslike relations and greater understanding between the two countries.
After 70 years of denial, Russia’s leadership (if not yet ordinary Russians) were ready to admit that Joseph Stalin’s NKVD (precursor to the KGB) slaughtered more than 20,000 Polish officers, intellectuals, and clergy in the nearby Katyn forest in 1940.
Indeed, Russian Prime Minister Vladimir Putin, a former KGB officer himself, invited his Polish counterpart, Donald Tusk, to commemorate that tragedy together.
But Kaczynski, a member of Solidarity in the 1980’s who was eager to overthrow the communist regime, was more mistrustful of the Russians than Tusk.
He put together his own delegation to visit Katyn, and wondered aloud if the Russians would give him a visa.
Certainly, no Russians were invited.
When the pilot of the presidential plane (ironically, again, Soviet made) was advised not to land in the thick fog, either he, or perhaps even the president himself, may have mistrusted the Russians’ willingness to give honest advice.
Indeed, they may well have wondered if the cunning ex-KGB men around Putin simply wanted to make Kaczynski’s Katyn commemoration a mockery?
Russian-Polish suspicions and disagreements date back to the sixteenth century, when Poland was the far greater power; indeed, the Grand Duchy of Moscow was a backwater.
Across the centuries, there have been wars, started by both sides, and partitions of Poland executed by the Russians, followed by attempts at “Russification,” with the Russian Christian Orthodox Empire trying to control the “silver-tongued,” “deceptive,” West European-oriented Catholic Poland.
Then there was the 1917 Bolshevik Revolution, which the Poles refused to join, and Marshal Jozef Pilsudski’s miraculous victory over the Red Army at the gates of Warsaw in 1920.
Throughout the interwar years, Poland and the nascent Soviet regime were at daggers drawn almost without interruption.
When Stalin signed the Molotov-Ribbentrop  pact with Nazi Germany in 1939, it gave him an opportunity to invade Poland.
The Katyn massacre was a direct result, with Stalin ordering the mass murder of Poland’s elite in order to decapitate Polish society and thus make it more pliable.
Katyn was also the occasion for the Soviets to break off relations with Poland’s wartime government-in-exile in London.
Because the Polish leaders refused to exonerate the Russians, Stalin accused the Poles of collaborating with the Germans in trying to shift the blame for Nazi crimes onto Russians.
Soon after, the idea of establishing a puppet regime in Warsaw took root.
Although the Nazi-Soviet Pact did not last long – Germany invaded Russia in 1941 – for Poland there was no way out.
With Hitler’s defeat, it once again became part of the Russian sphere, this time of Soviet Russia.
But Poland never stopped striding – and striking – for independence.
The rise of Solidarity in the 1980’s was the earliest and most severe blow to the stagnating Soviet system.
The Polish-born Pope John Paul II crystallized the anti-communist “threat” that Poland now posed to the Soviet Union.
The Pope’s call for religious freedom around the world, including in the socialist countries, rubbed the atheistic Soviets – and Orthodox Russians – the wrong way.
Indeed, throughout the twentieth century, animosity between Poland and Russia remained at fever pitch, manifested not only in politics but also culturally.
This, of course, continued an old pattern, too.
Alexander Pushkin, Nicolai Gogol, and Fyodor Dostoyevsky were all suspicious of the Poles, calling them “cold,” “distant,” and “manipulative,” and seeing Poland as always on the side of the West, rather than standing with its Slavic brothers.  Indeed, Pushkin’s friendship with Adam Mickiewicz ended in acrimony over the 1830 Polish insurrection against Tsarist rule.
In fact, the animosity ran so deep that when both countries were no longer communist, and Russia was looking to replace its November 7 Bolshevik Revolution holiday, it came up with November 4, the anniversary of the Russian boyars’ victory in 1612 over Polish King Sigismund’s short-lived occupation of Moscow.
Now there is talk, in both Warsaw and Moscow, that the second tragedy of Katyn might usher in a new era in bilateral relations.
Perhaps so, but as the Polish essayist Stanislaw Jerzy Lec said: “You can close your eyes to reality, but not to memories.”
The Cultural Bias of Genetics
All cultures impose on their members ideas about who they are, how they came to be, and where they fit in socially.
For example, pre-modern Europe believed that a woman who had sex before marriage might carry the imprint of her lover within her, so that her child born in wedlock would resemble the earlier lover, rather than the husband.
This served to justify the premium placed on female chastity.
Folk ideas about heredity are a particularly powerful cultural tool, but they are not unique to pre-modern societies.
Even contemporary science has its own cultural ideologies about heredity, which are often difficult to disentangle from the complex data and high technology that we believe produces objective, value-free glimpses of nature.
In the field of human origins, it is well-known that a human DNA sequence is nearly 99% identical to the corresponding part of a chimpanzee’s DNA.
From there, it is not uncommon to hear the conclusion that we are ”nothing but” chimpanzees (and fated to be aggressive, or to possess whatever attributes are being imputed to apes these days), or that apes merit human rights.
Once again, social meanings are enmeshed within beliefs about heredity.
Actually, that overwhelming similarity between human and chimpanzee DNA is a product of two cultural facts: our familiarity with the chimpanzee’s body, and our unfamiliarity with DNA.
After all, when the chimpanzee itself was new and interesting (in the 1700’s), contemporary scholars commonly judged it to be overwhelmingly similar to the human form – so much so, that it was commonly classified as a variant of humans.
After several centuries studying apes, we are familiar with every subtle difference in form between our species and theirs.
But the emergence of molecular comparisons in the 1960’s, and DNA technologies in the 1980’s, presented a new way of comparing species.
The comparison of linear polymers – proteins made up of chains of amino acids, or DNA made up of chains of nucleotides – held out the promise of a simple tabular approach to evolution.
When we compare the genetic material of humans and chimpanzees, we do, indeed, find that it corresponds at nearly 99 of every 100 comparable sites.
But this method omits much of what has been learned about genomic evolution in the last quarter-century.
Mutational processes are far more complex than was thought a few decades ago; with strand slippage, duplication, transposition, and illegitimate recombination producing qualitative differences between closely-related genomes.
So, while measuring nucleotide similarity in homologous regions is indeed in the 98-99% range, this number does not incorporate or acknowledge the measured difference in the size of the human and chimp genomes, or the widespread differences in chromosome or genome structure.
Because a DNA sequence is a one-dimensional entity, it lends itself much better to single-number comparisons than a four-dimensional body does.
The genes that lead to the production of the human and chimpanzee femur may be 98-99% identical, but it is impossible to say whether the bones themselves are more or less similar than that.
Perhaps the most seductive aspect of the DNA comparison is its lack of biological context.
The value of 98-99% similarity between human and ape DNA stands outside the rest of life.
We can gain some zoological perspective on what that number really means by comparing another species to apes and humans.
Compared to the body of an oyster, for example, chimpanzees and humans are at least 99% identical – bone for bone, muscle for muscle, nerve for nerve, organ for organ.
If anything, the DNA comparison underestimates the profound similarity of the human and ape bodies in the great tapestry of nature.
Context reveals something at the low end of the scale as well.
Two DNA sequences generated at random must be 25% identical, by virtue of the fact that DNA is a sequence of only four bases.
Therefore, all multicellular life, having presumably developed from a single common ancestral form, must be over 25% identical in their DNA sequences.
In other words, a human and a carrot have much in common genetically, despite their being little similarity between them physically.
Here the DNA comparison grossly overestimates the actual relationships between species.
Genetic comparisons simply do not afford an ”inner view” of the relationships between species, or an encapsulation of those relationships.
In other words, this apparent fact of nature – the overwhelming genetic similarity of human and ape – is a fact constructed from culture.
That is not to say it is inaccurate or false; just that its meaning is far less obvious than it may appear superficially. &nbsp;
We make sense of the world, and of our place in it, culturally – and science provides more information for constructing that place.
But like any other peoples, our own application of that information to the puzzle of our existence is strongly influenced by our non-scientific ideas, our pervasive folk ideologies of heredity.
The Cultural Contradictions of Multiculturalism
STOCKHOLM – State-sponsored multiculturalism has failed.
That proclamation by British Prime Minister David Cameron, following hard on the heels of similar renunciations of multiculturalism by German Chancellor Angela Merkel and French President Nicolas Sarkozy, suggests that a page is being turned in European society.
But is it?
Cameron’s attack on multiculturalism minced no words.
“Frankly,” he said, “we need a lot less of the passive tolerance of recent years and a much more active, muscular liberalism.”
He was not criticizing ethnic and cultural pluralism, but the idea of “state multiculturalism,” which applies different moral standards to various social groups.
In the future, Cameron declared, Muslim groups that do not, for example, endorse women’s rights, defend freedom of expression, or promote integration would lose all government funding.
It is not just official multiculturalism that has failed in Europe, however; so has the multiculturalism endorsed by large parts of European civil society.
Sweden, one of the most liberal countries in the world, but also one that has recently seen a surge in extremism, is a case in point.
Sweden has long been known for its lifestyle liberalism.
Swedes are overwhelmingly secular and indifferent toward the Swedish church.
Homosexuals have been able to register civil partnerships since 1995 and marry since 2009, and the country is one of the most radical in its understanding of women’s rights – as WikiLeaks founder Julian Assange can attest.
Moreover, Sweden’s far-reaching freedom of expression is one reason why Assange located WikiLeaks’ servers in the country.
But Sweden’s freedom of expression was also one of the motives behind a grisly suicide attack in Stockholm in December of last year.
According to a last testament left behind by the attacker, a Swedish citizen named Taimour Abdulwahab, Christmas shoppers in downtown Stockholm had to die in retaliation for “the Swedes’ support” for Lars Vilks, an artist who stirred outrage in the country with drawings of the Prophet Muhammad as a dog.
Vilks argued that his work was a provocation aimed at revealing the selective liberalism within the Swedish intellectual establishment – its multiculturalism, one could say.
The Stockholm suicide bombing was not the first act of violence linked to Vilks.
Two young men were recently sentenced to prison for trying to set fire to the artist’s home.
During a lecture at Uppsala University last summer, a mob attacked Vilks, a professor of art history, while crying Allahu akbar.
The then 64-year-old artist was head-butted, but escaped serious injury thanks to heavy police protection.
What is remarkable is not just the violence and threats against Vilks – anyone who doubts the determination of Islamist extremists in Sweden should watch the YouTube clip from that lecture – but also the reaction from the otherwise radically secular Swedish establishment.
A number of influential Swedish intellectuals and politicians have directed their harshest criticism against Vilks, not against those who have called for censorship and even incited violence.
Only a few of the country’s newspapers and political magazines published Vilks’ drawings.
Like murdered Dutch filmmaker Theo van Gogh and the British novelist Salman Rushdie before him, Vilks was criticized by liberals and the left for causing unrest with his art.
In this respect, Vilks’ work must be regarded as having succeeded in exposing moral double standards – no matter what one thinks of the drawing itself.
In Sweden, just as in similarly liberal Holland and Denmark, right-wing populists have profited from liberals’ failure to stand up for their values.
The Sweden Democrats (SD), a party with roots in the country’s white-supremacist movement, entered the parliament for the first time in September 2010, with the support of 5.7% of the Swedish electorate.
The SD has sought to position itself as the sole defender of gays and Jews in the face of intolerance stoked by large-scale Muslim immigration in the past two decades.
Swedes who stand far from the SD’s original platform are apparently willing to be represented by a party that until recently was full of neo-Nazis.
Thus, the lack of “muscular liberalism” in one of the world’s most liberal countries has paved the way for both Islamists and right-wing populists.
Europe’s leading politicians have spoken out, and now it is time for European civil society – its newspapers, critics, curators, academics, and publishers – to declare the failure of multiculturalism and show some courage in defending the values they claim to embody.
The Cultural Revolution at 40
Mao’s Cultural Revolution was launched 40 years ago this month, yet, despite 20 years of economic liberalization, its wounds remain a taboo subject.
Today’s rulers dare not face up to their own experiences or moral responsibility.
So, three decades after the Cultural Revolution ended, the national self-examination that China requires has not yet begun.
Of course, the Communist Party has deemed the Cultural Revolution a “catastrophe,” a judgment supported by mainstream opinion.
But China’s rulers permit discussion of the Cultural Revolution only within this official framework, suppressing any and all unofficial reflections.
The generalized official verdict, and the use of Lin Piao (once Mao Zedong’s Vice President and designated heir, who rebelled against him) and the “Gang of Four” as scapegoats, obscures the crimes of Mao and the Party, as well as the entrenched flaws in the system.
The Cultural Revolution’s major figures, who wrought so much mindless violence, thus either maintain their silence or offer spurious self-defenses.
Most victims also use various excuses to bottle up their memories.
Those who both persecuted and were persecuted are willing to talk only about their being victims.
For example, the fanatical Red Guard movement swallowed up almost every youth of the right age.
Yet all but a few old Red Guards remain silent, saying, “it is not worth remembering.”
During the Cultural Revolution’s early days, the Beijing-based Allied Movement, formed by the children of party cadres, committed horrendous acts of violence, operating under the slogan, “If the father is a hero, the son is a good man; if the father is a reactionary, the son is a turtle egg.”
But the memoirs of these rebellious vanguards of yesteryear highlight only their youthful passion and pure idealism, or their sufferings and those of their parents.
They do not mention their own barbaric assaults, vandalism, and looting, or their kangaroo courts.
The revolution’s veterans refuse to discuss their arrogant presumption of “natural Redness,” or to mention that they rebelled because they wanted power.
Worse still, they express no remorse toward their victims.
The Cultural Revolution swept up all of China.
So many people suffered that it is difficult to count the number of victims accurately.
This is all the more true of the persecutors.
Yet few reflect and apologize.
The terror of the Red Guards, the armed fights between the rebellious sects, the teams established to “cleanse” the social classes, and all the bloody massacres are simply left to rot in China’s memory.
The official ban blocks reflection, but human weakness and careerist self-interest among those who participated buttresses the official ban.
Consider Ye Xiangzhen, the daughter of senior general Ye Jianying, who once discussed her family’s Cultural Revolution experiences on television.
During the early stages of the Cultural Revolution, she played a dual role: daughter of a Chinese field marshal and leader of the rebels at the School of Art in the capital.
She complained that she was “too famous,” “too active,” and “too stressed” at the time, and she provided extensive details about how Mao’s wife, Jiang Qing, persecuted the Ye family and how the Ye children went to prison.
But she had only 58 words to say about her career as a Red Guard leader – no details or explanation of how she joined, which activities she participated in, and whether she was involved in “physical struggles” or persecuted others.
To call for those people who applied violence and persecuted others to examine themselves and repent is not intended to mete out legal responsibility and moral judgment.
But it would at least restore the truth about the Cultural Revolution, summarizing its lessons in order to avoid repetition.
More positively, restoring truth would counter the traditional Chinese instinct to blame all disasters on external forces, and might lead to a spiritual epiphany for a people struggling to find value in the emerging new China.
The person with the most responsibility for the catastrophe of the Cultural Revolution is, of course, Mao, yet he remains China’s savior.
The children of Mao’s senior cadres who enjoyed the greatest fame during the Cultural Revolution are now the principal beneficiaries of today’s economic reforms.
But this continuing silence by the guilty only transfers the costs to society as a whole, with Chinese life distorted by the weight of lies and evasions.
As one generation after another continues to live in denial, the lies will corrode everything they touch.
The Chinese people will no longer know what is personal honesty or historical truth, and they will repeatedly abuse, miss, or forsake historic opportunities.
As long as the Cultural Revolution remains unaccounted for, it will not have ended.
If historical truth is not restored, the lessons cannot be learned.
No amount of material prosperity can make China a healthy society without this necessary reckoning with the past.
The Curse of Unilateralism
The realization in the United States that the war in Iraq has been lost is perhaps the most momentous fact of international politics in 2006.
The time of American unilateralism is objectively over.
Whether US foreign policy will come to reflect this fact, only the future will tell.
Unfortunately, this also means that a unique opportunity has been lost.
For only the US – with all its power and sense of mission – had the ability to establish a new world order at the beginning of the twenty-first century.
To achieve this, the US would have had to subordinate its power to the goal of shaping the new order, much as it did at the close of World War II in 1945.
Instead, America succumbed to the temptation of unilateralism.
National greatness for a world power always arises from its ability to shape the world.
If a great power forgets this, or loses the ability to act accordingly, it begins to decline.
It is almost tempting to think that America’s great Cold War opponent, the Soviet Union, with its sudden disappearance 15 years ago this month, left its own Trojan horse for America – the poisoned gift of unilateralism.
Without a fundamental turnaround in American political consciousness, the unilateralist amnesia of US foreign policy will have far-reaching consequences and leave a huge vacuum in the global system.
No other nation – not China, Europe, India, or Russia – has the power and the sense of mission to take on America’s role.
Only America was (and potentially still is) able to fuse realism and idealism, self-interest and ethics, in its foreign policy.
Only the US pursued a foreign policy that conceived freedom and democracy as its mission.
This was not always and everywhere the case – certainly not in Latin America.
But where it did apply, America’s strength and willingness to seek international cooperation created an order whose institutions still hold the world together.
The UN, NATO, the IMF and the World Bank, the law of nations and international criminal law, even today’s free and united Europe – all are crowning achievements of US foreign policy.
They mark the moments in history when America’s power was used to further a global order, while also pursuing America’s own interests in the most effective and sustainable manner.
America’s departure from this great tradition did not begin with the terrorist attacks of September 11, 2001.
Indeed, as early as the final years of the Cold War, the US began to view the whole system of international treaties and institutions as an obstacle to enforcing its own interests.
America’s foreign policy elite increasingly came to perceive the US as a Gulliver tied down and oppressed by political midgets, with their laws of nations, treaties, and multilateral institutions.
The existing world order – created by the US itself – was first devalued in American eyes, then weakened, and finally consciously attacked.
Thus, the current debate in the US about the consequences of defeat in Iraq still falls short – because, despite all the critiques of American policy, that debate is still premised on the unilateral use of American power.
This applies to the views of the Democratic opposition as well as to the Baker-Hamilton report.
What is needed is a conscious, deliberate return of US foreign policy to multilateralism.
This mental turnaround is, indeed, essential if things are to improve, because the situation in Iraq represents, above all, a defeat of America’s unilateralist orientation.
The Middle East, North Korea, Darfur, Central and East Africa, the Caucasus – in none of these places can the US still act successfully on its own.
And yet, without America and its power, the prospects in all these places are still bleaker: more dangers and more chaos.
The situation is the same with respect to global growth, energy, resource management, climate change, environment, nuclear proliferation, arms control, and terrorism.
None of these problems can be resolved or even contained unilaterally.
Yet no attempted solution will get very far without the US and its decisive leadership.
This also applies to the future of the law of nations, the newly created international criminal law, and the United Nations.
Unless these rules and institutions are further developed, globalization, too, will take an ever more chaotic shape.
Madeleine Albright once called the US the “indispensable nation.”
She was right then, and she is still right today.
Only one power can rob the US of its unique position: America itself.
The question today is whether the current crisis of American consciousness marks the beginning of America’s decline or, let us hope, the beginning of a return to multilateralism.
Will America hark back to the spirit of 1945 or, despite being sobered and disappointed, decide to stick to its lone and lonely path?
No other power will be able to assume America’s role in the world for the foreseeable future.
The alternative to American leadership is a vacuum and increasing chaos.
But within one or two decades maybe China will define the rules, if the US continues to reject its multilateral responsibilities.
For all these reasons not only America’s friends have a vital interest in a US return to multilateralism.
Given the dangers of unilateralism for the current world order, so do America’s enemies.
The Cycles of Economic Discontent
FLORENCE – The nineteenth century was mesmerized by the cyclical behavior of business.
The French economist Clement Juglar became famous for establishing that business cycles ran for around nine or ten years.
We have recently had our own cycles of exuberance and disintegration.
But they are very different.
In the nineteenth-century world, people rapidly picked themselves up after downturns and went back to business as usual.
In that sense, the phenomenon of the business cycle looked relatively permanent and unchanging.
Nowadays, however, a cyclical collapse comes as a great surprise.
In its aftermath, we start to reinvent our view of economics.
Every ten years or so, we think that a particular model of growth is so broken that it cannot be resurrected.
The world needed to be rethought in 1979, 1989, 1998, and 2008.
Keynesianism definitively ended in 1979, following the second oil-price shock of the decade.
The coincidental combination of the election of Margaret Thatcher in the United Kingdom and Federal Reserve Chairman Paul Volcker’s interest-rate shock of October 1979 ended an era in which inflation had been seen as a solution to social problems.
State action and monetary expansion as a means of buying off discontent were discredited, as was the West European welfare state.
The association of Europe – and of European social democracy – with Keynesian demand stimulus was more than a little unfair, in that the greatest proponent of the Keynesian view was the Republican American President Richard Nixon.
But the political shift of 1979-1980, culminating in the election of Ronald Reagan, brought about a new opposition of the free market and innovation to social-democratic corporatism and centrism.
Ten years later, in 1989, the Soviet model of economic planning and modernization through centrally directed growth was discredited.
In its last phases, it had taken on large quantities of foreign debt, and over-indebtedness finally sunk a model that had fundamentally failed long before that.
The next beautiful idea that failed, in 1997-1998, was the concept of a particular “Asian miracle” (as it had been dubbed in the title of an influential World Bank publication).
Asian economies were supposedly better coordinated because of strategic interventions by the government along the lines of the initial postwar practices of Japan’s MITI.
But, like the Soviet Union and its satellites, the smaller and dynamic Asian economies had taken on too much debt.
The response to the economic crisis in Thailand and Korea in the late 1990’s was emphatic preaching about the inherent superiority of the so-called Anglo-Saxon economic model.
But this vision, in turn, also became problematic, and it was unambiguously discredited in 2007-2008, amid a massive outbreak of European and Asian Schadenfreude .
Then it became clear that the rest of the world was badly affected by the fallout from the financial crisis, and a more sinister interpretation became popular.
Many people in many countries interpreted a crisis that unambiguously began in the United States, but affected some other countries more harshly, as evidence of a fundamentally malign US plan.
The Chinese search for a replacement of the US dollar by a synthetic reserve currency is driven by a political backlash against the perceived iniquities of US financial and economic preeminence.
The cycle in which political models are torn up appears to be accelerating.
The emerging-market boom already looks as if it is the next vision to be cast in the garbage can of history.
The ratings agency Moody’s is preparing warnings about the extent and quality of Indian private-sector debt.
Chinese investors are worried about inflationary overheating.
The phase of revulsion and rejection is never complete, but the bold visions never recover their original splendor.
The European social-democratic model survived the 1970’s in a bedraggled form.
The idea of strong Asian economic growth as a permanent feature of the world economy returned with a vengeance only a few years after the Asia crisis.
If the major English-speaking economies remain open and do not close themselves off to trade and immigration, they will also see a return to growth.
But each wave of collapse breeds a greater degree of disillusion about particular institutions, which are blamed for the outcome.
It may be the welfare state in the 1970’s, the Communist Party apparatus in the 1980’s, the Asian industry and trade ministries in the 1990’s, or the nexus of the US Treasury and Wall Street in the 2000’s.
As each institution is eroded, there is less and less left in the way of alternatives. That is also true of currencies.
The dollar has been knocked off its pedestal by the crisis, but any conceivable substitute is obviously even more flawed and more problematical.
The euro is the composite currency of an area with a poor growth record and an inadequate response to the economic crisis.
The renminbi is still non-convertible.
So there is no master currency at all any more.
The dissident Chinese artist Ai Weiwei sums up the new mood of universal and unmitigated cynicism.
To make the point that all institutions are equally suspect, he created an exhibition entitled “Fuck off,” in which he shows photographs of himself with an obscene hand gesture in front of famous monuments: the Doge’s palace in Venice, once the commercial capital of the world, the Eiffel Tower, the White House, and the Forbidden City in Beijing.
His latest exhibition’s title mocks the recent (and almost universal) tendency of governments to offer meaningless apologies for past mistakes: it is called “So sorry.”
The Dirge of Cyprus
PRINCETON – Europe can choose its own musical accompaniment to its latest crisis.
In Berlin, 50 Cent’s “All Things Fall Apart” has just had its premiere, so that soundtrack might be appropriate.
Or the continent can reach back to Giuseppe Verdi, born two hundred years ago, whose penultimate, and probably greatest, operatic achievement starts on the coast of Cyprus with a storm of fantastic violence and the opening words of its hero, Otello: Esultate, rejoice!
The war has been won; but Otello’s achievement is later destroyed by his jealousy.
Today, Cyprus appears to have been rescued.
But the rescue has fueled a growing rift that jeopardizes the future of European integration, partly owing to the way that the upheaval of the early twentieth century – especially the Great Depression – has been reenacted in the debates about the post-2008 financial meltdown and the subsequent euro crisis.
The interwar economic slump became intractable because it was also a crisis of social stability, democracy, and the international political order.
Widespread bankruptcy and unemployment increased social tension, ultimately making normal democratic politics impossible.
In Germany, the epicenter of democracy’s collapse, radicals on both the right and the left raged against the postwar peace settlement and the Versailles Treaty.
In the last years of the increasingly unstable Weimar Republic, as democracy was fraying, German governments started to use their opponents’ radicalism in an effort to extract security concessions from the Western powers.
Domestic political pressure became a source of heightened international tension.
That is true in today’s Europe as well.
Democracy has become a central target of complaints by the European elite.
Luxembourg’s prime minister, Jean-Claude Juncker, a former chairman of the Euro group, has lamented that European leaders know what the right policies are, but do not know how to be reelected after implementing them.
Similarly, after his recent crushing election defeat, Italian Prime Minister Mario Monti wistfully explained that Italy’s voters were too impatient to bear reforms whose benefits would only become evident beyond the electoral cycle.
Events in Cyprus have exposed two other dimensions to the clashes over Europe’s dual sovereign-debt and banking crisis.
First, the discussion of a levy on bank deposits, and whether small customers should be exempted, put class conflict front and center.
Second, the question of foreign, and especially Russian, depositors – along with proximity to Syria – has turned the rescue of the Cypriot banking sector into an international relations problem.
The initial proposal to impose a one-time tax on accounts holding less than €100,000 came not from the European Union or from Germany, but from the Cypriot government, which must have known that it was likely to generate outrage, and that the Cypriot parliament would never vote for it.
Perhaps the government believed that mass protests – with placards denouncing the EU as a fig leaf for revived German domination of Europe – would strengthen its hand.
After all, even moderate Cypriots were outraged by the bullying of their small island by Germany and by Europe.
The other side in the negotiations also played class politics.
At one of the tensest moments, as Cyprus was seeking an alternative rescue package from Russia, the German Bundesbank announced the results of a new European Central Bank study indicating that average German wealth was lower than in the southern European states, largely because fewer Germans own their own houses.
The message seemed clearly intended to influence the negotiations: Why should poorer Germans be expected to sacrifice to support Mediterranean millionaires?
In the aftermath of the financial crisis, income and wealth distribution have moved to the center of political debate.
Even the Catholic Church seems to reflect the new mood: The election of Jorge Mario Bergoglio as Pope Francis is a clear reference to St. Francis of Assisi and the Church’s mission to stand up for the poor.
On the international-relations front, after 2010, as deposits from Europe left Cypriot banks, deposits from Russian businesses and individuals increased – and Russia has many reasons to use money as a way of buying political control.
Cyprus is a crucial staging post for American security operations in the eastern Mediterranean, and the gas fields off the Cypriot coast might be developed as an energy source that would – at least after 2017 – reduce European dependence on Russian supplies.
In an earlier phase of the crisis, Russia gave Cyprus a $3 billion credit.
Now, however, a new credit would serve only to make the burden of government debt unsustainable; what is needed is a purchase of all or some of the problematic Cypriot banks.
In the aftermath of a crisis that has been intensified by the rhetoric of class conflict, Russia might be able to extend its control more significantly, and at a lower price.
Deepening social polarization, its use in financial negotiations, and the intrusion of a new security element provide further evidence of what most economists and commentators on Europe have long argued: a monetary union is impossible to sustain in the absence of a political union.
A state, especially in the modern form of the European welfare state, depends on effective mechanisms for arbitrating and resolving social disputes – mechanisms that, as the turmoil surrounding Cyprus has shown, the EU lacks.
As long as that remains true, European integration may be doomed by the time the music stops.
The Danger of Trans Fats
Trans fats are unsaturated fatty acids with at least one double bond in the trans configuration.
While small amounts of trans fats are naturally present in meats and dairy products from cows, sheep, and other ruminants, the great majority of trans fats in our diet are industrially-produced, contained in foods made with partially hydrogenated vegetable oils.
Partial hydrogenation, which converts vegetable oils into semi-solid fats for use in margarines, commercial cooking, and manufacturing processes, converts approximately 30% of the natural fats to trans fats.
In the US, consumption of trans fats averages between 2-4% of total energy, with major sources being deep fried fast foods, bakery products (cakes, cookies, muffins, pies, etc.), packaged snack foods, margarines, and breads.
Considerable evidence exists for harmful effects of trans fat intake.
Furthermore, trans fats from partially hydrogenated oils have no intrinsic health value.
Thus, little justification can be made for the use of partially hydrogenated oils, compared with other natural oils or fats.
Importantly, adverse effects are seen at very low intakes: for example, 1-3% of total energy, or approximately 2-7 grams (20-60 calories) for a person consuming 2000 calories/day.
Thus, complete or near-complete avoidance of industrial trans fats (≤0.5% of energy) may be necessary to avoid adverse effects and minimize health risks.
In 2004, Denmark became the first country to legislate limits on trans fat content of foods, largely eliminating industrial trans fats from all foods (including restaurants) in that country.
4. Use of trans fats in food manufacturing and restaurants can be limited without any significant effects on food taste, cost, or availability. While industry often expresses concern that limiting the use of trans fats will affect food taste or cost, no evidence exists to support this hypothesis.
Conversely, substantial evidence exists that use of trans fats can be eliminated almost entirely without any effects on food taste, cost, or availability.
This has been clearly demonstrated in both Denmark and the Netherlands.
It should also be emphasized that much less is known about trans fat consumption in developing countries.
The current evidence suggests that, compared with Western nations, the intake of trans fats from partially hydrogenated oils may be much higher in developing countries, typically because partially hydrogenated oils represent the cheapest (and often subsidized) choice of fat for cooking.
Given that coronary heart disease is the leading cause of death in nearly all countries, including developing nations, intensive efforts must be undertaken to greatly reduce or eliminate use of partially hydrogenated oils in both the developed and the developing world.
In summary, considerable evidence exists for harm, and little evidence for intrinsic health value, of consumption of trans fats from partially hydrogenated vegetable oils.
Elimination of industrial trans fats from foods, either by voluntary or legislative measures, would likely prevent tens of thousands of heart attacks each year in the U.S. and other countries.
The Dangers of Nuclear Disarmament
MOSCOW – Russia and the United States have signed a new strategic nuclear-arms reduction treaty (START).
Officially, the treaty cuts their weapons by one-third; in fact, each party will decommission only several dozen.
Nevertheless, the treaty is a considerable achievement. It normalizes political relations between the two countries, thereby facilitating their further cooperation and rapprochement.
The return of strategic nuclear weapons to the center of world politics increases Russia’s political weight and highlights the field in which Russia can still assert itself as a superpower.
It also gives a political boost to Barack Obama, cast as the most constructive and progressive US president for decades, and possibly for many years to come.
After the treaty was signed, the US hosted a nuclear non-proliferation summit, a landmark event for the Obama administration, which has made the fight against nuclear proliferation a trademark policy.
The few accords reached at the summit, although welcome, are not as significant as the impression that the summit created that world leaders are ready to work together to confront nuclear proliferation.
But debates about the role of nuclear weapons in the modern world, as well as in the future, are only beginning.
The world system on which past discussions of nuclear weapons were based has become almost unrecognizable, calling into question the adequacy of the mentality and concepts inherited from that system.
The heart of the matter is this: it is obvious that nuclear weapons are immoral.
An A-bomb is millions of times more immoral than a spear or sword, hundreds of thousands of times more immoral than a rifle, thousands of times more immoral than a machine gun, and hundreds of times more immoral than salvo systems or cluster bombs.
But nuclear arms also have a significant moral distinction: unlike other weapons, they are an effective means of preventing the large-scale wars and mass destruction of people, property, and cultures that have plagued humanity throughout recorded history.
To reject nuclear weapons and strive for their elimination is, no doubt, a moral aim, at least in the abstract. But it is feasible only if humanity changes.
Apparently, the advocates of eliminating nuclear weapons believe that such change is possible. I do not.
Indeed, the risks of a world without nuclear weapons – or only a minimal number of them – are tremendous.
Nuclear deterrence – a threat to kill hundreds of thousands or millions of people – is a concept that does not fit into traditional morals.
Yet it has worked, preventing catastrophic wars while making people more civilized and cautious.
When one pole of nuclear deterrence weakened, due to Russia’s political decline in the 1990’s, NATO, a defensive union of democratic and peaceful states, committed aggression against Yugoslavia. Now that Russia has restored its capability, such a move would be unthinkable.
After Yugoslavia, there was an unprovoked attack on Iraq.
In a nearly perfect world, Russia and the US would not need large nuclear stockpiles.
But cutting nuclear weapons to a bare minimum in the current conditions would give a big advantage to small nuclear powers, which will see their nuclear potential gain near-parity with larger states.
Moreover, reducing nuclear weapons to a minimum might theoretically enhance the usefulness of missile-defense systems and their destabilizing role.
And even non-strategic missile-defense systems, the deployment of which might be useful, will be questioned.
If stockpiles of tactical nuclear weapons are reduced, as some US, European, and Russian experts have proposed, the opponents of Russia’s ongoing military reform will have even more reason to object to reconfiguring the country’s conventional armed forces away from confrontation with NATO toward a flexible-response capability vis-à-vis other threats.
Similarly, if the US withdraws its largely nominal tactical nuclear weapons from Europe, US-Europe strategic ties would weaken.
Many Europeans, above all in the new NATO member-states, would then demand more protection from the mythical Russian Leviathan.
The world community seems to be losing its strategic bearings. Instead of focusing on the real problem, namely the increasingly unstable international order, it is trying to apply Cold War-era concepts of disarmament.
At best, these are marginally useful; more often, they are harmful in today’s circumstances.
What is most needed nowadays is clear thinking about how to live with an expanding club of nuclear states while keeping the world relatively stable.
To this end, the two great nuclear powers need a coordinated deterrence policy towards new nuclear states.
Simultaneously, they should offer guarantees to non-nuclear states that might feel insecure.
In the first place, it is necessary to fill the increasing security vacuum in the Middle East.
China, the world’s rising strategic player, might join this policy, though it currently ranks third in terms of military power.
Arms-control talks are mostly needed for rendering national arsenals more transparent, and for building confidence between the great powers. That is all there is to their usefulness.
So, instead of mimicking Cold War-era treaties, it is necessary to launch an international discussion about the role of military force and nuclear weapons in the world as it is now evolving.
We might then eventually recognize that eliminating nuclear weapons is not just a myth, but a harmful myth, and that nuclear weapons are a useful asset that has saved, and may continue to save, humanity from itself.
The Dark Matter of Financial Globalization
The recent turmoil in global financial markets – and the liquidity and credit crunch that followed – raises two questions: how did defaulting sub-prime mortgages in the American states of California, Nevada, Arizona, and Florida lead to a worldwide crisis?
And why did systemic risk increase rather than decrease in recent years?
Blame should go to the phenomenon of “securitization.”
In the past, banks kept loans and mortgages on their books, retaining the credit risk.
For example, during the housing bust in the United States in the late 1980’s, many banks that were mortgage lenders (the Savings ampamp; Loans Associations) went belly up, leading to a banking crisis, a credit crunch, and a recession in 1990-1991.
This systemic risk – a financial shock leading to severe economic contagion – was supposed to be reduced by securitization.
Financial globalization meant that banks no longer held assets like mortgages on their books, but packaged them in asset-backed securities that were sold to investors in capital markets worldwide, thereby distributing risk more widely.
What went wrong?
The problem was not just sub-prime mortgages.
The same reckless lending practices – no down-payments, no verification of borrowers’ incomes and assets, interest-rate-only mortgages, negative amortization, teaser rates – occurred in more than 50% of all US mortgages in 2005-2007.
Because securitization meant that banks were not carrying the risk and earned fees for transactions, they no longer cared about the quality of their lending.
Indeed, a chain of financial intermediaries now earn fees without bearing the credit risk.
As a result, mortgage brokers maximize their income by generating larger volumes of mortgages, as do the banks that package these loans into mortgage-backed securities (MBS’s).
Investment banks then earn fees for re-packaging these securities in tranches of collateralized debt obligations, or CDO’s (and sometimes into CDO’s of CDO’s).
Moreover, credit rating agencies had serious conflicts of interest, because they received fees from the managers of these instruments.
Regulators sat on their hands, as the US regulatory philosophy was free-market fundamentalism.
Finally, the investors who bought MBS’s and CDO’s were greedy and believed the misleading ratings.
But the liquidity crunch was not the only problem; there was also a solvency problem.
Indeed, in the US today, hundreds of thousands – possibly two million – households are bankrupt and thus will default on their mortgages.
Around sixty sub-prime lenders have already gone bankrupt.
Many homebuilders are near bankrupt, as are some hedge funds and other highly leveraged institutions.
Even in the US corporate sector, defaults will rise, owing to sharply higher corporate bond spreads.
Easier monetary policy may boost liquidity, but it will not resolve the solvency crisis.
There are two reasons for this:
1. massive uncertainty about the size of the losses.
In part, the size will depend on how much home prices fall amp#45;amp#45; 10%? 20%?
Moreover, it is hard to price losses on exotic instruments that are illiquid (i.e. do not have a market price).
2. thanks to securitization, private equity, hedge funds, and over-the-counter trading, financial markets have become less transparent.
This opacity means that no one knows who is holding what, which saps confidence. When the repricing of risk finally occurred in September, investors panicked causing a liquidity run and a credit strike.
So what is to be done?
It will be hard to reverse financial liberalization, but its negative side effects – including greater systemic risk – require a series of reforms.
First, more information and transparency about complex assets and who is holding them are needed.
Second, complex instruments should be traded on exchanges rather than on over-the-counter markets, and they should be standardized so that liquid secondary markets for them can arise.
Third, we need better supervision and regulation of the financial system, including regulation of opaque or highly leveraged financial institutions such as hedge funds and even sovereign wealth funds.
Fourth, the role of rating agencies needs to be rethought, with more regulation and competition introduced.
Finally, liquidity risk should be properly assessed in risk management models, and both banks and other financial institutions should better price and manage such risk; most financial crises are triggered by maturity mismatches.
These crucial issues should be put on the agenda of the G7 finance ministers to prevent a serious backlash against financial globalization and reduce the risk that financial turmoil will lead to severe economic damage.
The Dark Side of Defending Freedom
BRUSSELS – The price of freedom, it is said, is eternal vigilance.
But that price can take the form of morally squalid decisions in which innocent people bear the brunt of the cost of freedom’s defense.
Under the cover of the Cold War, Western governments were regularly forced to make many strategically realistic but morally noxious decisions.
Dictators like Zaire’s Mobuto and Indonesia’s Suharto were embraced on the principle that “he might be a bastard, but at least he’s our bastard.”
In addition, all sorts of dubious “freedom fighters,” from the Contras in Nicaragua to Hissene Habré in Chad to Jonas Savimbi in Angola received Western arms and political backing.
Even the genocidal Khmer Rouge were, for a brief time, partly defended by the US in their forest redoubts after their eviction from Phnom Penh.
Twenty years after the Cold War’s end, the West has at times recognized its duty to make amends to those who were, in a very real sense, the “collateral damage” of that ideological struggle.
For example, the countries that were consigned by Roosevelt and Churchill to Stalin’s un-tender mercies are now mostly part of the European Union.
But there are other, untold stories of people who paid a heavy price for the West’s freedoms that have not gained much attention.
The fate of the Chagossians, the former residents of the Chagos Islands in the Indian Ocean, is particularly harrowing.
The way in which the inhabitants of this archipelago were systematically dispossessed and thrown off their land in the name of Western strategic interests is a human tragedy for which the West can and should make restitution.
Between 1968 and 1973, British Labour and Conservative governments organized the removal of everyone living on the 55 islands that now constitute the British Indian Ocean Territory (BIOT), and that were split off from Mauritius at the time of its independence.
This was done at the behest of the United States, which at the time was embroiled in the Vietnam War and required, for understandable security reasons, the use of the island of Diego Garcia as an air and naval base.
The British Foreign Office claimed at the time that the Chagos Islands had no settled population.
The Chagossians were shipped off to the Seychelles and Mauritius, where they were deposited and left to fend for themselves.
They were ostracized and isolated.
Many succumbed to mental illness, drug addiction, and alcoholism.
Despite the tragic impact of this compulsory removal, the islanders never gave up their campaign to win the right to return home.
Many Chagossians eventually moved away from the Seychelles and Mauritius.
Some settled in London (my European Parliament constituency).
They have been a persistent thorn in the side of successive British governments, but at long last Prime Minister David Cameron’s coalition government seems to be seeking to conclude this matter in a fair, humane, and just way.
The Chagossians want to return to the outer islands of the Chagos archipelago.
They have never sought to resettle on Diego Garcia, which remains off-limits and leased to the US government until 2016 – and continues to play a vital role in Western security and defense.
Given Cameron’s determination to place the US-UK relationship on a more equal footing than it was under Labour, I hope he will feel free to raise this issue with the Obama administration; indeed, a deal between the late UK Foreign Secretary Robin Cook and former US Secretary of State Madeleine Albright was apparently very nearly closed some years ago.
Despite claims to the contrary, there is no possible threat to US interests from the islanders’ return to the outer islands, hundreds of kilometers away from Diego Garcia.
Ten years ago, the Chagossians won a tremendous victory in the British High Court, which ruled that the islanders’ expulsion had been unlawful, and that they should be allowed to return.
Initially, the government accepted the ruling, but it was overturned by an Order-in-Council (an executive order from the Queen and not a law passed by Parliament) in 2004.
Now is not the time to discuss how appropriate Orders-in-Council are for lawmaking in a twenty-first-century democracy, but this move showed disregard for the Chagossians’ basic human rights.
Earlier this year, we saw this cynicism in action once again, when David Miliband, foreign secretary in the previous Labour government, declared the Chagos Islands to be a protected marine zone, thereby preventing the islanders – if eventually they do return – from making their living from fishing.
The British Foreign Office has often used the excuse that resettling the Chagossians would be prohibitively expensive.
But, when answering a parliamentary question that I put to the European Commission recently, the commissioner responsible for development policy, Andris Piebalgs, indicated that the Commission would consider any request from the UK for co-financing the Chagossians’ repatriation, which the EU fully and rightly accepts as a sovereign UK matter.
Ten years after the Chagossians’ Pyrrhic legal victory – a victory that seemed merely to strengthen the Foreign Office’s resolve to stop them from returning to the Islands – they have taken their campaign to the European Court of Human Rights, which has suggested that the case be withdrawn in favor of a “friendly settlement.”
Many Chagossians who have settled in the UK would go back immediately – if only the government would let them.
The Dark Side of Self-Determination
CAMBRIDGE – National self-determination seems a straightforward moral principle, but it is fraught with problems.
After Russia sent troops into Georgia in August 2008, it recognized the independence of two breakaway Georgian provinces, South Ossetia and Abkhazia.
When few other states followed its example, Russia pointed out that the NATO countries had used force to help Kosovo separate from Serbia.
Self-determination is generally defined as the right of a people to form its own state.
This is an important principle, but who is the self that is to do the determining?
Consider Somalia back in the 1960’s.  Africans used the principle of self-determination to end colonial rule.
Unlike many other African states, Somalis had roughly the same linguistic and ethnic background.
In contrast, neighboring Kenya was formed by colonial rule from dozens of different peoples or tribes, with different linguistic backgrounds and customs.
Part of northern Kenya was inhabited by Somalis.
Somalia said the principle of national self-determination should allow Somalis in northeastern Kenya (and in the southern Ethiopia) to secede, because they were one Somali nation.
Kenya and Ethiopia refused, saying they were still in the process of building a nation.
The result was a series of wars in northeast Africa over the Somali nationalist question.
The ironic sequel was Somalia’s later fragmentation in a civil war among its clans and warlord leaders.
Voting does not always solve problems of self-determination.
First, there is the question of where one votes.
Consider Ireland, where for many years Catholics objected that if a vote were held within Northern Ireland, the two-thirds Protestant majority would rule.
Protestants replied that if a vote were held within the geographical area of the entire island, the two-thirds Catholic majority would rule.
Eventually, after decades of strife, outside mediation helped.
But this still does not address the question of when one votes.
In the 1960’s, the Somalis wanted to vote right away; Kenya wanted to wait 40 or 50 years while it went about its nation-building, or reshaping tribal allegiances into a Kenyan identity.
Does secession harm those left behind?
What about the resources the secessionists take with them, or the disruption they create in the country they leave?
For example, the victorious powers in World War I invoked the principle of self-determination, but after the dismantlement of the Austro-Hungarian Empire in 1918, the Sudetenland was incorporated into Czechoslovakia, even though its inhabitants spoke German.
After the Munich Agreement in 1938, the Sudeten Germans seceded from Czechoslovakia and joined Germany, which meant that the mountainous frontier fell under German control – a terrible loss for Czech defenses.